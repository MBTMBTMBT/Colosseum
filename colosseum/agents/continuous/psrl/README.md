The *posterior sampling for reinforcement learning* agent for infinite horizon with undiscounted rewards MDPs is taken from [1].
A regret upper bound of `O(DS\sqrt{AT})` ignoring logarithmic terms for any communicating MDP with unknown but finite 
diameter is given in [1].
`D` is the diameter, `S` is the number of states, `A` is the number of action and `T` is the optimization time horizon.

[ [1](https://arxiv.org/pdf/1705.07041.pdf) ] Agrawal, Shipra, and Randy Jia. "Posterior sampling for reinforcement learning: worst-case regret bounds." Advances in Neural Information Processing Systems. 2017.
