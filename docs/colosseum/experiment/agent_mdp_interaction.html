<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>colosseum.experiment.agent_mdp_interaction API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>colosseum.experiment.agent_mdp_interaction</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import io
from time import time
from typing import TYPE_CHECKING, Dict, Iterable, List, Tuple, Union

import numpy as np
import pandas as pd
import toolz
import tqdm
from matplotlib import pyplot as plt
from tqdm import trange
from wrapt_timeout_decorator import timeout

from colosseum import config
from colosseum.config import process_debug_output
from colosseum.experiment.utils import get_episodic_regrets_and_average_reward
from colosseum.mdp.utils.markov_chain import get_average_reward
from colosseum.utils.acme import InMemoryLogger
from colosseum.utils.acme.base_logger import Logger

if TYPE_CHECKING:
    from colosseum.mdp import ContinuousMDP, EpisodicMDP
    from colosseum.agent.agents import BaseAgent


class MDPLoop:
    @staticmethod
    def get_available_indicators() -&gt; List[str]:
        &#34;&#34;&#34;
        returns the code names for the indicators that are calculated by the MDPLoop.
        &#34;&#34;&#34;
        return [
            &#34;cumulative_reward&#34;,
            &#34;cumulative_expected_reward&#34;,
            &#34;normalized_cumulative_expected_reward&#34;,
            &#34;random_cumulative_reward&#34;,
            &#34;normalized_cumulative_reward&#34;,
            &#34;random_cumulative_reward&#34;,
            &#34;cumulative_regret&#34;,
            &#34;random_cumulative_regret&#34;,
            &#34;steps_per_second&#34;,
            &#34;normalized_cumulative_regret&#34;,
            &#34;random_normalized_cumulative_regret&#34;,
        ]

    def __init__(
        self,
        mdp: Union[&#34;BaseMDP&#34;, &#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;],
        agent: &#34;BaseAgent&#34;,
        logger: Logger = InMemoryLogger(),
        n_log_intervals_to_check_for_agent_optimality: int = 10,
        enforce_time_constraint: bool = True,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        mdp: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
            the MDP.
        agent : BaseAgent
            the agent.
        logger : Logger, optional
            a logger used to store the result of the interaction between the agent and the MDP.
        &#34;&#34;&#34;
        self.logger = logger
        self._enforce_time_constraint = enforce_time_constraint
        self._mdp = mdp
        self._agent = agent
        self._episodic = self._mdp.is_episodic()
        self._n_steps_to_check_for_agent_optimality = (
            n_log_intervals_to_check_for_agent_optimality
        )
        assert self._episodic == agent.is_episodic()
        self.actions_sequence = []

    @property
    def remaining_time(self) -&gt; float:
        return self._max_time - (time() - self._mdp_loop_timer)

    def _limit_update_time(self, t, f):
        try:
            if self.remaining_time &lt; 0.5:
                raise TimeoutError()
            timeout(self.remaining_time)(f)()
        except TimeoutError or SystemError:
            if config.DEBUG_LEVEL &gt; 0:
                print(&#34;Time exceeded with function &#34;, f)
            self._limit_exceeded(t)

    def _limit_exceeded(self, t):
        self._is_training = False
        self._last_training_step = t
        if config.DEBUG_LEVEL &gt; 0:
            do = f&#34;Stopped training at {time() - self._mdp_loop_timer:.2f}&#34;
            process_debug_output(do)
        if self._verbose:
            self._verbose_postfix[&#34;is_training&#34;] = f&#34;No, time exhausted at {t}&#34;

    def run(
        self,
        T: int,
        log_every: int = -1,
        max_time: float = np.inf,
    ) -&gt; Tuple[int, Dict[str, float]]:
        &#34;&#34;&#34;

        Parameters
        ----------
        T : int
            number of total interactions between the agent and the MDP.
        log_every : int, optional
            the number of time steps after which performance indicators are calculated. By default, it does not calculate
            them at any time except at the last one.
        max_time : float, optional
            the maximum number of seconds the interactions can take. If it is surpassed then the loop is interrupted.
            By default, the maximum given time is infinite.

        Returns
        ----------
        a tuple containing the time step at which the training has been interrupted due to the time constraint, which is
        -1 if the constraint has been respected, and a dictionary containing the performance indicators.
        &#34;&#34;&#34;
        if max_time == np.inf:
            enforce_time_constraint = False
        else:
            enforce_time_constraint = self._enforce_time_constraint

        assert (
            type(log_every) == int
        ), f&#34;The log_every variable should be an integer, received value: {log_every}.&#34;
        log_every = -1 if log_every == 0 else log_every

        self._reset_run_variables()
        self._max_time = max_time

        ts = self._mdp.reset()
        first_before_new_episode_timer = time()
        if enforce_time_constraint and self.remaining_time &lt; np.inf:
            self._limit_update_time(0, self._agent.before_start_interacting)
        else:
            self._agent.before_start_interacting()
        if config.DEBUG_LEVEL &gt; 0:
            if self._is_training:
                do = f&#34;before_start_interacting completed in {time() - first_before_new_episode_timer:.2f}.&#34;
            else:
                do = &#34;before_start_interacting exceeded the time limit.&#34;
            process_debug_output(do)

        self._set_loop(T)
        for t in self._loop:
            if self._is_training and self.remaining_time &lt; 0.5:
                self._limit_exceeded(t)

            # MDP step
            h = self._mdp.h
            action = self._agent.select_action(ts, h)
            new_ts = self._mdp.step(action)
            self.actions_sequence.append(new_ts.reward)

            # Single step agent update
            if self._is_training:
                if enforce_time_constraint and self.remaining_time &lt; np.inf:
                    self._limit_update_time(
                        t,
                        lambda: self._agent.step_update(ts, action, new_ts, h),
                    )
                else:
                    self._agent.step_update(ts, action, new_ts, h)

            # End of episode agent update
            if self._is_training and self._agent.is_episode_end(ts, action, new_ts, h):
                if enforce_time_constraint and self.remaining_time &lt; np.inf:
                    self._limit_update_time(t, self._agent.episode_end_update)
                else:
                    self._agent.episode_end_update()

            if t &gt; 0 and log_every &gt; 0 and t % log_every == 0:
                # Log the performance of the agent
                self._update_performance_logs(t)
                self._n_steps_since_last_log = 0

                # User defined custom log
                self._agent.agent_logs()

                # Verbose loggings
                self._update_user_loggings(t)

                # Storing the latest regrets
                self._latest_expected_regrets.append(self._normalized_regret)
                if (
                    len(self._latest_expected_regrets)
                    &gt; self._n_steps_to_check_for_agent_optimality
                ):
                    self._latest_expected_regrets.pop(0)

                # Stop training if the agent has confidently reached the optimal policy
                if self._is_training and t &gt; 0.2 * T and self._is_policy_optimal():
                    if type(self._loop) == tqdm.std.tqdm:
                        self._verbose_postfix[&#34;is_training&#34;] = f&#34;No, optimal at {t}&#34;
                    self._is_training = False

            self._n_steps_since_last_log += 1
            self._cumulative_reward += new_ts.reward
            ts = new_ts

            # Resetting episodic MDPs
            if self._mdp.is_episodic() and new_ts.last():
                assert self._mdp.necessary_reset or t == T - 2
                ts = self._mdp.reset()
                self._n_episodes += 1

        self._update_performance_logs(t)
        self.logger.close()
        return self._last_training_step, self._last_logs

    def _reset_run_variables(self):
        self._cumulative_reward = 0.0
        self._cumulative_regret = 0.0
        self._normalized_cumulative_regret = 0.0
        self._random_cumulative_reward = 0.0
        self._random_cumulative_regret = 0.0
        self._normalized_random_cumulative_regret = 0.0
        self._cumulative_expected_reward_agent = 0.0

        self._verbose = False
        self._verbose_postfix = dict(is_training=&#34;True&#34;)
        self._is_training = True
        self._n_steps_since_last_log = 0
        self._last_training_step = -1
        self._n_episodes = 0
        self._last_logs = None
        self._past_logs = None
        self._cached_episodic_regrets = None
        self._cached_continuous_regrets = None
        self._latest_expected_regrets = []

        # Cache the regret for the random agent
        if self._episodic:
            self._episodic_regret_random_agent = (
                self._mdp.episodic_optimal_average_reward
                - self._mdp.episodic_random_average_reward
            )
            self._episodic_normalized_regret_random_agent = (
                self._episodic_regret_random_agent
                / (
                    self._mdp.episodic_optimal_average_reward
                    - self._mdp.episodic_worst_average_reward
                )
            )
        else:
            self._regret_random_agent = (
                self._mdp.optimal_average_reward - self._mdp.random_average_reward
            )
            self._normalized_regret_random_agent = self._regret_random_agent / (
                self._mdp.optimal_average_reward - self._mdp.worst_average_reward
            )
            assert (
                self._mdp.optimal_average_reward - self._mdp.worst_average_reward
                &gt; 0.0002
            ), type(self._mdp).__name__ + str(self._mdp.parameters)

        self.logger.reset()
        self._mdp_loop_timer = time()
        self._verbose_time = time()

    def _update_performance_logs(self, t: int):
        self._compute_performance_indicators(t + 1)

        self._last_logs = dict(
            steps=t,
            steps_per_second=t / (time() - self._mdp_loop_timer),
            cumulative_reward=self._cumulative_reward,
            cumulative_expected_reward=self._cumulative_expected_reward_agent,
            normalized_cumulative_expected_reward=(
                (self._cumulative_expected_reward_agent - self._mdp.r_min)
                / (self._mdp.r_max - self._mdp.r_min)
            ),
            random_cumulative_reward=self._cumulative_reward_random_agent,
            normalized_cumulative_reward=(
                (self._cumulative_reward - self._mdp.r_min)
                / (self._mdp.r_max - self._mdp.r_min)
            ),
            normalized_random_cumulative_reward=(
                (self._cumulative_reward_random_agent - self._mdp.r_min)
                / (self._mdp.r_max - self._mdp.r_min)
            ),
            cumulative_regret=self._cumulative_regret,
            normalized_cumulative_regret=self._normalized_cumulative_regret,
            random_cumulative_regret=self._cumulative_regret_random_agent,
            random_normalized_cumulative_regret=self._normalized_cumulative_regret_random_agent,
        )

        # Communicate the indicators to the logger with a maximum of five digits
        a = toolz.valmap(lambda x: np.round(x, 5), self._last_logs)
        self.logger.write(a)

    def _compute_regrets(self):
        if self._episodic:
            return self._compute_episodic_regret()
        return self._compute_continuous_regret()

    def _compute_performance_indicators(self, t: int):
        self._compute_regrets()

        if self._episodic:
            self._cumulative_regret_random_agent = (
                self._episodic_regret_random_agent * t
            )
            self._normalized_cumulative_regret_random_agent = (
                self._episodic_normalized_regret_random_agent * t
            )
            self._cumulative_reward_random_agent = (
                self._mdp.episodic_random_average_reward * t
            )
        else:
            self._cumulative_regret_random_agent = self._regret_random_agent * t
            self._normalized_cumulative_regret_random_agent = (
                self._normalized_regret_random_agent * t
            )
            self._cumulative_reward_random_agent = self._mdp.random_average_reward * t

        # Avoid numerical errors that lead to negative rewards
        assert (
            self._regret &gt;= 0.0
        ), f&#34;{self._regret} on {type(self._mdp).__name__} {self._mdp.parameters} for policy {self._agent.current_optimal_stochastic_policy}&#34;
        assert self._normalized_regret &gt;= 0.0, self._normalized_regret

        self._cumulative_regret += self._regret * self._n_steps_since_last_log
        self._normalized_cumulative_regret += (
            self._normalized_regret * self._n_steps_since_last_log
        )
        self._cumulative_expected_reward_agent += (
            self._agent_average_reward * self._n_steps_since_last_log
        )

    @property
    def _agent_average_reward(self):
        if self._episodic:
            return self._episodic_agent_average_reward / self._mdp.H
        return self._agent_continuous_average_reward

    def _compute_continuous_regret(self):
        if not self._is_training:
            if self._cached_continuous_regrets is None:
                self._cached_continuous_regrets = self._get_continuous_regrets()
            self._regret, self._normalized_regret = self._cached_continuous_regrets
        else:
            self._regret, self._normalized_regret = self._get_continuous_regrets()

    def _get_continuous_regrets(self):
        self._agent_continuous_average_reward = get_average_reward(
            self._mdp.T,
            self._mdp.R,
            self._agent.current_optimal_stochastic_policy,
            self._mdp.starting_states_and_probs,
        )
        r = self._mdp.optimal_average_reward - self._agent_continuous_average_reward
        if np.isclose(r, 0.0, atol=1e-3):
            r = 0.0
        if r &lt; 0:
            r = 0
        nr = r / (self._mdp.optimal_average_reward - self._mdp.worst_average_reward)
        return r, nr

    def _compute_episodic_regret(self):
        if not self._is_training:
            # If the agent is not training, the policy will not change we can cache and reuse the regret for each given
            # starting node.
            if self._cached_episodic_regrets is None:
                Rs, epi_agent_ar = get_episodic_regrets_and_average_reward(
                    self._mdp.H,
                    self._mdp.T,
                    self._mdp.R,
                    self._agent.current_optimal_stochastic_policy,
                    self._mdp.starting_distribution,
                    self._mdp.optimal_value[1],
                )
                self._episodic_agent_average_reward = epi_agent_ar
                self._cached_episodic_regrets = {
                    n: (
                        Rs[self._mdp.node_to_index[n]] / self._mdp.H,  # expected regret
                        Rs[self._mdp.node_to_index[n]]  # normalized expected regret
                        / self._mdp.get_minimal_regret_for_starting_node(n),
                    )
                    for n in self._mdp.starting_nodes
                }
            self._regret, self._normalized_regret = self._cached_episodic_regrets[
                self._mdp.last_starting_node
            ]
        else:
            Rs, epi_agent_ar = get_episodic_regrets_and_average_reward(
                self._mdp.H,
                self._mdp.T,
                self._mdp.R,
                self._agent.current_optimal_stochastic_policy,
                self._mdp.starting_distribution,
                self._mdp.optimal_value[1],
            )
            self._episodic_agent_average_reward = epi_agent_ar
            self._regret = (
                Rs[self._mdp.node_to_index[self._mdp.last_starting_node]] / self._mdp.H
            )
            self._normalized_regret = (
                self._regret
                / self._mdp.get_minimal_regret_for_starting_node(
                    self._mdp.last_starting_node
                )
                * self._mdp.H
            )

    def _is_policy_optimal(self) -&gt; bool:
        &#34;&#34;&#34;
        checks whether the agent has confidently reached an optimal policy by checking if the latest logged regrets are
        all close to zero.
        &#34;&#34;&#34;
        if (
            len(self._latest_expected_regrets)
            == self._n_steps_to_check_for_agent_optimality
            and np.isclose(
                0,
                self._latest_expected_regrets,
                atol=1e-4 if self._mdp.is_episodic() else 1e-5,
            ).all()
        ):
            # After we get an empirical suggestions that the policy may be optimal, we check if the expected regret is
            # zero as well
            self._compute_regrets()
            return np.isclose(self._normalized_regret, 0).all()
        return False

    def _set_loop(self, T: int) -&gt; Iterable:
        &#34;&#34;&#34;
        creates a loop lasting for T steps taking into account the verbosity level.
        &#34;&#34;&#34;
        if config.VERBOSE_LEVEL != 0:
            desc = f&#34;Experiment loop {type(self._agent).__name__}@{type(self._mdp).__name__}&#34;
            if type(config.VERBOSE_LEVEL) == str:
                self.s = io.StringIO()  # we need this reference
                self._loop = trange(T, desc=desc, file=self.s, mininterval=5)
            else:
                self._loop = trange(T, desc=desc, mininterval=5)
            self._verbose = True
        else:
            self._loop = range(T)

    def _update_user_loggings(self, t: int):
        if self._verbose:  # and time() - self._verbose_time &gt; 5:
            self._verbose_postfix[&#34;Instantaneous normalized regret&#34;] = np.round(
                self._normalized_regret / t, 8
            )
            self._loop.set_postfix(self._verbose_postfix, refresh=False)

        # if time() - self._verbose_time &gt; 5:
        #     self._verbose_time = time()
        #     if self._verbose:
        #         self._loop.set_postfix_str(f&#34;Episode: {self._n_episodes}&#34;)
        #     if type(config.VERBOSE_LEVEL) == str:
        #         if not os.path.isdir(
        #             config.VERBOSE_LEVEL[: config.VERBOSE_LEVEL.rfind(os.sep)]
        #         ):
        #             os.makedirs(
        #                 config.VERBOSE_LEVEL[: config.VERBOSE_LEVEL.rfind(os.sep)],
        #                 exist_ok=True,
        #             )
        #         with open(config.VERBOSE_LEVEL, &#34;a&#34;) as f:
        #             f.write(
        #                 datetime.datetime.now().strftime(&#34;%H:%M:%S&#34;)
        #                 + &#34;  &#34;
        #                 + self.s.getvalue()
        #                 .split(&#34;\x1b&#34;)[0][2:]
        #                 .replace(&#34;\x00&#34;, &#34;&#34;)
        #                 .replace(&#34;\n\r&#34;, &#34;&#34;)
        #                 + &#34;\n&#34;
        #             )
        #         self.s.truncate(0)

    def plot(
        self,
        y: Union[str, List[str], Tuple[str, ...]] = (&#34;cumulative_regret&#34;,),
        ax=None,
        labels: Union[str, List[str], Tuple[str, ...]] = None,
            common_label : str = None
    ):
        &#34;&#34;&#34;
        quick utility function to plot the performance indicators directly from the MDPLoop.
        &#34;&#34;&#34;
        show = ax is None
        if ax is None:
            fig, ax = plt.subplots()
        if type(y) == str:
            y = [y]
        if type(labels) == str:
            labels = [labels]
        if type(labels) == list:
            assert len(labels) == len(
                y
            ), &#34;Please make sure that the labels corresponds to the measures.&#34;

        df_e = pd.DataFrame(self.logger.data)
        time_steps = [0] + df_e.loc[:, &#34;steps&#34;].tolist()
        for i, yy in enumerate(y):
            ax.plot(
                time_steps[1:] if yy == &#34;steps_per_second&#34; else time_steps,
                ([] if yy == &#34;steps_per_second&#34; else [0]) + df_e.loc[:, yy].tolist(),
                label=(yy.replace(&#34;_&#34;, &#34; &#34;).capitalize()
                if labels is None or labels[i] is None
                else labels[i]) + (&#34;&#34; if common_label is None or &#34;random&#34; in yy.lower() else f&#34; ({common_label})&#34;),
            )
        ax.set_xlabel(&#34;time step&#34;)
        ax.legend()
        if show:
            plt.tight_layout()
            plt.show()</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="colosseum.experiment.agent_mdp_interaction.MDPLoop"><code class="flex name class">
<span>class <span class="ident">MDPLoop</span></span>
<span>(</span><span>mdp: Union[ForwardRef('BaseMDP'), ForwardRef('EpisodicMDP'), ForwardRef('ContinuousMDP')], agent: BaseAgent, logger: <a title="colosseum.utils.acme.base_logger.Logger" href="../utils/acme/base_logger.html#colosseum.utils.acme.base_logger.Logger">Logger</a> = &lt;colosseum.utils.acme.in_memory_logger.InMemoryLogger object&gt;, n_log_intervals_to_check_for_agent_optimality: int = 10, enforce_time_constraint: bool = True)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mdp</code></strong> :&ensp;<code>Union["EpisodicMDP", "ContinuousMDP"]</code></dt>
<dd>the MDP.</dd>
<dt><strong><code>agent</code></strong> :&ensp;<code>BaseAgent</code></dt>
<dd>the agent.</dd>
<dt><strong><code>logger</code></strong> :&ensp;<code>Logger</code>, optional</dt>
<dd>a logger used to store the result of the interaction between the agent and the MDP.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MDPLoop:
    @staticmethod
    def get_available_indicators() -&gt; List[str]:
        &#34;&#34;&#34;
        returns the code names for the indicators that are calculated by the MDPLoop.
        &#34;&#34;&#34;
        return [
            &#34;cumulative_reward&#34;,
            &#34;cumulative_expected_reward&#34;,
            &#34;normalized_cumulative_expected_reward&#34;,
            &#34;random_cumulative_reward&#34;,
            &#34;normalized_cumulative_reward&#34;,
            &#34;random_cumulative_reward&#34;,
            &#34;cumulative_regret&#34;,
            &#34;random_cumulative_regret&#34;,
            &#34;steps_per_second&#34;,
            &#34;normalized_cumulative_regret&#34;,
            &#34;random_normalized_cumulative_regret&#34;,
        ]

    def __init__(
        self,
        mdp: Union[&#34;BaseMDP&#34;, &#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;],
        agent: &#34;BaseAgent&#34;,
        logger: Logger = InMemoryLogger(),
        n_log_intervals_to_check_for_agent_optimality: int = 10,
        enforce_time_constraint: bool = True,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        mdp: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
            the MDP.
        agent : BaseAgent
            the agent.
        logger : Logger, optional
            a logger used to store the result of the interaction between the agent and the MDP.
        &#34;&#34;&#34;
        self.logger = logger
        self._enforce_time_constraint = enforce_time_constraint
        self._mdp = mdp
        self._agent = agent
        self._episodic = self._mdp.is_episodic()
        self._n_steps_to_check_for_agent_optimality = (
            n_log_intervals_to_check_for_agent_optimality
        )
        assert self._episodic == agent.is_episodic()
        self.actions_sequence = []

    @property
    def remaining_time(self) -&gt; float:
        return self._max_time - (time() - self._mdp_loop_timer)

    def _limit_update_time(self, t, f):
        try:
            if self.remaining_time &lt; 0.5:
                raise TimeoutError()
            timeout(self.remaining_time)(f)()
        except TimeoutError or SystemError:
            if config.DEBUG_LEVEL &gt; 0:
                print(&#34;Time exceeded with function &#34;, f)
            self._limit_exceeded(t)

    def _limit_exceeded(self, t):
        self._is_training = False
        self._last_training_step = t
        if config.DEBUG_LEVEL &gt; 0:
            do = f&#34;Stopped training at {time() - self._mdp_loop_timer:.2f}&#34;
            process_debug_output(do)
        if self._verbose:
            self._verbose_postfix[&#34;is_training&#34;] = f&#34;No, time exhausted at {t}&#34;

    def run(
        self,
        T: int,
        log_every: int = -1,
        max_time: float = np.inf,
    ) -&gt; Tuple[int, Dict[str, float]]:
        &#34;&#34;&#34;

        Parameters
        ----------
        T : int
            number of total interactions between the agent and the MDP.
        log_every : int, optional
            the number of time steps after which performance indicators are calculated. By default, it does not calculate
            them at any time except at the last one.
        max_time : float, optional
            the maximum number of seconds the interactions can take. If it is surpassed then the loop is interrupted.
            By default, the maximum given time is infinite.

        Returns
        ----------
        a tuple containing the time step at which the training has been interrupted due to the time constraint, which is
        -1 if the constraint has been respected, and a dictionary containing the performance indicators.
        &#34;&#34;&#34;
        if max_time == np.inf:
            enforce_time_constraint = False
        else:
            enforce_time_constraint = self._enforce_time_constraint

        assert (
            type(log_every) == int
        ), f&#34;The log_every variable should be an integer, received value: {log_every}.&#34;
        log_every = -1 if log_every == 0 else log_every

        self._reset_run_variables()
        self._max_time = max_time

        ts = self._mdp.reset()
        first_before_new_episode_timer = time()
        if enforce_time_constraint and self.remaining_time &lt; np.inf:
            self._limit_update_time(0, self._agent.before_start_interacting)
        else:
            self._agent.before_start_interacting()
        if config.DEBUG_LEVEL &gt; 0:
            if self._is_training:
                do = f&#34;before_start_interacting completed in {time() - first_before_new_episode_timer:.2f}.&#34;
            else:
                do = &#34;before_start_interacting exceeded the time limit.&#34;
            process_debug_output(do)

        self._set_loop(T)
        for t in self._loop:
            if self._is_training and self.remaining_time &lt; 0.5:
                self._limit_exceeded(t)

            # MDP step
            h = self._mdp.h
            action = self._agent.select_action(ts, h)
            new_ts = self._mdp.step(action)
            self.actions_sequence.append(new_ts.reward)

            # Single step agent update
            if self._is_training:
                if enforce_time_constraint and self.remaining_time &lt; np.inf:
                    self._limit_update_time(
                        t,
                        lambda: self._agent.step_update(ts, action, new_ts, h),
                    )
                else:
                    self._agent.step_update(ts, action, new_ts, h)

            # End of episode agent update
            if self._is_training and self._agent.is_episode_end(ts, action, new_ts, h):
                if enforce_time_constraint and self.remaining_time &lt; np.inf:
                    self._limit_update_time(t, self._agent.episode_end_update)
                else:
                    self._agent.episode_end_update()

            if t &gt; 0 and log_every &gt; 0 and t % log_every == 0:
                # Log the performance of the agent
                self._update_performance_logs(t)
                self._n_steps_since_last_log = 0

                # User defined custom log
                self._agent.agent_logs()

                # Verbose loggings
                self._update_user_loggings(t)

                # Storing the latest regrets
                self._latest_expected_regrets.append(self._normalized_regret)
                if (
                    len(self._latest_expected_regrets)
                    &gt; self._n_steps_to_check_for_agent_optimality
                ):
                    self._latest_expected_regrets.pop(0)

                # Stop training if the agent has confidently reached the optimal policy
                if self._is_training and t &gt; 0.2 * T and self._is_policy_optimal():
                    if type(self._loop) == tqdm.std.tqdm:
                        self._verbose_postfix[&#34;is_training&#34;] = f&#34;No, optimal at {t}&#34;
                    self._is_training = False

            self._n_steps_since_last_log += 1
            self._cumulative_reward += new_ts.reward
            ts = new_ts

            # Resetting episodic MDPs
            if self._mdp.is_episodic() and new_ts.last():
                assert self._mdp.necessary_reset or t == T - 2
                ts = self._mdp.reset()
                self._n_episodes += 1

        self._update_performance_logs(t)
        self.logger.close()
        return self._last_training_step, self._last_logs

    def _reset_run_variables(self):
        self._cumulative_reward = 0.0
        self._cumulative_regret = 0.0
        self._normalized_cumulative_regret = 0.0
        self._random_cumulative_reward = 0.0
        self._random_cumulative_regret = 0.0
        self._normalized_random_cumulative_regret = 0.0
        self._cumulative_expected_reward_agent = 0.0

        self._verbose = False
        self._verbose_postfix = dict(is_training=&#34;True&#34;)
        self._is_training = True
        self._n_steps_since_last_log = 0
        self._last_training_step = -1
        self._n_episodes = 0
        self._last_logs = None
        self._past_logs = None
        self._cached_episodic_regrets = None
        self._cached_continuous_regrets = None
        self._latest_expected_regrets = []

        # Cache the regret for the random agent
        if self._episodic:
            self._episodic_regret_random_agent = (
                self._mdp.episodic_optimal_average_reward
                - self._mdp.episodic_random_average_reward
            )
            self._episodic_normalized_regret_random_agent = (
                self._episodic_regret_random_agent
                / (
                    self._mdp.episodic_optimal_average_reward
                    - self._mdp.episodic_worst_average_reward
                )
            )
        else:
            self._regret_random_agent = (
                self._mdp.optimal_average_reward - self._mdp.random_average_reward
            )
            self._normalized_regret_random_agent = self._regret_random_agent / (
                self._mdp.optimal_average_reward - self._mdp.worst_average_reward
            )
            assert (
                self._mdp.optimal_average_reward - self._mdp.worst_average_reward
                &gt; 0.0002
            ), type(self._mdp).__name__ + str(self._mdp.parameters)

        self.logger.reset()
        self._mdp_loop_timer = time()
        self._verbose_time = time()

    def _update_performance_logs(self, t: int):
        self._compute_performance_indicators(t + 1)

        self._last_logs = dict(
            steps=t,
            steps_per_second=t / (time() - self._mdp_loop_timer),
            cumulative_reward=self._cumulative_reward,
            cumulative_expected_reward=self._cumulative_expected_reward_agent,
            normalized_cumulative_expected_reward=(
                (self._cumulative_expected_reward_agent - self._mdp.r_min)
                / (self._mdp.r_max - self._mdp.r_min)
            ),
            random_cumulative_reward=self._cumulative_reward_random_agent,
            normalized_cumulative_reward=(
                (self._cumulative_reward - self._mdp.r_min)
                / (self._mdp.r_max - self._mdp.r_min)
            ),
            normalized_random_cumulative_reward=(
                (self._cumulative_reward_random_agent - self._mdp.r_min)
                / (self._mdp.r_max - self._mdp.r_min)
            ),
            cumulative_regret=self._cumulative_regret,
            normalized_cumulative_regret=self._normalized_cumulative_regret,
            random_cumulative_regret=self._cumulative_regret_random_agent,
            random_normalized_cumulative_regret=self._normalized_cumulative_regret_random_agent,
        )

        # Communicate the indicators to the logger with a maximum of five digits
        a = toolz.valmap(lambda x: np.round(x, 5), self._last_logs)
        self.logger.write(a)

    def _compute_regrets(self):
        if self._episodic:
            return self._compute_episodic_regret()
        return self._compute_continuous_regret()

    def _compute_performance_indicators(self, t: int):
        self._compute_regrets()

        if self._episodic:
            self._cumulative_regret_random_agent = (
                self._episodic_regret_random_agent * t
            )
            self._normalized_cumulative_regret_random_agent = (
                self._episodic_normalized_regret_random_agent * t
            )
            self._cumulative_reward_random_agent = (
                self._mdp.episodic_random_average_reward * t
            )
        else:
            self._cumulative_regret_random_agent = self._regret_random_agent * t
            self._normalized_cumulative_regret_random_agent = (
                self._normalized_regret_random_agent * t
            )
            self._cumulative_reward_random_agent = self._mdp.random_average_reward * t

        # Avoid numerical errors that lead to negative rewards
        assert (
            self._regret &gt;= 0.0
        ), f&#34;{self._regret} on {type(self._mdp).__name__} {self._mdp.parameters} for policy {self._agent.current_optimal_stochastic_policy}&#34;
        assert self._normalized_regret &gt;= 0.0, self._normalized_regret

        self._cumulative_regret += self._regret * self._n_steps_since_last_log
        self._normalized_cumulative_regret += (
            self._normalized_regret * self._n_steps_since_last_log
        )
        self._cumulative_expected_reward_agent += (
            self._agent_average_reward * self._n_steps_since_last_log
        )

    @property
    def _agent_average_reward(self):
        if self._episodic:
            return self._episodic_agent_average_reward / self._mdp.H
        return self._agent_continuous_average_reward

    def _compute_continuous_regret(self):
        if not self._is_training:
            if self._cached_continuous_regrets is None:
                self._cached_continuous_regrets = self._get_continuous_regrets()
            self._regret, self._normalized_regret = self._cached_continuous_regrets
        else:
            self._regret, self._normalized_regret = self._get_continuous_regrets()

    def _get_continuous_regrets(self):
        self._agent_continuous_average_reward = get_average_reward(
            self._mdp.T,
            self._mdp.R,
            self._agent.current_optimal_stochastic_policy,
            self._mdp.starting_states_and_probs,
        )
        r = self._mdp.optimal_average_reward - self._agent_continuous_average_reward
        if np.isclose(r, 0.0, atol=1e-3):
            r = 0.0
        if r &lt; 0:
            r = 0
        nr = r / (self._mdp.optimal_average_reward - self._mdp.worst_average_reward)
        return r, nr

    def _compute_episodic_regret(self):
        if not self._is_training:
            # If the agent is not training, the policy will not change we can cache and reuse the regret for each given
            # starting node.
            if self._cached_episodic_regrets is None:
                Rs, epi_agent_ar = get_episodic_regrets_and_average_reward(
                    self._mdp.H,
                    self._mdp.T,
                    self._mdp.R,
                    self._agent.current_optimal_stochastic_policy,
                    self._mdp.starting_distribution,
                    self._mdp.optimal_value[1],
                )
                self._episodic_agent_average_reward = epi_agent_ar
                self._cached_episodic_regrets = {
                    n: (
                        Rs[self._mdp.node_to_index[n]] / self._mdp.H,  # expected regret
                        Rs[self._mdp.node_to_index[n]]  # normalized expected regret
                        / self._mdp.get_minimal_regret_for_starting_node(n),
                    )
                    for n in self._mdp.starting_nodes
                }
            self._regret, self._normalized_regret = self._cached_episodic_regrets[
                self._mdp.last_starting_node
            ]
        else:
            Rs, epi_agent_ar = get_episodic_regrets_and_average_reward(
                self._mdp.H,
                self._mdp.T,
                self._mdp.R,
                self._agent.current_optimal_stochastic_policy,
                self._mdp.starting_distribution,
                self._mdp.optimal_value[1],
            )
            self._episodic_agent_average_reward = epi_agent_ar
            self._regret = (
                Rs[self._mdp.node_to_index[self._mdp.last_starting_node]] / self._mdp.H
            )
            self._normalized_regret = (
                self._regret
                / self._mdp.get_minimal_regret_for_starting_node(
                    self._mdp.last_starting_node
                )
                * self._mdp.H
            )

    def _is_policy_optimal(self) -&gt; bool:
        &#34;&#34;&#34;
        checks whether the agent has confidently reached an optimal policy by checking if the latest logged regrets are
        all close to zero.
        &#34;&#34;&#34;
        if (
            len(self._latest_expected_regrets)
            == self._n_steps_to_check_for_agent_optimality
            and np.isclose(
                0,
                self._latest_expected_regrets,
                atol=1e-4 if self._mdp.is_episodic() else 1e-5,
            ).all()
        ):
            # After we get an empirical suggestions that the policy may be optimal, we check if the expected regret is
            # zero as well
            self._compute_regrets()
            return np.isclose(self._normalized_regret, 0).all()
        return False

    def _set_loop(self, T: int) -&gt; Iterable:
        &#34;&#34;&#34;
        creates a loop lasting for T steps taking into account the verbosity level.
        &#34;&#34;&#34;
        if config.VERBOSE_LEVEL != 0:
            desc = f&#34;Experiment loop {type(self._agent).__name__}@{type(self._mdp).__name__}&#34;
            if type(config.VERBOSE_LEVEL) == str:
                self.s = io.StringIO()  # we need this reference
                self._loop = trange(T, desc=desc, file=self.s, mininterval=5)
            else:
                self._loop = trange(T, desc=desc, mininterval=5)
            self._verbose = True
        else:
            self._loop = range(T)

    def _update_user_loggings(self, t: int):
        if self._verbose:  # and time() - self._verbose_time &gt; 5:
            self._verbose_postfix[&#34;Instantaneous normalized regret&#34;] = np.round(
                self._normalized_regret / t, 8
            )
            self._loop.set_postfix(self._verbose_postfix, refresh=False)

        # if time() - self._verbose_time &gt; 5:
        #     self._verbose_time = time()
        #     if self._verbose:
        #         self._loop.set_postfix_str(f&#34;Episode: {self._n_episodes}&#34;)
        #     if type(config.VERBOSE_LEVEL) == str:
        #         if not os.path.isdir(
        #             config.VERBOSE_LEVEL[: config.VERBOSE_LEVEL.rfind(os.sep)]
        #         ):
        #             os.makedirs(
        #                 config.VERBOSE_LEVEL[: config.VERBOSE_LEVEL.rfind(os.sep)],
        #                 exist_ok=True,
        #             )
        #         with open(config.VERBOSE_LEVEL, &#34;a&#34;) as f:
        #             f.write(
        #                 datetime.datetime.now().strftime(&#34;%H:%M:%S&#34;)
        #                 + &#34;  &#34;
        #                 + self.s.getvalue()
        #                 .split(&#34;\x1b&#34;)[0][2:]
        #                 .replace(&#34;\x00&#34;, &#34;&#34;)
        #                 .replace(&#34;\n\r&#34;, &#34;&#34;)
        #                 + &#34;\n&#34;
        #             )
        #         self.s.truncate(0)

    def plot(
        self,
        y: Union[str, List[str], Tuple[str, ...]] = (&#34;cumulative_regret&#34;,),
        ax=None,
        labels: Union[str, List[str], Tuple[str, ...]] = None,
            common_label : str = None
    ):
        &#34;&#34;&#34;
        quick utility function to plot the performance indicators directly from the MDPLoop.
        &#34;&#34;&#34;
        show = ax is None
        if ax is None:
            fig, ax = plt.subplots()
        if type(y) == str:
            y = [y]
        if type(labels) == str:
            labels = [labels]
        if type(labels) == list:
            assert len(labels) == len(
                y
            ), &#34;Please make sure that the labels corresponds to the measures.&#34;

        df_e = pd.DataFrame(self.logger.data)
        time_steps = [0] + df_e.loc[:, &#34;steps&#34;].tolist()
        for i, yy in enumerate(y):
            ax.plot(
                time_steps[1:] if yy == &#34;steps_per_second&#34; else time_steps,
                ([] if yy == &#34;steps_per_second&#34; else [0]) + df_e.loc[:, yy].tolist(),
                label=(yy.replace(&#34;_&#34;, &#34; &#34;).capitalize()
                if labels is None or labels[i] is None
                else labels[i]) + (&#34;&#34; if common_label is None or &#34;random&#34; in yy.lower() else f&#34; ({common_label})&#34;),
            )
        ax.set_xlabel(&#34;time step&#34;)
        ax.legend()
        if show:
            plt.tight_layout()
            plt.show()</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="colosseum.experiment.agent_mdp_interaction.MDPLoop.get_available_indicators"><code class="name flex">
<span>def <span class="ident">get_available_indicators</span></span>(<span>) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"><p>returns the code names for the indicators that are calculated by the MDPLoop.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_available_indicators() -&gt; List[str]:
    &#34;&#34;&#34;
    returns the code names for the indicators that are calculated by the MDPLoop.
    &#34;&#34;&#34;
    return [
        &#34;cumulative_reward&#34;,
        &#34;cumulative_expected_reward&#34;,
        &#34;normalized_cumulative_expected_reward&#34;,
        &#34;random_cumulative_reward&#34;,
        &#34;normalized_cumulative_reward&#34;,
        &#34;random_cumulative_reward&#34;,
        &#34;cumulative_regret&#34;,
        &#34;random_cumulative_regret&#34;,
        &#34;steps_per_second&#34;,
        &#34;normalized_cumulative_regret&#34;,
        &#34;random_normalized_cumulative_regret&#34;,
    ]</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="colosseum.experiment.agent_mdp_interaction.MDPLoop.remaining_time"><code class="name">var <span class="ident">remaining_time</span> : float</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def remaining_time(self) -&gt; float:
    return self._max_time - (time() - self._mdp_loop_timer)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="colosseum.experiment.agent_mdp_interaction.MDPLoop.plot"><code class="name flex">
<span>def <span class="ident">plot</span></span>(<span>self, y: Union[str, List[str], Tuple[str, ...]] = ('cumulative_regret',), ax=None, labels: Union[str, List[str], Tuple[str, ...]] = None, common_label: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>quick utility function to plot the performance indicators directly from the MDPLoop.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot(
    self,
    y: Union[str, List[str], Tuple[str, ...]] = (&#34;cumulative_regret&#34;,),
    ax=None,
    labels: Union[str, List[str], Tuple[str, ...]] = None,
        common_label : str = None
):
    &#34;&#34;&#34;
    quick utility function to plot the performance indicators directly from the MDPLoop.
    &#34;&#34;&#34;
    show = ax is None
    if ax is None:
        fig, ax = plt.subplots()
    if type(y) == str:
        y = [y]
    if type(labels) == str:
        labels = [labels]
    if type(labels) == list:
        assert len(labels) == len(
            y
        ), &#34;Please make sure that the labels corresponds to the measures.&#34;

    df_e = pd.DataFrame(self.logger.data)
    time_steps = [0] + df_e.loc[:, &#34;steps&#34;].tolist()
    for i, yy in enumerate(y):
        ax.plot(
            time_steps[1:] if yy == &#34;steps_per_second&#34; else time_steps,
            ([] if yy == &#34;steps_per_second&#34; else [0]) + df_e.loc[:, yy].tolist(),
            label=(yy.replace(&#34;_&#34;, &#34; &#34;).capitalize()
            if labels is None or labels[i] is None
            else labels[i]) + (&#34;&#34; if common_label is None or &#34;random&#34; in yy.lower() else f&#34; ({common_label})&#34;),
        )
    ax.set_xlabel(&#34;time step&#34;)
    ax.legend()
    if show:
        plt.tight_layout()
        plt.show()</code></pre>
</details>
</dd>
<dt id="colosseum.experiment.agent_mdp_interaction.MDPLoop.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self, T: int, log_every: int = -1, max_time: float = inf) ‑> Tuple[int, Dict[str, float]]</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>T</code></strong> :&ensp;<code>int</code></dt>
<dd>number of total interactions between the agent and the MDP.</dd>
<dt><strong><code>log_every</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>the number of time steps after which performance indicators are calculated. By default, it does not calculate
them at any time except at the last one.</dd>
<dt><strong><code>max_time</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>the maximum number of seconds the interactions can take. If it is surpassed then the loop is interrupted.
By default, the maximum given time is infinite.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>a tuple containing the time step at which the training has been interrupted due to the time constraint, which is
-1 if the constraint has been respected, and a dictionary containing the performance indicators.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(
    self,
    T: int,
    log_every: int = -1,
    max_time: float = np.inf,
) -&gt; Tuple[int, Dict[str, float]]:
    &#34;&#34;&#34;

    Parameters
    ----------
    T : int
        number of total interactions between the agent and the MDP.
    log_every : int, optional
        the number of time steps after which performance indicators are calculated. By default, it does not calculate
        them at any time except at the last one.
    max_time : float, optional
        the maximum number of seconds the interactions can take. If it is surpassed then the loop is interrupted.
        By default, the maximum given time is infinite.

    Returns
    ----------
    a tuple containing the time step at which the training has been interrupted due to the time constraint, which is
    -1 if the constraint has been respected, and a dictionary containing the performance indicators.
    &#34;&#34;&#34;
    if max_time == np.inf:
        enforce_time_constraint = False
    else:
        enforce_time_constraint = self._enforce_time_constraint

    assert (
        type(log_every) == int
    ), f&#34;The log_every variable should be an integer, received value: {log_every}.&#34;
    log_every = -1 if log_every == 0 else log_every

    self._reset_run_variables()
    self._max_time = max_time

    ts = self._mdp.reset()
    first_before_new_episode_timer = time()
    if enforce_time_constraint and self.remaining_time &lt; np.inf:
        self._limit_update_time(0, self._agent.before_start_interacting)
    else:
        self._agent.before_start_interacting()
    if config.DEBUG_LEVEL &gt; 0:
        if self._is_training:
            do = f&#34;before_start_interacting completed in {time() - first_before_new_episode_timer:.2f}.&#34;
        else:
            do = &#34;before_start_interacting exceeded the time limit.&#34;
        process_debug_output(do)

    self._set_loop(T)
    for t in self._loop:
        if self._is_training and self.remaining_time &lt; 0.5:
            self._limit_exceeded(t)

        # MDP step
        h = self._mdp.h
        action = self._agent.select_action(ts, h)
        new_ts = self._mdp.step(action)
        self.actions_sequence.append(new_ts.reward)

        # Single step agent update
        if self._is_training:
            if enforce_time_constraint and self.remaining_time &lt; np.inf:
                self._limit_update_time(
                    t,
                    lambda: self._agent.step_update(ts, action, new_ts, h),
                )
            else:
                self._agent.step_update(ts, action, new_ts, h)

        # End of episode agent update
        if self._is_training and self._agent.is_episode_end(ts, action, new_ts, h):
            if enforce_time_constraint and self.remaining_time &lt; np.inf:
                self._limit_update_time(t, self._agent.episode_end_update)
            else:
                self._agent.episode_end_update()

        if t &gt; 0 and log_every &gt; 0 and t % log_every == 0:
            # Log the performance of the agent
            self._update_performance_logs(t)
            self._n_steps_since_last_log = 0

            # User defined custom log
            self._agent.agent_logs()

            # Verbose loggings
            self._update_user_loggings(t)

            # Storing the latest regrets
            self._latest_expected_regrets.append(self._normalized_regret)
            if (
                len(self._latest_expected_regrets)
                &gt; self._n_steps_to_check_for_agent_optimality
            ):
                self._latest_expected_regrets.pop(0)

            # Stop training if the agent has confidently reached the optimal policy
            if self._is_training and t &gt; 0.2 * T and self._is_policy_optimal():
                if type(self._loop) == tqdm.std.tqdm:
                    self._verbose_postfix[&#34;is_training&#34;] = f&#34;No, optimal at {t}&#34;
                self._is_training = False

        self._n_steps_since_last_log += 1
        self._cumulative_reward += new_ts.reward
        ts = new_ts

        # Resetting episodic MDPs
        if self._mdp.is_episodic() and new_ts.last():
            assert self._mdp.necessary_reset or t == T - 2
            ts = self._mdp.reset()
            self._n_episodes += 1

    self._update_performance_logs(t)
    self.logger.close()
    return self._last_training_step, self._last_logs</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="colosseum.experiment" href="index.html">colosseum.experiment</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="colosseum.experiment.agent_mdp_interaction.MDPLoop" href="#colosseum.experiment.agent_mdp_interaction.MDPLoop">MDPLoop</a></code></h4>
<ul class="">
<li><code><a title="colosseum.experiment.agent_mdp_interaction.MDPLoop.get_available_indicators" href="#colosseum.experiment.agent_mdp_interaction.MDPLoop.get_available_indicators">get_available_indicators</a></code></li>
<li><code><a title="colosseum.experiment.agent_mdp_interaction.MDPLoop.plot" href="#colosseum.experiment.agent_mdp_interaction.MDPLoop.plot">plot</a></code></li>
<li><code><a title="colosseum.experiment.agent_mdp_interaction.MDPLoop.remaining_time" href="#colosseum.experiment.agent_mdp_interaction.MDPLoop.remaining_time">remaining_time</a></code></li>
<li><code><a title="colosseum.experiment.agent_mdp_interaction.MDPLoop.run" href="#colosseum.experiment.agent_mdp_interaction.MDPLoop.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>