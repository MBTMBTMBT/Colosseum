<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>colosseum.hardness.analysis.utils API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>colosseum.hardness.analysis.utils</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import json
import os
import random
import re
from copy import deepcopy
from dataclasses import dataclass
from multiprocessing import Pool
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, NamedTuple, Type

import numpy as np
import pandas as pd
import seaborn as sns
from frozendict import frozendict
from matplotlib import pyplot as plt
from tqdm import tqdm, trange

from colosseum import config
from colosseum.experiment.hyperopt.base import agent_hyperparameters_generator
from colosseum.experiment.hyperopt.hyperopt_measures import get_regret_score
from colosseum.agent.agents.episodic import PSRLEpisodic
from colosseum.agent.agents.infinite_horizon import UCRL2Continuous
from colosseum.utils import clean_for_storing, ensure_folder, make_environment_spec
from colosseum.utils.formatter import clean_for_file_path

sns.set_theme()

if TYPE_CHECKING:
    from colosseum.mdp import BaseMDP


class SingleInstanceHyperparamOptParameters(NamedTuple):
    mdp_class: &#34;BaseMDP&#34;
    mdp_parameters: Dict[str, Any]
    n_cores: int
    n_seed: int
    num_samples: int
    optimization_horizon: int
    max_interaction_s: float
    log_every: int
    verbose: bool

@dataclass()
class HardnessAnalysisParams:
    mdp_base_params: Dict[str, Any]
    sizes: Iterable[int]
    ps: Iterable[float]
    n_seeds: int
    mdp_class_name: str = None
    optimal_agent_optimization: Dict = None

def plot_hardness_analysis(
    varying_parameter_name: str,
    varying_parameter_values: Iterable,
    mdp_class,
    base_mdp_parameters,
    n_seeds: int,
    look_for_cache: bool = True,
    cache_folder: str = config.get_hardness_measures_cache_folder(),
    hyper_opt_hyper_params=None,
    # frozendict(
    #     n_cores=5,
    #     n_seed=2,
    #     num_samples=5,
    #     optimization_horizon=100_000,
    #     max_interaction_s=1 * 60,
    #     log_every=int(100_000 / 3),
    #     verbose=False,
    # ),
    ax=None,
):
    show = False
    if ax is None:
        show = True
        fig, ax = plt.subplots()

    base_mdp_parameters = deepcopy(base_mdp_parameters)

    if hyper_opt_hyper_params is not None:
        hyper_opt_hyper_params = SingleInstanceHyperparamOptParameters(
            mdp_class, base_mdp_parameters, **hyper_opt_hyper_params
        )

    df = get_varying_parameter_df(
        varying_parameter_name,
        varying_parameter_values,
        mdp_class,
        base_mdp_parameters,
        n_seeds,
        look_for_cache=look_for_cache,
        cache_folder=cache_folder,
        hyper_opt_hyper_params=hyper_opt_hyper_params,
    )

    measures = [&#34;diameter&#34;, &#34;value_norm&#34;, &#34;suboptimal_gaps&#34;, &#34;optimal_regret&#34;]
    df.fillna(0, inplace=True)
    for m in measures:
        if np.isclose(df.loc[:, m], df.loc[:, m].max(), atol=1e-3).all():
            df.loc[:, m] = 0.5
    df.loc[:, measures] = (df.loc[:, measures] - df.loc[:, measures].min(0)) / (
        df.loc[:, measures].max(0) - df.loc[:, measures].min(0)
    )
    df.fillna(0.5, inplace=True)

    linewidth = 2.5
    marker_size = 10
    sns.lineplot(
        x=varying_parameter_name,
        y=&#34;diameter&#34;,
        label=&#34;Diameter&#34;,
        data=df,
        ax=ax,
        marker=&#34;o&#34;,
        markersize=marker_size,
        linewidth=linewidth,
    )
    sns.lineplot(
        x=varying_parameter_name,
        y=&#34;value_norm&#34;,
        label=&#34;Environmental value norm&#34;,
        data=df,
        ax=ax,
        marker=&#34;^&#34;,
        markersize=marker_size,
        linewidth=linewidth,
    )
    sns.lineplot(
        x=varying_parameter_name,
        y=&#34;suboptimal_gaps&#34;,
        label=&#34;Sum  of  the  reciprocals \nof  the  sub-optimality  gaps&#34;,
        data=df,
        ax=ax,
        marker=&#34;D&#34;,
        markersize=marker_size,
        linewidth=linewidth,
    )
    if hyper_opt_hyper_params is not None:
        sns.lineplot(
            x=varying_parameter_name,
            y=&#34;optimal_regret&#34;,
            label=&#34;Cumulative regret&#34;,
            data=df,
            ax=ax,
            marker=&#34;X&#34;,
            markersize=marker_size,
            linewidth=linewidth,
        )
    ax.set_ylabel(&#34;Normalized measure of hardness&#34;)
    ax.set_xlabel(varying_parameter_name.capitalize())
    if show:
        plt.tight_layout()
        plt.show()


def get_varying_parameter_df(
    varying_parameter_name: str,
    varying_parameter_values: List,
    mdp_class,
    base_mdp_parameters,
    n_seeds: int,
    look_for_cache: bool = True,
    cache_folder: str = None,
    hyper_opt_hyper_params: SingleInstanceHyperparamOptParameters = None,
) -&gt; pd.DataFrame:
    base_mdp_parameters[&#34;randomize_actions&#34;] = False

    df = pd.DataFrame(
        columns=[
            &#34;MDP&#34;,
            varying_parameter_name,
            &#34;diameter&#34;,
            &#34;value_norm&#34;,
            &#34;suboptimal_gaps&#34;,
            &#34;optimal_regret&#34;,
            &#34;seed&#34;,
        ]
    )
    if cache_folder:
        cache_folder = f&#34;{ensure_folder(cache_folder)}{ensure_folder(mdp_class.__name__)}&#34;
        os.makedirs(cache_folder, exist_ok=True)
    for varying_p in tqdm(
        varying_parameter_values,
        desc=f&#34;{mdp_class.__name__} for varying {varying_parameter_name}.&#34;,
    ):
        base_mdp_parameters[varying_parameter_name] = varying_p

        # Compute the hyperparameters for the mdp class and parameters
        if hyper_opt_hyper_params is not None:
            agent_hyper_params_f = f&#34;{cache_folder}optimal_agent_hprs_{varying_parameter_name}_{varying_p}_{clean_for_file_path(str(clean_for_storing(base_mdp_parameters).items()))}.json&#34;
            agent_hyper_params_f = re.sub(r&#34;[^\w\-/_\. ]&#34;, &#34;_&#34;, agent_hyper_params_f)
            if look_for_cache and os.path.isfile(agent_hyper_params_f):
                with open(agent_hyper_params_f, &#34;r&#34;) as f:
                    agent_hyper_params = json.load(f)
            else:
                agent_hyper_params, _ = get_optimal_hyperparameters_for_single_instance(
                    *hyper_opt_hyper_params
                )
                # print(agent_hyper_params)
                with open(agent_hyper_params_f, &#34;w&#34;) as f:
                    json.dump(agent_hyper_params, f)
            # print(agent_hyper_params_f)

        for seed in range(n_seeds if mdp_class.does_seed_change_MDP_structure() else 1):
            diameter = compute_hardness_measure(
                mdp_class, dict(seed=seed, **base_mdp_parameters), &#34;diameter&#34;, cache_folder, look_for_cache
            )
            value_norm = compute_hardness_measure(
                mdp_class, dict(seed=seed, **base_mdp_parameters), &#34;value_norm&#34;, cache_folder, look_for_cache
            )
            suboptimal_gaps = compute_hardness_measure(
                mdp_class, dict(seed=seed, **base_mdp_parameters), &#34;suboptimal_gaps&#34;, cache_folder, look_for_cache
            )
            optimal_regret = None
            if hyper_opt_hyper_params is not None:
                optimal_regret = compute_hardness_measure(
                    mdp_class, dict(seed=seed, **base_mdp_parameters),
                    &#34;optimal_regret&#34;,
                    cache_folder,
                    look_for_cache,
                    agent_hyper_params,
                    hyper_opt_hyper_params,
                )
            df.loc[len(df)] = [
                mdp_class.__name__,
                varying_p,
                diameter,
                value_norm,
                suboptimal_gaps,
                optimal_regret,
                seed,
            ]
    return df


def compute_hardness_measure(
    mdp_class: Type[&#34;BaseMDP&#34;],
    mdp_kwargs : Dict[str, Any],
    measure: str,
    folder: str = None,
    look_for_cache: bool = True,
    agent_hyper_params: Dict[str, Any] = None,
    hyper_opt_hyper_params: SingleInstanceHyperparamOptParameters = None,
):
    &#34;&#34;&#34;
    returns the measure of hardness.

    Parameters
    ----------
    mdp : BaseMDP
        is the MDP for which the measure of hardness is computed.
    measure : str
        is the name of the measure of hardness.
    folder : str, optional
        is the folder where cached values are looked for and stored. By default, it is set to None.
    look_for_cache : bool, optional
        checks whether to looked for cached values in the folder.
    &#34;&#34;&#34;
    mdp = mdp_class(**mdp_kwargs, instantiate_mdp=mdp_class.is_episodic())
    measure_f = f&#34;{folder}{measure}_{mdp.hash}.txt&#34;
    if look_for_cache and os.path.isfile(measure_f):
        with open(measure_f, &#34;r&#34;) as f:
            measure = float(f.read())
        return measure

    if not mdp_class.is_episodic():
        mdp.instantiate_MDP()
    if measure == &#34;optimal_regret&#34;:
        assert agent_hyper_params is not None
        agent_class = PSRLEpisodic if mdp_class.is_episodic() else UCRL2Continuous
        agent = agent_class.get_agent_instance_from_hyperparameters(
            0,
            hyper_opt_hyper_params.optimization_horizon,
            make_environment_spec(mdp),
            agent_hyper_params,
        )
        compute_measure_f = lambda: get_regret_score(
            mdp,
            agent,
            hyper_opt_hyper_params.optimization_horizon,
            hyper_opt_hyper_params.max_interaction_s,
            log_every=hyper_opt_hyper_params.log_every,
            enforce_time_constraint=False,
        )
    else:
        mdp = mdp_class(**mdp_kwargs)
        compute_measure_f = lambda: mdp.get_measure_from_name(measure)

    if folder is None:
        return compute_measure_f()


    measure = compute_measure_f()
    os.makedirs(os.path.dirname(measure_f), exist_ok=True)
    with open(measure_f, &#34;w&#34;) as f:
        f.write(str(measure))
    return measure


# def tune_run(agent_next_hyperparams):
#     scores = []
#     for seed in range(n_seed):
#         mdp_parameters[&#34;seed&#34;] = seed
#         mdp = mdp_class(**mdp_parameters)
#         agent = agent_class.get_agent_instance_from_hyperparameters(
#             seed,
#             optimization_horizon,
#             make_environment_spec(mdp),
#             agent_next_hyperparams,
#         )
#         scores.append(
#             get_regret_score(
#                 mdp,
#                 agent,
#                 optimization_horizon,
#                 max_interaction_s,
#                 log_every=log_every,
#                 enforce_time_constraint=False,
#             )
#         )
#     # tune.report(regret=np.mean(scores))
#     return agent_next_hyperparams, np.mean(scores)


def run(x):
    (
        agent_next_hyperparams,
        n_seed,
        mdp_parameters,
        mdp_class,
        agent_class,
        optimization_horizon,
        max_interaction_s,
        log_every,
    ) = x

    scores = []
    for seed in range(n_seed):
        mdp_parameters[&#34;seed&#34;] = seed
        mdp = mdp_class(**mdp_parameters)
        agent = agent_class.get_agent_instance_from_hyperparameters(
            seed,
            optimization_horizon,
            make_environment_spec(mdp),
            agent_next_hyperparams,
        )
        scores.append(
            get_regret_score(
                mdp,
                agent,
                optimization_horizon,
                max_interaction_s,
                log_every=log_every,
                enforce_time_constraint=False,
            )
        )
    # tune.report(regret=np.mean(scores))
    return agent_next_hyperparams, np.mean(scores)


def get_optimal_hyperparameters_for_single_instance(
    mdp_class,
    mdp_parameters,
    n_cores: int,
    n_seed: int,
    num_samples: int,
    optimization_horizon: int,
    max_interaction_s: float,
    log_every: int,
    verbose: bool,
):

    agent_class = PSRLEpisodic if mdp_class.is_episodic() else UCRL2Continuous
    # ray.init(num_cpus=n_cores, log_to_driver=False)
    # analysis = tune.run(
    #     tune_run,
    #     config=agent_class.get_hyperparameters_search_spaces(),
    #     num_samples=num_samples,
    #     # verbose=verbose,
    # )
    # best_trial = analysis.get_best_trial(&#34;regret&#34;, mode=&#34;min&#34;)
    # ray.shutdown()

    gen = agent_hyperparameters_generator(
        42, agent_class.get_hyperparameters_search_spaces()
    )
    if verbose:
        loop = trange(num_samples, desc=&#34;Single instance hyperparameters optimization&#34;)
    best_score = np.inf
    best_hyprms = None
    rng = random.Random(42)
    with Pool(processes=n_cores) as p:
        for hprms, score in p.imap_unordered(
            run,
            list(
                (
                    next(gen),
                    n_seed,
                    mdp_parameters,
                    mdp_class,
                    agent_class,
                    optimization_horizon,
                    max_interaction_s,
                    log_every,
                )
                for _ in range(num_samples)
            ),
        ):
            if score &lt; best_score:
                best_hyprms = hprms
                best_score = score
            elif np.isclose(score, best_score):
                if rng.random() &gt; 0.5:
                    best_hyprms = hprms
                    best_score = score
            if verbose:
                loop.update(1)
                loop.set_description(f&#34;Current best score: {best_score:.2f}&#34;)

    # return best_trial.config, best_trial.last_result[&#34;regret&#34;]
    return best_hyprms, best_score</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="colosseum.hardness.analysis.utils.compute_hardness_measure"><code class="name flex">
<span>def <span class="ident">compute_hardness_measure</span></span>(<span>mdp_class: Type[ForwardRef('BaseMDP')], mdp_kwargs: Dict[str, Any], measure: str, folder: str = None, look_for_cache: bool = True, agent_hyper_params: Dict[str, Any] = None, hyper_opt_hyper_params: <a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters">SingleInstanceHyperparamOptParameters</a> = None)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the measure of hardness.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mdp</code></strong> :&ensp;<code>BaseMDP</code></dt>
<dd>is the MDP for which the measure of hardness is computed.</dd>
<dt><strong><code>measure</code></strong> :&ensp;<code>str</code></dt>
<dd>is the name of the measure of hardness.</dd>
<dt><strong><code>folder</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>is the folder where cached values are looked for and stored. By default, it is set to None.</dd>
<dt><strong><code>look_for_cache</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>checks whether to looked for cached values in the folder.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compute_hardness_measure(
    mdp_class: Type[&#34;BaseMDP&#34;],
    mdp_kwargs : Dict[str, Any],
    measure: str,
    folder: str = None,
    look_for_cache: bool = True,
    agent_hyper_params: Dict[str, Any] = None,
    hyper_opt_hyper_params: SingleInstanceHyperparamOptParameters = None,
):
    &#34;&#34;&#34;
    returns the measure of hardness.

    Parameters
    ----------
    mdp : BaseMDP
        is the MDP for which the measure of hardness is computed.
    measure : str
        is the name of the measure of hardness.
    folder : str, optional
        is the folder where cached values are looked for and stored. By default, it is set to None.
    look_for_cache : bool, optional
        checks whether to looked for cached values in the folder.
    &#34;&#34;&#34;
    mdp = mdp_class(**mdp_kwargs, instantiate_mdp=mdp_class.is_episodic())
    measure_f = f&#34;{folder}{measure}_{mdp.hash}.txt&#34;
    if look_for_cache and os.path.isfile(measure_f):
        with open(measure_f, &#34;r&#34;) as f:
            measure = float(f.read())
        return measure

    if not mdp_class.is_episodic():
        mdp.instantiate_MDP()
    if measure == &#34;optimal_regret&#34;:
        assert agent_hyper_params is not None
        agent_class = PSRLEpisodic if mdp_class.is_episodic() else UCRL2Continuous
        agent = agent_class.get_agent_instance_from_hyperparameters(
            0,
            hyper_opt_hyper_params.optimization_horizon,
            make_environment_spec(mdp),
            agent_hyper_params,
        )
        compute_measure_f = lambda: get_regret_score(
            mdp,
            agent,
            hyper_opt_hyper_params.optimization_horizon,
            hyper_opt_hyper_params.max_interaction_s,
            log_every=hyper_opt_hyper_params.log_every,
            enforce_time_constraint=False,
        )
    else:
        mdp = mdp_class(**mdp_kwargs)
        compute_measure_f = lambda: mdp.get_measure_from_name(measure)

    if folder is None:
        return compute_measure_f()


    measure = compute_measure_f()
    os.makedirs(os.path.dirname(measure_f), exist_ok=True)
    with open(measure_f, &#34;w&#34;) as f:
        f.write(str(measure))
    return measure</code></pre>
</details>
</dd>
<dt id="colosseum.hardness.analysis.utils.get_optimal_hyperparameters_for_single_instance"><code class="name flex">
<span>def <span class="ident">get_optimal_hyperparameters_for_single_instance</span></span>(<span>mdp_class, mdp_parameters, n_cores: int, n_seed: int, num_samples: int, optimization_horizon: int, max_interaction_s: float, log_every: int, verbose: bool)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_optimal_hyperparameters_for_single_instance(
    mdp_class,
    mdp_parameters,
    n_cores: int,
    n_seed: int,
    num_samples: int,
    optimization_horizon: int,
    max_interaction_s: float,
    log_every: int,
    verbose: bool,
):

    agent_class = PSRLEpisodic if mdp_class.is_episodic() else UCRL2Continuous
    # ray.init(num_cpus=n_cores, log_to_driver=False)
    # analysis = tune.run(
    #     tune_run,
    #     config=agent_class.get_hyperparameters_search_spaces(),
    #     num_samples=num_samples,
    #     # verbose=verbose,
    # )
    # best_trial = analysis.get_best_trial(&#34;regret&#34;, mode=&#34;min&#34;)
    # ray.shutdown()

    gen = agent_hyperparameters_generator(
        42, agent_class.get_hyperparameters_search_spaces()
    )
    if verbose:
        loop = trange(num_samples, desc=&#34;Single instance hyperparameters optimization&#34;)
    best_score = np.inf
    best_hyprms = None
    rng = random.Random(42)
    with Pool(processes=n_cores) as p:
        for hprms, score in p.imap_unordered(
            run,
            list(
                (
                    next(gen),
                    n_seed,
                    mdp_parameters,
                    mdp_class,
                    agent_class,
                    optimization_horizon,
                    max_interaction_s,
                    log_every,
                )
                for _ in range(num_samples)
            ),
        ):
            if score &lt; best_score:
                best_hyprms = hprms
                best_score = score
            elif np.isclose(score, best_score):
                if rng.random() &gt; 0.5:
                    best_hyprms = hprms
                    best_score = score
            if verbose:
                loop.update(1)
                loop.set_description(f&#34;Current best score: {best_score:.2f}&#34;)

    # return best_trial.config, best_trial.last_result[&#34;regret&#34;]
    return best_hyprms, best_score</code></pre>
</details>
</dd>
<dt id="colosseum.hardness.analysis.utils.get_varying_parameter_df"><code class="name flex">
<span>def <span class="ident">get_varying_parameter_df</span></span>(<span>varying_parameter_name: str, varying_parameter_values: List[~T], mdp_class, base_mdp_parameters, n_seeds: int, look_for_cache: bool = True, cache_folder: str = None, hyper_opt_hyper_params: <a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters">SingleInstanceHyperparamOptParameters</a> = None) ‑> pandas.core.frame.DataFrame</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_varying_parameter_df(
    varying_parameter_name: str,
    varying_parameter_values: List,
    mdp_class,
    base_mdp_parameters,
    n_seeds: int,
    look_for_cache: bool = True,
    cache_folder: str = None,
    hyper_opt_hyper_params: SingleInstanceHyperparamOptParameters = None,
) -&gt; pd.DataFrame:
    base_mdp_parameters[&#34;randomize_actions&#34;] = False

    df = pd.DataFrame(
        columns=[
            &#34;MDP&#34;,
            varying_parameter_name,
            &#34;diameter&#34;,
            &#34;value_norm&#34;,
            &#34;suboptimal_gaps&#34;,
            &#34;optimal_regret&#34;,
            &#34;seed&#34;,
        ]
    )
    if cache_folder:
        cache_folder = f&#34;{ensure_folder(cache_folder)}{ensure_folder(mdp_class.__name__)}&#34;
        os.makedirs(cache_folder, exist_ok=True)
    for varying_p in tqdm(
        varying_parameter_values,
        desc=f&#34;{mdp_class.__name__} for varying {varying_parameter_name}.&#34;,
    ):
        base_mdp_parameters[varying_parameter_name] = varying_p

        # Compute the hyperparameters for the mdp class and parameters
        if hyper_opt_hyper_params is not None:
            agent_hyper_params_f = f&#34;{cache_folder}optimal_agent_hprs_{varying_parameter_name}_{varying_p}_{clean_for_file_path(str(clean_for_storing(base_mdp_parameters).items()))}.json&#34;
            agent_hyper_params_f = re.sub(r&#34;[^\w\-/_\. ]&#34;, &#34;_&#34;, agent_hyper_params_f)
            if look_for_cache and os.path.isfile(agent_hyper_params_f):
                with open(agent_hyper_params_f, &#34;r&#34;) as f:
                    agent_hyper_params = json.load(f)
            else:
                agent_hyper_params, _ = get_optimal_hyperparameters_for_single_instance(
                    *hyper_opt_hyper_params
                )
                # print(agent_hyper_params)
                with open(agent_hyper_params_f, &#34;w&#34;) as f:
                    json.dump(agent_hyper_params, f)
            # print(agent_hyper_params_f)

        for seed in range(n_seeds if mdp_class.does_seed_change_MDP_structure() else 1):
            diameter = compute_hardness_measure(
                mdp_class, dict(seed=seed, **base_mdp_parameters), &#34;diameter&#34;, cache_folder, look_for_cache
            )
            value_norm = compute_hardness_measure(
                mdp_class, dict(seed=seed, **base_mdp_parameters), &#34;value_norm&#34;, cache_folder, look_for_cache
            )
            suboptimal_gaps = compute_hardness_measure(
                mdp_class, dict(seed=seed, **base_mdp_parameters), &#34;suboptimal_gaps&#34;, cache_folder, look_for_cache
            )
            optimal_regret = None
            if hyper_opt_hyper_params is not None:
                optimal_regret = compute_hardness_measure(
                    mdp_class, dict(seed=seed, **base_mdp_parameters),
                    &#34;optimal_regret&#34;,
                    cache_folder,
                    look_for_cache,
                    agent_hyper_params,
                    hyper_opt_hyper_params,
                )
            df.loc[len(df)] = [
                mdp_class.__name__,
                varying_p,
                diameter,
                value_norm,
                suboptimal_gaps,
                optimal_regret,
                seed,
            ]
    return df</code></pre>
</details>
</dd>
<dt id="colosseum.hardness.analysis.utils.plot_hardness_analysis"><code class="name flex">
<span>def <span class="ident">plot_hardness_analysis</span></span>(<span>varying_parameter_name: str, varying_parameter_values: Iterable[+T_co], mdp_class, base_mdp_parameters, n_seeds: int, look_for_cache: bool = True, cache_folder: str = 'hardness_measures_cache/', hyper_opt_hyper_params=None, ax=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def plot_hardness_analysis(
    varying_parameter_name: str,
    varying_parameter_values: Iterable,
    mdp_class,
    base_mdp_parameters,
    n_seeds: int,
    look_for_cache: bool = True,
    cache_folder: str = config.get_hardness_measures_cache_folder(),
    hyper_opt_hyper_params=None,
    # frozendict(
    #     n_cores=5,
    #     n_seed=2,
    #     num_samples=5,
    #     optimization_horizon=100_000,
    #     max_interaction_s=1 * 60,
    #     log_every=int(100_000 / 3),
    #     verbose=False,
    # ),
    ax=None,
):
    show = False
    if ax is None:
        show = True
        fig, ax = plt.subplots()

    base_mdp_parameters = deepcopy(base_mdp_parameters)

    if hyper_opt_hyper_params is not None:
        hyper_opt_hyper_params = SingleInstanceHyperparamOptParameters(
            mdp_class, base_mdp_parameters, **hyper_opt_hyper_params
        )

    df = get_varying_parameter_df(
        varying_parameter_name,
        varying_parameter_values,
        mdp_class,
        base_mdp_parameters,
        n_seeds,
        look_for_cache=look_for_cache,
        cache_folder=cache_folder,
        hyper_opt_hyper_params=hyper_opt_hyper_params,
    )

    measures = [&#34;diameter&#34;, &#34;value_norm&#34;, &#34;suboptimal_gaps&#34;, &#34;optimal_regret&#34;]
    df.fillna(0, inplace=True)
    for m in measures:
        if np.isclose(df.loc[:, m], df.loc[:, m].max(), atol=1e-3).all():
            df.loc[:, m] = 0.5
    df.loc[:, measures] = (df.loc[:, measures] - df.loc[:, measures].min(0)) / (
        df.loc[:, measures].max(0) - df.loc[:, measures].min(0)
    )
    df.fillna(0.5, inplace=True)

    linewidth = 2.5
    marker_size = 10
    sns.lineplot(
        x=varying_parameter_name,
        y=&#34;diameter&#34;,
        label=&#34;Diameter&#34;,
        data=df,
        ax=ax,
        marker=&#34;o&#34;,
        markersize=marker_size,
        linewidth=linewidth,
    )
    sns.lineplot(
        x=varying_parameter_name,
        y=&#34;value_norm&#34;,
        label=&#34;Environmental value norm&#34;,
        data=df,
        ax=ax,
        marker=&#34;^&#34;,
        markersize=marker_size,
        linewidth=linewidth,
    )
    sns.lineplot(
        x=varying_parameter_name,
        y=&#34;suboptimal_gaps&#34;,
        label=&#34;Sum  of  the  reciprocals \nof  the  sub-optimality  gaps&#34;,
        data=df,
        ax=ax,
        marker=&#34;D&#34;,
        markersize=marker_size,
        linewidth=linewidth,
    )
    if hyper_opt_hyper_params is not None:
        sns.lineplot(
            x=varying_parameter_name,
            y=&#34;optimal_regret&#34;,
            label=&#34;Cumulative regret&#34;,
            data=df,
            ax=ax,
            marker=&#34;X&#34;,
            markersize=marker_size,
            linewidth=linewidth,
        )
    ax.set_ylabel(&#34;Normalized measure of hardness&#34;)
    ax.set_xlabel(varying_parameter_name.capitalize())
    if show:
        plt.tight_layout()
        plt.show()</code></pre>
</details>
</dd>
<dt id="colosseum.hardness.analysis.utils.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run(x):
    (
        agent_next_hyperparams,
        n_seed,
        mdp_parameters,
        mdp_class,
        agent_class,
        optimization_horizon,
        max_interaction_s,
        log_every,
    ) = x

    scores = []
    for seed in range(n_seed):
        mdp_parameters[&#34;seed&#34;] = seed
        mdp = mdp_class(**mdp_parameters)
        agent = agent_class.get_agent_instance_from_hyperparameters(
            seed,
            optimization_horizon,
            make_environment_spec(mdp),
            agent_next_hyperparams,
        )
        scores.append(
            get_regret_score(
                mdp,
                agent,
                optimization_horizon,
                max_interaction_s,
                log_every=log_every,
                enforce_time_constraint=False,
            )
        )
    # tune.report(regret=np.mean(scores))
    return agent_next_hyperparams, np.mean(scores)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams"><code class="flex name class">
<span>class <span class="ident">HardnessAnalysisParams</span></span>
<span>(</span><span>mdp_base_params: Dict[str, Any], sizes: Iterable[int], ps: Iterable[float], n_seeds: int, mdp_class_name: str = None, optimal_agent_optimization: Dict[~KT, ~VT] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>HardnessAnalysisParams(mdp_base_params: Dict[str, Any], sizes: Iterable[int], ps: Iterable[float], n_seeds: int, mdp_class_name: str = None, optimal_agent_optimization: Dict = None)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HardnessAnalysisParams:
    mdp_base_params: Dict[str, Any]
    sizes: Iterable[int]
    ps: Iterable[float]
    n_seeds: int
    mdp_class_name: str = None
    optimal_agent_optimization: Dict = None</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams.mdp_base_params"><code class="name">var <span class="ident">mdp_base_params</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams.mdp_class_name"><code class="name">var <span class="ident">mdp_class_name</span> : str</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams.n_seeds"><code class="name">var <span class="ident">n_seeds</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams.optimal_agent_optimization"><code class="name">var <span class="ident">optimal_agent_optimization</span> : Dict[~KT, ~VT]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams.ps"><code class="name">var <span class="ident">ps</span> : Iterable[float]</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.HardnessAnalysisParams.sizes"><code class="name">var <span class="ident">sizes</span> : Iterable[int]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters"><code class="flex name class">
<span>class <span class="ident">SingleInstanceHyperparamOptParameters</span></span>
<span>(</span><span>mdp_class: BaseMDP, mdp_parameters: Dict[str, Any], n_cores: int, n_seed: int, num_samples: int, optimization_horizon: int, max_interaction_s: float, log_every: int, verbose: bool)</span>
</code></dt>
<dd>
<div class="desc"><p>SingleInstanceHyperparamOptParameters(mdp_class, mdp_parameters, n_cores, n_seed, num_samples, optimization_horizon, max_interaction_s, log_every, verbose)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SingleInstanceHyperparamOptParameters(NamedTuple):
    mdp_class: &#34;BaseMDP&#34;
    mdp_parameters: Dict[str, Any]
    n_cores: int
    n_seed: int
    num_samples: int
    optimization_horizon: int
    max_interaction_s: float
    log_every: int
    verbose: bool</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>builtins.tuple</li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.log_every"><code class="name">var <span class="ident">log_every</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 7</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.max_interaction_s"><code class="name">var <span class="ident">max_interaction_s</span> : float</code></dt>
<dd>
<div class="desc"><p>Alias for field number 6</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.mdp_class"><code class="name">var <span class="ident">mdp_class</span> : BaseMDP</code></dt>
<dd>
<div class="desc"><p>Alias for field number 0</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.mdp_parameters"><code class="name">var <span class="ident">mdp_parameters</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"><p>Alias for field number 1</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.n_cores"><code class="name">var <span class="ident">n_cores</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 2</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.n_seed"><code class="name">var <span class="ident">n_seed</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 3</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.num_samples"><code class="name">var <span class="ident">num_samples</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 4</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.optimization_horizon"><code class="name">var <span class="ident">optimization_horizon</span> : int</code></dt>
<dd>
<div class="desc"><p>Alias for field number 5</p></div>
</dd>
<dt id="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.verbose"><code class="name">var <span class="ident">verbose</span> : bool</code></dt>
<dd>
<div class="desc"><p>Alias for field number 8</p></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="colosseum.hardness.analysis" href="index.html">colosseum.hardness.analysis</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="colosseum.hardness.analysis.utils.compute_hardness_measure" href="#colosseum.hardness.analysis.utils.compute_hardness_measure">compute_hardness_measure</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.get_optimal_hyperparameters_for_single_instance" href="#colosseum.hardness.analysis.utils.get_optimal_hyperparameters_for_single_instance">get_optimal_hyperparameters_for_single_instance</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.get_varying_parameter_df" href="#colosseum.hardness.analysis.utils.get_varying_parameter_df">get_varying_parameter_df</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.plot_hardness_analysis" href="#colosseum.hardness.analysis.utils.plot_hardness_analysis">plot_hardness_analysis</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.run" href="#colosseum.hardness.analysis.utils.run">run</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams">HardnessAnalysisParams</a></code></h4>
<ul class="">
<li><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams.mdp_base_params" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams.mdp_base_params">mdp_base_params</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams.mdp_class_name" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams.mdp_class_name">mdp_class_name</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams.n_seeds" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams.n_seeds">n_seeds</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams.optimal_agent_optimization" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams.optimal_agent_optimization">optimal_agent_optimization</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams.ps" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams.ps">ps</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.HardnessAnalysisParams.sizes" href="#colosseum.hardness.analysis.utils.HardnessAnalysisParams.sizes">sizes</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters">SingleInstanceHyperparamOptParameters</a></code></h4>
<ul class="">
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.log_every" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.log_every">log_every</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.max_interaction_s" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.max_interaction_s">max_interaction_s</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.mdp_class" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.mdp_class">mdp_class</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.mdp_parameters" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.mdp_parameters">mdp_parameters</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.n_cores" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.n_cores">n_cores</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.n_seed" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.n_seed">n_seed</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.num_samples" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.num_samples">num_samples</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.optimization_horizon" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.optimization_horizon">optimization_horizon</a></code></li>
<li><code><a title="colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.verbose" href="#colosseum.hardness.analysis.utils.SingleInstanceHyperparamOptParameters.verbose">verbose</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>