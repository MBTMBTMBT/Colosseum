<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>colosseum.mdp.base API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>colosseum.mdp.base</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
import os
import random
import sys
from typing import TYPE_CHECKING, Any, Dict, Iterable, List, Tuple, Type, Union

import dm_env
import networkx as nx
import numpy as np
import yaml
from dm_env.specs import BoundedArray, Array
from pydtmc import MarkovChain
from scipy.stats import rv_continuous

from colosseum.dynamic_programming import (
    discounted_policy_iteration,
    discounted_value_iteration,
)
from colosseum.dynamic_programming.infinite_horizon import discounted_policy_evaluation
from colosseum.dynamic_programming.utils import get_policy_from_q_values
from colosseum.hardness.measures import (
    calculate_norm_average,
    calculate_norm_discounted,
    find_hardness_report_file,
    get_diameter,
    get_sum_reciprocals_suboptimality_gaps,
)
from colosseum.mdp.utils.communication_class import (
    MDPCommunicationClass,
    get_communication_class,
    get_recurrent_nodes_set,
)
from colosseum.mdp.utils.custom_samplers import NextStateSampler
from colosseum.mdp.utils.markov_chain import (
    get_average_rewards,
    get_markov_chain,
    get_stationary_distribution,
    get_transition_probabilities,
)
from colosseum.mdp.utils.mdp_creation import (
    NodeInfoClass,
    get_transition_matrix_and_rewards,
    instantiate_transitions,
)
from colosseum.mdp.utils.state_representation import (
    StateRepresentationType,
    RepresentationMapping,
)
from colosseum.utils import clean_for_storing
from colosseum.utils.acme.specs import DiscreteArray
from colosseum.utils.formatter import clean_for_file_path

if TYPE_CHECKING:
    from colosseum.mdp import ACTION_TYPE, NODE_TYPE, ContinuousMDP, EpisodicMDP

sys.setrecursionlimit(5000)


class BaseMDP(dm_env.Environment, abc.ABC):
    @staticmethod
    def get_available_hardness_measures() -&gt; List[str]:
        return [&#34;diameter&#34;, &#34;value_norm&#34;, &#34;suboptimal_gaps&#34;]

    @staticmethod
    @abc.abstractmethod
    def does_seed_change_MDP_structure() -&gt; bool:
        &#34;&#34;&#34;
        returns True if when changing the seed the transition matrix and/or rewards matrix change. This for example may
        happen when there are fewer starting states that possible one and the effective starting states are picked
        randomly based on the seed.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def is_episodic() -&gt; bool:
        &#34;&#34;&#34;
        returns whether the MDP is episodic.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def sample_parameters(n: int, seed: int = None) -&gt; List[Dict[str, Any]]:
        &#34;&#34;&#34;
        returns n sampled parameters that can be used to construct an MDP in a reasonable amount of time.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def _sample_parameters(
        n: int, is_episodic: bool, seed: int = None
    ) -&gt; List[Dict[str, Any]]:
        &#34;&#34;&#34;
        returns n sampled parameters that can be used to construct an MDP in a reasonable amount of time.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def get_node_class() -&gt; Type[&#34;NODE_TYPE&#34;]:
        &#34;&#34;&#34;
        returns the class of the nodes of the MDP.
        &#34;&#34;&#34;

    @property
    @abc.abstractmethod
    def n_actions(self) -&gt; int:
        &#34;&#34;&#34;
        returns the number of available actions.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_next_nodes_parameters(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;
    ) -&gt; Tuple[Tuple[dict, float], ...]:
        &#34;&#34;&#34;
        returns the parameters of the possible next nodes reachable from the given node and action.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_reward_distribution(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ) -&gt; rv_continuous:
        &#34;&#34;&#34;
        returns the distribution over rewards in the zero one interval when transitioning from a given node and action
        to a given next node. Note that rescaling the rewards to a different range is handled separately.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_starting_node_sampler(self) -&gt; NextStateSampler:
        &#34;&#34;&#34;
        returns a sampler over next states that corresponds to the starting state distribution.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _check_parameters_in_input(self):
        &#34;&#34;&#34;
        checks whether the parameters given in input can produce a correct MDP instance.
        &#34;&#34;&#34;
        assert self._p_rand is None or (0 &lt; self._p_rand &lt; 0.9999)
        assert self._p_lazy is None or (0 &lt; self._p_lazy &lt; 0.9999)

    @abc.abstractmethod
    def get_grid_representation(self, node: &#34;NODE_TYPE&#34;, h: int = None):
        &#34;&#34;&#34;
        produces an ASCII representation of the node given in input.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_grid_representation(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        produces an ASCII representation of the node given in input.
        &#34;&#34;&#34;

    @property
    @abc.abstractmethod
    def _possible_starting_nodes(self) -&gt; List[&#34;NODE_TYPE&#34;]:
        &#34;&#34;&#34;
        returns a list containing all the possible starting node for this MDP instance.
        &#34;&#34;&#34;

    @property
    @abc.abstractmethod
    def parameters(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        returns the parameters of the MDP.
        &#34;&#34;&#34;
        return dict(
            seed=self._seed,
            randomize_actions=self._randomize_actions,
            p_lazy=self._p_lazy,
            p_rand=self._p_rand,
            rewards_range=self._rewards_range,
            make_reward_stochastic=self._make_reward_stochastic,
            variance_multipliers=self._variance_multipliers,
        )

    @property
    def hash(self) -&gt; str:
        &#34;&#34;&#34;
        returns a hash value based on the parameters of the MDP. This can be use to create cache files.
        &#34;&#34;&#34;
        s = &#34;_&#34;.join(map(str, clean_for_storing(list(self.parameters.values()))))
        return f&#34;mdp_{type(self).__name__}_&#34; + clean_for_file_path(s)

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;
        returns a string containing all the information about the MDP including parameters, measures of hardness and
        graph metrics.
        &#34;&#34;&#34;
        string = type(self).__name__ + &#34;\n&#34;
        m_l = 0
        for k, v in self.summary.items():
            m_l = max(m_l, len(max(v.keys(), key=len)) + 4)
        for k, v in self.summary.items():
            string += &#34;\t&#34; + k + &#34;\n&#34;
            for kk, vv in v.items():
                string += f&#34;\t\t{kk}{&#39; &#39; * (m_l - len(kk))}:\t{vv}\n&#34;
        return string

    @abc.abstractmethod
    def __init__(
        self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;],
        seed: int,
        randomize_actions: bool = True,
        make_reward_stochastic=False,
        variance_multipliers: float = 1.0,
        p_lazy: float = None,
        p_rand: float = None,
        rewards_range: Tuple[float, float] = (0.0, 1.0),
        representation_mapping: Type[RepresentationMapping] = None,
        representation_mapping_kwargs: Dict[str, Any] = dict(),
        hardness_reports_folder=&#34;hardness_reports&#34; + os.sep,
        instantiate_mdp: bool = True,
        force_sparse_transition : bool = False
    ):
        &#34;&#34;&#34;
        instantiates the MDP.

        Parameters
        ----------
        seed : int
            is the random seed.
        randomize_actions : bool, optional
            checks whether to apply a random mapping to the actions for each state. This avoids issues linked to
            the possible bias of the agents to always take action zero at the beginning of the interactions.
            By default, it is set to true.
        variance_multipliers : float, optional
            A constant that can be used to increase the variance of the reward distributions without changing their means.
            The lower the value, the higher the variance. By default, it is set to 1.
        p_lazy : float, optional
            is the probability of an action not producing any effect on the MDP.
            By default, it is set to zero.
        p_rand : float, optional
            is the probability of selecting an action at random instead of the one specified by the agent.
            By default, it is set to zero.
        rewards_range : Tuple[float, float], optional
            is the maximum value of the reward.
            By default, it is set to the zero one interval.
        representation_mapping : RepresentationMapping
            the representation mapping assigned to each state. By default, no representation mapping is used.
        hardness_reports_folder : str, optional
            is the path where the MDP looks for previously cached hardness reports.
        instantiate_mdp : bool
            checks whether to immediately instantiate the MDP.
        &#34;&#34;&#34;

        # MDP generic variables
        self._seed = seed
        self._randomize_actions = randomize_actions
        self._make_reward_stochastic = make_reward_stochastic
        self._variance_multipliers = variance_multipliers
        if representation_mapping is not None:
            representation_mapping = representation_mapping(
                self, **representation_mapping_kwargs
            )
        self._representation_mapping = representation_mapping
        self._hardness_reports_folder = hardness_reports_folder
        self._force_sparse_transition = force_sparse_transition
        self._p_rand = p_rand if p_rand is None or p_rand &gt; 0.0 else None
        self._p_lazy = p_lazy if p_lazy is None or p_lazy &gt; 0.0 else None
        self.rewards_range = self._rewards_range = (
            rewards_range
            if rewards_range[0] &lt; rewards_range[1]
            else rewards_range[::-1]
        )
        self._are_all_rewards_deterministic = True
        self._are_all_transition_deterministic = True
        self.r_min, self.r_max = self.rewards_range

        # MDP loop variables
        self._hr = None
        self.cur_node = None
        self.last_edge = None
        self.last_starting_node = None
        self.is_reset_necessary = True
        self.current_timestep = 0
        self._rng = np.random.RandomState(seed)
        self._fast_rng = random.Random(seed)

        # Caching variables. Note that we cannot use lru_cache because it prevents from deleting the objects.
        self._cached_rewards = dict()
        self._cached_reward_distributions = dict()
        self._action_mapping = dict()
        self._communication_class = None
        self._recurrent_nodes_set = None
        if not hasattr(self, &#34;_transition_matrix_and_rewards&#34;):
            self._transition_matrix_and_rewards = None
        self._graph_layout = None
        self._graph_metrics = None
        self._summary = None
        self._diameter = None
        self._sum_reciprocals_suboptimality_gaps = None
        self._optimal_value_norm = dict()
        self._optimal_value = None
        self._worst_value = None
        self._random_value = None
        self._optimal_policy = dict()
        self._worst_policy = dict()
        self._otp = None
        self._omc = None
        self._osd = None
        self._oars = None
        self._oar = None
        self._wtp = None
        self._wmc = None
        self._wsd = None
        self._wars = None
        self._war = None
        self._rtp = None
        self._rmc = None
        self._rsd = None
        self._rars = None
        self._rar = None

        if instantiate_mdp:
            self.instantiate_MDP()

    def instantiate_MDP(self):
        # Instantiating the MDP
        self._check_parameters_in_input()
        self._starting_node_sampler = self._get_starting_node_sampler()
        self.starting_nodes = self._starting_node_sampler.next_nodes
        self.G = nx.DiGraph()
        self._instantiate_mdp()
        self.n_states = len(self.G.nodes)

        # Some shortcuts variables
        if not self.is_episodic():
            self._vi, self._pe = (
                discounted_value_iteration,
                discounted_policy_evaluation,
            )
        self.random_policy = (
            np.ones((self.n_states, self.n_actions), dtype=np.float32) / self.n_actions
        )

        # Cache node to index mapping
        mapping = self._rng.rand(self.n_states, self.n_actions).argsort(1)
        self.node_to_index = dict()
        self.index_to_node = dict()
        for i, node in enumerate(self.G.nodes):
            self.node_to_index[node] = i
            self.index_to_node[i] = node

        # Compute the starting state distribution
        self.starting_distribution = np.zeros(self.n_states)
        self.starting_states = []
        for n, p in self._starting_node_sampler.next_nodes_and_probs:
            s = self.node_to_index[n]
            self.starting_distribution[s] = p
            self.starting_states.append(s)
        self.starting_states_and_probs = list(
            zip(self.starting_states, self._starting_node_sampler.probs)
        )

    def _get_action_mapping(self, node: &#34;NODE_TYPE&#34;) -&gt; Tuple[&#34;ACTION_TYPE&#34;, ...]:
        &#34;&#34;&#34;
        returns the random action mapping for the node given in input.
        &#34;&#34;&#34;
        if node not in self._action_mapping:
            self._action_mapping[node] = (
                self._rng.rand(self.n_actions).argsort().tolist()
                if self._randomize_actions
                else list(range(self.n_actions))
            )
        return self._action_mapping[node]

    def _inverse_action_mapping(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;
    ) -&gt; &#34;ACTION_TYPE&#34;:
        &#34;&#34;&#34;
        returns the effective action corresponding to the action given in input in the given node. In other words, it
        reverses the random action mapping.
        &#34;&#34;&#34;
        return self._get_action_mapping(node)[action]

    def _produce_random_seed(self) -&gt; int:
        &#34;&#34;&#34;
        returns a new random seed that can be used for internal objects.
        &#34;&#34;&#34;
        return self._fast_rng.randint(0, 10_000)

    def _instantiate_mdp(self):
        &#34;&#34;&#34;
        recursively instantiate the MDP.
        &#34;&#34;&#34;
        for sn in self.starting_nodes:
            instantiate_transitions(self, sn)

    @property
    def T(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the transition matrix.
        &#34;&#34;&#34;
        return self.transition_matrix_and_rewards[0]

    @property
    def R(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the rewards matrix.
        &#34;&#34;&#34;
        return self.transition_matrix_and_rewards[1]

    @property
    def recurrent_nodes_set(self) -&gt; Iterable[&#34;NODE_TYPE&#34;]:
        &#34;&#34;&#34;
        returns the recurrent node set.
        &#34;&#34;&#34;
        if self._recurrent_nodes_set is None:
            self._recurrent_nodes_set = get_recurrent_nodes_set(
                self.communication_class, self.G
            )
        return self._recurrent_nodes_set

    @property
    def communication_class(
        self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
    ) -&gt; MDPCommunicationClass:
        &#34;&#34;&#34;
        returns the communication class.
        &#34;&#34;&#34;
        if self._communication_class is None:
            self._communication_class = get_communication_class(
                self.T, self.get_episodic_graph(True) if self.is_episodic() else self.G
            )
        return self._communication_class

    def get_optimal_policy(self, stochastic_form: bool) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the optimal policy. It can be either in the stochastic form, so in the form of dirac probabaility
        distribution or as a simple mapping to integer.
        &#34;&#34;&#34;
        if stochastic_form not in self._optimal_policy:
            self._optimal_policy[stochastic_form] = get_policy_from_q_values(
                self.optimal_value[0], stochastic_form
            )
        return self._optimal_policy[stochastic_form]

    def get_worst_policy(self, stochastic_form) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the worst performing policy. It can be either in the stochastic form, so in the form of dirac probabaility
        distribution or as a simple mapping to integer.
        &#34;&#34;&#34;
        if stochastic_form not in self._worst_policy:
            self._worst_policy[stochastic_form] = get_policy_from_q_values(
                self._vi(self.T, -self.R)[0], stochastic_form
            )
        return self._worst_policy[stochastic_form]

    @property
    def optimal_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the optimal q and state values.
        &#34;&#34;&#34;
        if self._optimal_value is None:
            self._optimal_value = self._vi(*self.transition_matrix_and_rewards)
        return self._optimal_value

    @property
    def worst_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the q and state values for the worst performing policy.
        &#34;&#34;&#34;
        if self._worst_value is None:
            self._worst_value = self._pe(
                *self.transition_matrix_and_rewards, self.get_worst_policy(True)
            )
        return self._worst_value

    @property
    def random_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the q and state values for the randomly acting policy.
        &#34;&#34;&#34;
        if self._random_value is None:
            self._random_value = self._pe(
                *self.transition_matrix_and_rewards, self.random_policy
            )
        return self._random_value

    @property
    def optimal_transition_probabilities(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the transition probabilities corresponding to the optimal policy.
        &#34;&#34;&#34;
        if self._otp is None:
            T = self.T_cf if self.is_episodic() else self.T
            pi = (
                self.get_optimal_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_optimal_policy(True)
            )
            self._otp = get_transition_probabilities(T, pi)
        return self._otp

    @property
    def worst_transition_probabilities(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the transition probabilities corresponding to the worst performing policy.
        &#34;&#34;&#34;
        if self._wtp is None:
            T = self.T_cf if self.is_episodic() else self.T
            pi = (
                self.get_worst_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_worst_policy(True)
            )
            self._wtp = get_transition_probabilities(T, pi)
        return self._wtp

    @property
    def random_transition_probabilities(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the transition probabilities corresponding to the randomly acting policy.
        &#34;&#34;&#34;
        if self._rtp is None:
            T = self.T_cf if self.is_episodic() else self.T
            pi = self.random_policy_cf if self.is_episodic() else self.random_policy
            self._rtp = get_transition_probabilities(T, pi)
        return self._rtp

    @property
    def optimal_markov_chain(self) -&gt; MarkovChain:
        &#34;&#34;&#34;
        returns the Markov chain corresponding to the optimal policy.
        &#34;&#34;&#34;
        if self._omc is None:
            self._omc = get_markov_chain(self.optimal_transition_probabilities)
        return self._omc

    @property
    def worst_markov_chain(self) -&gt; MarkovChain:
        &#34;&#34;&#34;
        returns the Markov chain corresponding to the worst performing policy.
        &#34;&#34;&#34;
        if self._wmc is None:
            self._wmc = get_markov_chain(self.worst_transition_probabilities)
        return self._wmc

    @property
    def random_markov_chain(self) -&gt; MarkovChain:
        &#34;&#34;&#34;
        returns the Markov chain corresponding to the randomly acting policy.
        &#34;&#34;&#34;
        if self._rmc is None:
            self._rmc = get_markov_chain(self.random_transition_probabilities)
        return self._rmc

    @property
    def optimal_stationary_distribution(self) -&gt; np.array:
        &#34;&#34;&#34;
        returns the stationary distribution yielded by the optimal policy.
        &#34;&#34;&#34;
        if self._osd is None:
            self._osd = get_stationary_distribution(
                self.optimal_transition_probabilities,
                self.starting_states_and_probs,
            )
        return self._osd

    @property
    def worst_stationary_distribution(self) -&gt; np.array:
        &#34;&#34;&#34;
        returns the stationary distribution yielded by the worst performing policy.
        &#34;&#34;&#34;
        if self._wsd is None:
            self._wsd = get_stationary_distribution(
                self.worst_transition_probabilities,
                self.starting_states_and_probs,
            )
            assert not np.isnan(self._wsd).any()
        return self._wsd

    @property
    def random_stationary_distribution(self) -&gt; np.array:
        &#34;&#34;&#34;
        returns the stationary distribution yielded by the randomly acting policy.
        &#34;&#34;&#34;
        if self._rsd is None:
            self._rsd = get_stationary_distribution(
                self.random_transition_probabilities,
                None,
            )
        return self._rsd

    @property
    def optimal_average_rewards(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the expected rewards obtained at each state when following the optimal policy.
        &#34;&#34;&#34;
        if self._oars is None:
            R = self.R_cf if self.is_episodic() else self.R
            pi = (
                self.get_optimal_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_optimal_policy(True)
            )
            self._oars = get_average_rewards(R, pi)
        return self._oars

    @property
    def worst_average_rewards(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the expected rewards obtained at each state when following the worst performing policy.
        &#34;&#34;&#34;
        if self._wars is None:
            R = self.R_cf if self.is_episodic() else self.R
            pi = (
                self.get_worst_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_worst_policy(True)
            )
            self._wars = get_average_rewards(R, pi)
        return self._wars

    @property
    def random_average_rewards(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the expected rewards obtained at each state when following the randomly acting policy.
        &#34;&#34;&#34;
        if self._rars is None:
            R = self.R_cf if self.is_episodic() else self.R
            pi = self.random_policy_cf if self.is_episodic() else self.random_policy
            self._rars = get_average_rewards(R, pi)
        return self._rars

    @property
    def optimal_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected average reward obtained when following the optimal policy.
        &#34;&#34;&#34;
        if self._oar is None:
            self._oar = sum(
                self.optimal_stationary_distribution * self.optimal_average_rewards
            )
        return self._oar

    @property
    def worst_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected average reward obtained when following the worst performing policy.
        &#34;&#34;&#34;
        if self._war is None:
            self._war = sum(
                self.worst_stationary_distribution * self.worst_average_rewards
            )
        return self._war

    @property
    def random_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected average reward obtained when following the randomly acting policy.
        &#34;&#34;&#34;
        if self._rar is None:
            self._rar = sum(
                self.random_stationary_distribution * self.random_average_rewards
            )
        return self._rar

    @property
    def transition_matrix_and_rewards(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the transition probabilities matrix and the reward matrix.
        &#34;&#34;&#34;
        if self._transition_matrix_and_rewards is None:
            self._transition_matrix_and_rewards = get_transition_matrix_and_rewards(
                self.n_states,
                self.n_actions,
                self.G,
                self.get_info_class,
                self.get_reward_distribution,
                self.node_to_index,
                self._force_sparse_transition
            )
        return self._transition_matrix_and_rewards

    @property
    def graph_layout(self) -&gt; Dict[&#34;NODE_TYPE&#34;, Tuple[float, float]]:
        &#34;&#34;&#34;
        returns a graph layout for the MDP. It can be customized by implementing the custom_graph_layout function.
        &#34;&#34;&#34;
        if self._graph_layout is None:
            self._graph_layout = (
                self.custom_graph_layout()
                if hasattr(self, &#34;custom_graph_layout&#34;)
                else nx.nx_agraph.graphviz_layout(self.G)
            )
        return self._graph_layout

    @property
    def graph_metrics(self) -&gt; dict:
        &#34;&#34;&#34;
        returns a dictionary containing graph metric for the MDP graph.
        &#34;&#34;&#34;
        if self._graph_metrics is None:
            G = self.get_episodic_graph(True) if self.is_episodic() else self.G
            self._graph_metrics = dict()
            self._graph_metrics[&#34;# nodes&#34;] = len(G.nodes)
            self._graph_metrics[&#34;# edges&#34;] = len(G.edges)
        return self._graph_metrics

    @property
    def diameter(self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]) -&gt; float:
        &#34;&#34;&#34;
        returns the diameter of the MDP.
        &#34;&#34;&#34;
        if self._diameter is None:
            if self.hardness_report:
                self._diameter = self.hardness_report[&#34;MDP measure of hardness&#34;][
                    &#34;diameter&#34;
                ]
            else:
                self._diameter = get_diameter(
                    self.episodic_transition_matrix_and_rewards[0]
                    if self.is_episodic()
                    else self.T,
                    self.is_episodic(),
                )
        return self._diameter

    @property
    def sum_reciprocals_suboptimality_gaps(
        self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
    ) -&gt; float:
        &#34;&#34;&#34;
        returns the sum of the reciprocals of the sub-optimality gaps.
        &#34;&#34;&#34;
        if self._sum_reciprocals_suboptimality_gaps is None:
            if self.hardness_report:
                self._sum_reciprocals_suboptimality_gaps = self.hardness_report[
                    &#34;MDP measure of hardness&#34;
                ][&#34;suboptimal_gaps&#34;]
            else:
                self._sum_reciprocals_suboptimality_gaps = (
                    get_sum_reciprocals_suboptimality_gaps(
                        *self.optimal_value,
                        self.reachable_states if self.is_episodic() else None,
                    )
                )
        return self._sum_reciprocals_suboptimality_gaps

    def _compute_value_norm(self, discounted: bool) -&gt; float:
        &#34;&#34;&#34;
        returns the environmental value norm in the undiscounted or undiscounted setting.
        &#34;&#34;&#34;
        T, R = (self.T_cf, self.R_cf) if self.is_episodic() else (self.T, self.R)
        V = (
            self.optimal_value_continuous_form[1]
            if self.is_episodic()
            else self.optimal_value[1]
        )
        if discounted:
            return calculate_norm_discounted(T, V)
        return calculate_norm_average(
            T, self.optimal_transition_probabilities, self.optimal_average_rewards
        )

    @property
    def discounted_value_norm(self) -&gt; float:
        &#34;&#34;&#34;
        returns the discounted environmental value norm.
        &#34;&#34;&#34;
        if True not in self._optimal_value_norm:
            if (
                self._are_all_transition_deterministic
                and self._are_all_rewards_deterministic
            ):
                self._optimal_value_norm[True] = 0.0
            elif self.hardness_report:
                self._optimal_value_norm[True] = self.hardness_report[
                    &#34;MDP measure of hardness&#34;
                ][&#34;value_norm&#34;]
            else:
                self._optimal_value_norm[True] = self._compute_value_norm(True)
        return self._optimal_value_norm[True]

    @property
    def undiscounted_value_norm(self) -&gt; float:
        &#34;&#34;&#34;
        returns the undiscounted environmental value norm.
        &#34;&#34;&#34;
        if False not in self._optimal_value_norm:
            self._optimal_value_norm[False] = self._compute_value_norm(False)
        return self._optimal_value_norm[False]

    @property
    def value_norm(self):
        &#34;&#34;&#34;
        is an alias for the discounted value norm.
        &#34;&#34;&#34;
        return self.discounted_value_norm

    @property
    def measures_of_hardness(self) -&gt; Dict[str, float]:
        &#34;&#34;&#34;
        returns a dictionary containing all the measures of hardness availble.
        &#34;&#34;&#34;
        return dict(
            diameter=self.diameter,
            suboptimal_gaps=self.sum_reciprocals_suboptimality_gaps,
            value_norm=self.value_norm,
        )

    @property
    def summary(self) -&gt; Dict[str, Dict[str, Any]]:
        &#34;&#34;&#34;
        returns a dictionary with information about the parameters, the measures of hardness and the graph metrics.
        &#34;&#34;&#34;
        if self._summary is None:
            self._summary = {
                &#34;Parameters&#34;: clean_for_storing(self.parameters),
                &#34;Measure of hardness&#34;: clean_for_storing(self.measures_of_hardness),
                &#34;Graph metrics&#34;: clean_for_storing(self.graph_metrics),
            }
        return self._summary

    @property
    def hardness_report(self) -&gt; Union[Dict, None]:
        &#34;&#34;&#34;
        looks for a cached hardness report in the folder given in input when the MDP was created. It returns None if it
        is not able to find it.
        &#34;&#34;&#34;
        if self._hr is None:
            report_file = find_hardness_report_file(self, self._hardness_reports_folder)
            if report_file:
                with open(report_file, &#34;r&#34;) as f:
                    report = yaml.load(f, yaml.Loader)
                self._hr = report
            else:
                self._hr = False
        if self._hr:
            return self._hr
        return None

    def get_info_class(self, n: &#34;NODE_TYPE&#34;) -&gt; NodeInfoClass:
        &#34;&#34;&#34;
        returns the container class (NodeInfoClass) associated with node n.
        &#34;&#34;&#34;
        return self.G.nodes[n][&#34;info_class&#34;]

    def get_transition_distributions(
        self, node: &#34;NODE_TYPE&#34;
    ) -&gt; Dict[int, Union[rv_continuous, NextStateSampler]]:
        &#34;&#34;&#34;
        returns a dictionary containing the transition distributions for any action at the node given in input.
        &#34;&#34;&#34;
        return self.get_info_class(node).transition_distributions

    def get_reward_distribution(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ):
        &#34;&#34;&#34;
        returns the reward distribution for transitioning in next_node when selecting action from node.
        &#34;&#34;&#34;
        if (node, action, next_node) not in self._cached_reward_distributions:
            self._cached_reward_distributions[
                node, action, next_node
            ] = self._get_reward_distribution(
                node, self._inverse_action_mapping(node, action), next_node
            )
        return self._cached_reward_distributions[node, action, next_node]

    def sample_reward(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ) -&gt; float:
        &#34;&#34;&#34;
        returns a sample from the reward distribution when transitioning in next_node when selecting action from node.
        &#34;&#34;&#34;
        if (node, action, next_node) not in self._cached_rewards or len(
            self._cached_rewards[node, action, next_node]
        ) == 0:
            self._cached_rewards[node, action, next_node] = (
                self.get_reward_distribution(node, action, next_node)
                .rvs(5000, random_state=self._rng)
                .tolist()
            )
        r = self._cached_rewards[node, action, next_node].pop(0)
        return (
            r * (self.rewards_range[1] - self.rewards_range[0]) - self.rewards_range[0]
        )

    def get_measure_from_name(self, measure_name: str) -&gt; float:
        &#34;&#34;&#34;
        returns the value of the measure given in input.
        &#34;&#34;&#34;
        if measure_name == &#34;diameter&#34;:
            return self.diameter
        elif measure_name in [&#34;value_norm&#34;, &#34;environmental_value_norm&#34;]:
            return self.value_norm
        elif measure_name == &#34;suboptimal_gaps&#34;:
            return self.sum_reciprocals_suboptimality_gaps
        else:
            raise ValueError(
                f&#34;{measure_name} is not a valid hardness measure name: available ones are &#34;
                + str(self.get_available_hardness_measures())
            )

    def action_spec(self) -&gt; DiscreteArray:
        &#34;&#34;&#34;
        returns the action spec of the environment.
        &#34;&#34;&#34;
        return DiscreteArray(self.n_actions, name=&#34;action&#34;)

    def observation_spec(self) -&gt; Array:
        &#34;&#34;&#34;
        returns the observation spec of the environment.
        &#34;&#34;&#34;
        if self._representation_mapping is None:
            return DiscreteArray(self.n_states, name=&#34;observation&#34;)
        obs = self.get_observation(self.starting_nodes[0], 0)
        return BoundedArray(obs.shape, obs.dtype, -np.inf, np.inf, &#34;observation&#34;)

    def get_observation(self, node: &#34;NODE_TYPE&#34;, h: int = None):
        &#34;&#34;&#34;
        returns the representation corresponding to the node given in input. Episodic MDPs also requires the current
        in-episode time step.
        &#34;&#34;&#34;
        if self._representation_mapping is None:
            return self.node_to_index[self.cur_node]
        return self._representation_mapping.get_observation(node, h)

    def reset(self) -&gt; dm_env.TimeStep:
        &#34;&#34;&#34;
        resets the environment to a newly sampled starting node.
        &#34;&#34;&#34;
        self.necessary_reset = False
        self.h = 0
        self.cur_node = self.last_starting_node = self._starting_node_sampler.sample()
        node_info_class = self.get_info_class(self.cur_node)
        node_info_class.update_visitation_counts()
        return dm_env.restart(self.get_observation(self.cur_node, self.h))

    def step(self, action: int, auto_reset=False) -&gt; dm_env.TimeStep:
        &#34;&#34;&#34;
        takes a step in the MDP for the given action. When auto_reset is set to True then it automatically reset the
        at the end of the episodes.
        &#34;&#34;&#34;
        if auto_reset and self.necessary_reset:
            return self.reset()
        assert not self.necessary_reset

        # This can be the in episode time step (episodic setting) or the total numuber of time steps (continuous setting)
        self.h += 1

        # In case the action is a numpy array
        action = int(action)

        # Moving the current node according to the action played
        old_node = self.cur_node
        self.cur_node = self.get_info_class(old_node).sample_next_state(action)
        self.last_edge = old_node, self.cur_node
        node_info_class = self.get_info_class(self.cur_node)
        node_info_class.update_visitation_counts(action)

        # Calculate reward and observation
        reward = self.sample_reward(old_node, action, self.cur_node)
        observation = self.get_observation(self.cur_node, self.h)

        # Wrapping the time step in a dm_env.TimeStep
        if self.is_episodic() and self.h &gt;= self.H:
            self.necessary_reset = True
            if self._representation_mapping is None:
                observation = -1
            else:
                observation = np.zeros_like(self.observation_spec().generate_value())
            return dm_env.termination(reward=reward, observation=observation)
        return dm_env.transition(reward=reward, observation=observation)

    def random_step(self, auto_reset=False) -&gt; Tuple[dm_env.TimeStep, int]:
        &#34;&#34;&#34;
        takes a step with a random action and returns both the next step and the random action. If auto_reset is set to
        True than it automatically resets episodic MDPs.
        &#34;&#34;&#34;
        action = int(self._rng.randint(self.action_spec().num_values))
        ts = self.step(action, auto_reset)
        return ts, action

    def get_visitation_counts(
        self, state_only=True
    ) -&gt; Dict[Union[&#34;NODE_TYPE&#34;, Tuple[&#34;NODE_TYPE&#34;, &#34;ACTION_TYPE&#34;]], int]:
        &#34;&#34;&#34;
        when state_only is True it returns the visitation counts for the states and when it is False it returns the
        visitation counts for state action pairs.
        &#34;&#34;&#34;
        if state_only:
            return {
                node: self.get_info_class(node).state_visitation_count
                for node in self.G.nodes
            }
        return {
            (node, a): self.get_info_class(node).actions_visitation_count[a]
            for node in self.G.nodes
            for a in range(self.n_actions)
        }

    def reset_visitation_counts(self):
        &#34;&#34;&#34;
        resets the visitation counts to zero for all states and state action pairs.
        &#34;&#34;&#34;
        for node in self.G.nodes:
            self.get_info_class(node).state_visitation_count = 0
            for a in range(self.n_actions):
                self.get_info_class(node).actions_visitation_count[a] = 0

    def get_value_node_labels(
        self: Union[&#34;ContinuousMDP&#34;, &#34;EpisodicMDP&#34;], V: np.ndarray = None
    ) -&gt; Dict[&#34;NODE_TYPE&#34;, float]:
        &#34;&#34;&#34;
        returns a mapping from node to state values. By default, it uses the optimal values.
        &#34;&#34;&#34;
        if V is None:
            _, V = self.optimal_value
        else:
            if isinstance(self, EpisodicMDP):
                h, d = V.shape
                assert h == self.H and d == self.n_states
            else:
                assert len(V) == self.n_states
        return {
            node: np.round(
                V[0, self.node_to_index[node]]
                if self.is_episodic()
                else V[self.node_to_index[node]],
                2,
            )
            for node in self.G.nodes
        }</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="colosseum.mdp.base.BaseMDP"><code class="flex name class">
<span>class <span class="ident">BaseMDP</span></span>
<span>(</span><span>seed: int, randomize_actions: bool = True, make_reward_stochastic=False, variance_multipliers: float = 1.0, p_lazy: float = None, p_rand: float = None, rewards_range: Tuple[float, float] = (0.0, 1.0), representation_mapping: Type[<a title="colosseum.mdp.utils.state_representation.RepresentationMapping" href="utils/state_representation.html#colosseum.mdp.utils.state_representation.RepresentationMapping">RepresentationMapping</a>] = None, representation_mapping_kwargs: Dict[str, Any] = {}, hardness_reports_folder='hardness_reports/', instantiate_mdp: bool = True, force_sparse_transition: bool = False)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for Python RL environments.</p>
<p>Observations and valid actions are described with <code>Array</code> specs, defined in
the <code>specs</code> module.</p>
<p>instantiates the MDP.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>is the random seed.</dd>
<dt><strong><code>randomize_actions</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>checks whether to apply a random mapping to the actions for each state. This avoids issues linked to
the possible bias of the agents to always take action zero at the beginning of the interactions.
By default, it is set to true.</dd>
<dt><strong><code>variance_multipliers</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>A constant that can be used to increase the variance of the reward distributions without changing their means.
The lower the value, the higher the variance. By default, it is set to 1.</dd>
<dt><strong><code>p_lazy</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>is the probability of an action not producing any effect on the MDP.
By default, it is set to zero.</dd>
<dt><strong><code>p_rand</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>is the probability of selecting an action at random instead of the one specified by the agent.
By default, it is set to zero.</dd>
<dt><strong><code>rewards_range</code></strong> :&ensp;<code>Tuple[float, float]</code>, optional</dt>
<dd>is the maximum value of the reward.
By default, it is set to the zero one interval.</dd>
<dt><strong><code>representation_mapping</code></strong> :&ensp;<code>RepresentationMapping</code></dt>
<dd>the representation mapping assigned to each state. By default, no representation mapping is used.</dd>
<dt><strong><code>hardness_reports_folder</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>is the path where the MDP looks for previously cached hardness reports.</dd>
<dt><strong><code>instantiate_mdp</code></strong> :&ensp;<code>bool</code></dt>
<dd>checks whether to immediately instantiate the MDP.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BaseMDP(dm_env.Environment, abc.ABC):
    @staticmethod
    def get_available_hardness_measures() -&gt; List[str]:
        return [&#34;diameter&#34;, &#34;value_norm&#34;, &#34;suboptimal_gaps&#34;]

    @staticmethod
    @abc.abstractmethod
    def does_seed_change_MDP_structure() -&gt; bool:
        &#34;&#34;&#34;
        returns True if when changing the seed the transition matrix and/or rewards matrix change. This for example may
        happen when there are fewer starting states that possible one and the effective starting states are picked
        randomly based on the seed.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def is_episodic() -&gt; bool:
        &#34;&#34;&#34;
        returns whether the MDP is episodic.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def sample_parameters(n: int, seed: int = None) -&gt; List[Dict[str, Any]]:
        &#34;&#34;&#34;
        returns n sampled parameters that can be used to construct an MDP in a reasonable amount of time.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def _sample_parameters(
        n: int, is_episodic: bool, seed: int = None
    ) -&gt; List[Dict[str, Any]]:
        &#34;&#34;&#34;
        returns n sampled parameters that can be used to construct an MDP in a reasonable amount of time.
        &#34;&#34;&#34;

    @staticmethod
    @abc.abstractmethod
    def get_node_class() -&gt; Type[&#34;NODE_TYPE&#34;]:
        &#34;&#34;&#34;
        returns the class of the nodes of the MDP.
        &#34;&#34;&#34;

    @property
    @abc.abstractmethod
    def n_actions(self) -&gt; int:
        &#34;&#34;&#34;
        returns the number of available actions.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_next_nodes_parameters(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;
    ) -&gt; Tuple[Tuple[dict, float], ...]:
        &#34;&#34;&#34;
        returns the parameters of the possible next nodes reachable from the given node and action.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_reward_distribution(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ) -&gt; rv_continuous:
        &#34;&#34;&#34;
        returns the distribution over rewards in the zero one interval when transitioning from a given node and action
        to a given next node. Note that rescaling the rewards to a different range is handled separately.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_starting_node_sampler(self) -&gt; NextStateSampler:
        &#34;&#34;&#34;
        returns a sampler over next states that corresponds to the starting state distribution.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _check_parameters_in_input(self):
        &#34;&#34;&#34;
        checks whether the parameters given in input can produce a correct MDP instance.
        &#34;&#34;&#34;
        assert self._p_rand is None or (0 &lt; self._p_rand &lt; 0.9999)
        assert self._p_lazy is None or (0 &lt; self._p_lazy &lt; 0.9999)

    @abc.abstractmethod
    def get_grid_representation(self, node: &#34;NODE_TYPE&#34;, h: int = None):
        &#34;&#34;&#34;
        produces an ASCII representation of the node given in input.
        &#34;&#34;&#34;

    @abc.abstractmethod
    def _get_grid_representation(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        produces an ASCII representation of the node given in input.
        &#34;&#34;&#34;

    @property
    @abc.abstractmethod
    def _possible_starting_nodes(self) -&gt; List[&#34;NODE_TYPE&#34;]:
        &#34;&#34;&#34;
        returns a list containing all the possible starting node for this MDP instance.
        &#34;&#34;&#34;

    @property
    @abc.abstractmethod
    def parameters(self) -&gt; Dict[str, Any]:
        &#34;&#34;&#34;
        returns the parameters of the MDP.
        &#34;&#34;&#34;
        return dict(
            seed=self._seed,
            randomize_actions=self._randomize_actions,
            p_lazy=self._p_lazy,
            p_rand=self._p_rand,
            rewards_range=self._rewards_range,
            make_reward_stochastic=self._make_reward_stochastic,
            variance_multipliers=self._variance_multipliers,
        )

    @property
    def hash(self) -&gt; str:
        &#34;&#34;&#34;
        returns a hash value based on the parameters of the MDP. This can be use to create cache files.
        &#34;&#34;&#34;
        s = &#34;_&#34;.join(map(str, clean_for_storing(list(self.parameters.values()))))
        return f&#34;mdp_{type(self).__name__}_&#34; + clean_for_file_path(s)

    def __str__(self) -&gt; str:
        &#34;&#34;&#34;
        returns a string containing all the information about the MDP including parameters, measures of hardness and
        graph metrics.
        &#34;&#34;&#34;
        string = type(self).__name__ + &#34;\n&#34;
        m_l = 0
        for k, v in self.summary.items():
            m_l = max(m_l, len(max(v.keys(), key=len)) + 4)
        for k, v in self.summary.items():
            string += &#34;\t&#34; + k + &#34;\n&#34;
            for kk, vv in v.items():
                string += f&#34;\t\t{kk}{&#39; &#39; * (m_l - len(kk))}:\t{vv}\n&#34;
        return string

    @abc.abstractmethod
    def __init__(
        self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;],
        seed: int,
        randomize_actions: bool = True,
        make_reward_stochastic=False,
        variance_multipliers: float = 1.0,
        p_lazy: float = None,
        p_rand: float = None,
        rewards_range: Tuple[float, float] = (0.0, 1.0),
        representation_mapping: Type[RepresentationMapping] = None,
        representation_mapping_kwargs: Dict[str, Any] = dict(),
        hardness_reports_folder=&#34;hardness_reports&#34; + os.sep,
        instantiate_mdp: bool = True,
        force_sparse_transition : bool = False
    ):
        &#34;&#34;&#34;
        instantiates the MDP.

        Parameters
        ----------
        seed : int
            is the random seed.
        randomize_actions : bool, optional
            checks whether to apply a random mapping to the actions for each state. This avoids issues linked to
            the possible bias of the agents to always take action zero at the beginning of the interactions.
            By default, it is set to true.
        variance_multipliers : float, optional
            A constant that can be used to increase the variance of the reward distributions without changing their means.
            The lower the value, the higher the variance. By default, it is set to 1.
        p_lazy : float, optional
            is the probability of an action not producing any effect on the MDP.
            By default, it is set to zero.
        p_rand : float, optional
            is the probability of selecting an action at random instead of the one specified by the agent.
            By default, it is set to zero.
        rewards_range : Tuple[float, float], optional
            is the maximum value of the reward.
            By default, it is set to the zero one interval.
        representation_mapping : RepresentationMapping
            the representation mapping assigned to each state. By default, no representation mapping is used.
        hardness_reports_folder : str, optional
            is the path where the MDP looks for previously cached hardness reports.
        instantiate_mdp : bool
            checks whether to immediately instantiate the MDP.
        &#34;&#34;&#34;

        # MDP generic variables
        self._seed = seed
        self._randomize_actions = randomize_actions
        self._make_reward_stochastic = make_reward_stochastic
        self._variance_multipliers = variance_multipliers
        if representation_mapping is not None:
            representation_mapping = representation_mapping(
                self, **representation_mapping_kwargs
            )
        self._representation_mapping = representation_mapping
        self._hardness_reports_folder = hardness_reports_folder
        self._force_sparse_transition = force_sparse_transition
        self._p_rand = p_rand if p_rand is None or p_rand &gt; 0.0 else None
        self._p_lazy = p_lazy if p_lazy is None or p_lazy &gt; 0.0 else None
        self.rewards_range = self._rewards_range = (
            rewards_range
            if rewards_range[0] &lt; rewards_range[1]
            else rewards_range[::-1]
        )
        self._are_all_rewards_deterministic = True
        self._are_all_transition_deterministic = True
        self.r_min, self.r_max = self.rewards_range

        # MDP loop variables
        self._hr = None
        self.cur_node = None
        self.last_edge = None
        self.last_starting_node = None
        self.is_reset_necessary = True
        self.current_timestep = 0
        self._rng = np.random.RandomState(seed)
        self._fast_rng = random.Random(seed)

        # Caching variables. Note that we cannot use lru_cache because it prevents from deleting the objects.
        self._cached_rewards = dict()
        self._cached_reward_distributions = dict()
        self._action_mapping = dict()
        self._communication_class = None
        self._recurrent_nodes_set = None
        if not hasattr(self, &#34;_transition_matrix_and_rewards&#34;):
            self._transition_matrix_and_rewards = None
        self._graph_layout = None
        self._graph_metrics = None
        self._summary = None
        self._diameter = None
        self._sum_reciprocals_suboptimality_gaps = None
        self._optimal_value_norm = dict()
        self._optimal_value = None
        self._worst_value = None
        self._random_value = None
        self._optimal_policy = dict()
        self._worst_policy = dict()
        self._otp = None
        self._omc = None
        self._osd = None
        self._oars = None
        self._oar = None
        self._wtp = None
        self._wmc = None
        self._wsd = None
        self._wars = None
        self._war = None
        self._rtp = None
        self._rmc = None
        self._rsd = None
        self._rars = None
        self._rar = None

        if instantiate_mdp:
            self.instantiate_MDP()

    def instantiate_MDP(self):
        # Instantiating the MDP
        self._check_parameters_in_input()
        self._starting_node_sampler = self._get_starting_node_sampler()
        self.starting_nodes = self._starting_node_sampler.next_nodes
        self.G = nx.DiGraph()
        self._instantiate_mdp()
        self.n_states = len(self.G.nodes)

        # Some shortcuts variables
        if not self.is_episodic():
            self._vi, self._pe = (
                discounted_value_iteration,
                discounted_policy_evaluation,
            )
        self.random_policy = (
            np.ones((self.n_states, self.n_actions), dtype=np.float32) / self.n_actions
        )

        # Cache node to index mapping
        mapping = self._rng.rand(self.n_states, self.n_actions).argsort(1)
        self.node_to_index = dict()
        self.index_to_node = dict()
        for i, node in enumerate(self.G.nodes):
            self.node_to_index[node] = i
            self.index_to_node[i] = node

        # Compute the starting state distribution
        self.starting_distribution = np.zeros(self.n_states)
        self.starting_states = []
        for n, p in self._starting_node_sampler.next_nodes_and_probs:
            s = self.node_to_index[n]
            self.starting_distribution[s] = p
            self.starting_states.append(s)
        self.starting_states_and_probs = list(
            zip(self.starting_states, self._starting_node_sampler.probs)
        )

    def _get_action_mapping(self, node: &#34;NODE_TYPE&#34;) -&gt; Tuple[&#34;ACTION_TYPE&#34;, ...]:
        &#34;&#34;&#34;
        returns the random action mapping for the node given in input.
        &#34;&#34;&#34;
        if node not in self._action_mapping:
            self._action_mapping[node] = (
                self._rng.rand(self.n_actions).argsort().tolist()
                if self._randomize_actions
                else list(range(self.n_actions))
            )
        return self._action_mapping[node]

    def _inverse_action_mapping(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;
    ) -&gt; &#34;ACTION_TYPE&#34;:
        &#34;&#34;&#34;
        returns the effective action corresponding to the action given in input in the given node. In other words, it
        reverses the random action mapping.
        &#34;&#34;&#34;
        return self._get_action_mapping(node)[action]

    def _produce_random_seed(self) -&gt; int:
        &#34;&#34;&#34;
        returns a new random seed that can be used for internal objects.
        &#34;&#34;&#34;
        return self._fast_rng.randint(0, 10_000)

    def _instantiate_mdp(self):
        &#34;&#34;&#34;
        recursively instantiate the MDP.
        &#34;&#34;&#34;
        for sn in self.starting_nodes:
            instantiate_transitions(self, sn)

    @property
    def T(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the transition matrix.
        &#34;&#34;&#34;
        return self.transition_matrix_and_rewards[0]

    @property
    def R(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the rewards matrix.
        &#34;&#34;&#34;
        return self.transition_matrix_and_rewards[1]

    @property
    def recurrent_nodes_set(self) -&gt; Iterable[&#34;NODE_TYPE&#34;]:
        &#34;&#34;&#34;
        returns the recurrent node set.
        &#34;&#34;&#34;
        if self._recurrent_nodes_set is None:
            self._recurrent_nodes_set = get_recurrent_nodes_set(
                self.communication_class, self.G
            )
        return self._recurrent_nodes_set

    @property
    def communication_class(
        self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
    ) -&gt; MDPCommunicationClass:
        &#34;&#34;&#34;
        returns the communication class.
        &#34;&#34;&#34;
        if self._communication_class is None:
            self._communication_class = get_communication_class(
                self.T, self.get_episodic_graph(True) if self.is_episodic() else self.G
            )
        return self._communication_class

    def get_optimal_policy(self, stochastic_form: bool) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the optimal policy. It can be either in the stochastic form, so in the form of dirac probabaility
        distribution or as a simple mapping to integer.
        &#34;&#34;&#34;
        if stochastic_form not in self._optimal_policy:
            self._optimal_policy[stochastic_form] = get_policy_from_q_values(
                self.optimal_value[0], stochastic_form
            )
        return self._optimal_policy[stochastic_form]

    def get_worst_policy(self, stochastic_form) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the worst performing policy. It can be either in the stochastic form, so in the form of dirac probabaility
        distribution or as a simple mapping to integer.
        &#34;&#34;&#34;
        if stochastic_form not in self._worst_policy:
            self._worst_policy[stochastic_form] = get_policy_from_q_values(
                self._vi(self.T, -self.R)[0], stochastic_form
            )
        return self._worst_policy[stochastic_form]

    @property
    def optimal_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the optimal q and state values.
        &#34;&#34;&#34;
        if self._optimal_value is None:
            self._optimal_value = self._vi(*self.transition_matrix_and_rewards)
        return self._optimal_value

    @property
    def worst_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the q and state values for the worst performing policy.
        &#34;&#34;&#34;
        if self._worst_value is None:
            self._worst_value = self._pe(
                *self.transition_matrix_and_rewards, self.get_worst_policy(True)
            )
        return self._worst_value

    @property
    def random_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the q and state values for the randomly acting policy.
        &#34;&#34;&#34;
        if self._random_value is None:
            self._random_value = self._pe(
                *self.transition_matrix_and_rewards, self.random_policy
            )
        return self._random_value

    @property
    def optimal_transition_probabilities(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the transition probabilities corresponding to the optimal policy.
        &#34;&#34;&#34;
        if self._otp is None:
            T = self.T_cf if self.is_episodic() else self.T
            pi = (
                self.get_optimal_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_optimal_policy(True)
            )
            self._otp = get_transition_probabilities(T, pi)
        return self._otp

    @property
    def worst_transition_probabilities(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the transition probabilities corresponding to the worst performing policy.
        &#34;&#34;&#34;
        if self._wtp is None:
            T = self.T_cf if self.is_episodic() else self.T
            pi = (
                self.get_worst_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_worst_policy(True)
            )
            self._wtp = get_transition_probabilities(T, pi)
        return self._wtp

    @property
    def random_transition_probabilities(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the transition probabilities corresponding to the randomly acting policy.
        &#34;&#34;&#34;
        if self._rtp is None:
            T = self.T_cf if self.is_episodic() else self.T
            pi = self.random_policy_cf if self.is_episodic() else self.random_policy
            self._rtp = get_transition_probabilities(T, pi)
        return self._rtp

    @property
    def optimal_markov_chain(self) -&gt; MarkovChain:
        &#34;&#34;&#34;
        returns the Markov chain corresponding to the optimal policy.
        &#34;&#34;&#34;
        if self._omc is None:
            self._omc = get_markov_chain(self.optimal_transition_probabilities)
        return self._omc

    @property
    def worst_markov_chain(self) -&gt; MarkovChain:
        &#34;&#34;&#34;
        returns the Markov chain corresponding to the worst performing policy.
        &#34;&#34;&#34;
        if self._wmc is None:
            self._wmc = get_markov_chain(self.worst_transition_probabilities)
        return self._wmc

    @property
    def random_markov_chain(self) -&gt; MarkovChain:
        &#34;&#34;&#34;
        returns the Markov chain corresponding to the randomly acting policy.
        &#34;&#34;&#34;
        if self._rmc is None:
            self._rmc = get_markov_chain(self.random_transition_probabilities)
        return self._rmc

    @property
    def optimal_stationary_distribution(self) -&gt; np.array:
        &#34;&#34;&#34;
        returns the stationary distribution yielded by the optimal policy.
        &#34;&#34;&#34;
        if self._osd is None:
            self._osd = get_stationary_distribution(
                self.optimal_transition_probabilities,
                self.starting_states_and_probs,
            )
        return self._osd

    @property
    def worst_stationary_distribution(self) -&gt; np.array:
        &#34;&#34;&#34;
        returns the stationary distribution yielded by the worst performing policy.
        &#34;&#34;&#34;
        if self._wsd is None:
            self._wsd = get_stationary_distribution(
                self.worst_transition_probabilities,
                self.starting_states_and_probs,
            )
            assert not np.isnan(self._wsd).any()
        return self._wsd

    @property
    def random_stationary_distribution(self) -&gt; np.array:
        &#34;&#34;&#34;
        returns the stationary distribution yielded by the randomly acting policy.
        &#34;&#34;&#34;
        if self._rsd is None:
            self._rsd = get_stationary_distribution(
                self.random_transition_probabilities,
                None,
            )
        return self._rsd

    @property
    def optimal_average_rewards(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the expected rewards obtained at each state when following the optimal policy.
        &#34;&#34;&#34;
        if self._oars is None:
            R = self.R_cf if self.is_episodic() else self.R
            pi = (
                self.get_optimal_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_optimal_policy(True)
            )
            self._oars = get_average_rewards(R, pi)
        return self._oars

    @property
    def worst_average_rewards(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the expected rewards obtained at each state when following the worst performing policy.
        &#34;&#34;&#34;
        if self._wars is None:
            R = self.R_cf if self.is_episodic() else self.R
            pi = (
                self.get_worst_policy_continuous_form(True)
                if self.is_episodic()
                else self.get_worst_policy(True)
            )
            self._wars = get_average_rewards(R, pi)
        return self._wars

    @property
    def random_average_rewards(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the expected rewards obtained at each state when following the randomly acting policy.
        &#34;&#34;&#34;
        if self._rars is None:
            R = self.R_cf if self.is_episodic() else self.R
            pi = self.random_policy_cf if self.is_episodic() else self.random_policy
            self._rars = get_average_rewards(R, pi)
        return self._rars

    @property
    def optimal_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected average reward obtained when following the optimal policy.
        &#34;&#34;&#34;
        if self._oar is None:
            self._oar = sum(
                self.optimal_stationary_distribution * self.optimal_average_rewards
            )
        return self._oar

    @property
    def worst_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected average reward obtained when following the worst performing policy.
        &#34;&#34;&#34;
        if self._war is None:
            self._war = sum(
                self.worst_stationary_distribution * self.worst_average_rewards
            )
        return self._war

    @property
    def random_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected average reward obtained when following the randomly acting policy.
        &#34;&#34;&#34;
        if self._rar is None:
            self._rar = sum(
                self.random_stationary_distribution * self.random_average_rewards
            )
        return self._rar

    @property
    def transition_matrix_and_rewards(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the transition probabilities matrix and the reward matrix.
        &#34;&#34;&#34;
        if self._transition_matrix_and_rewards is None:
            self._transition_matrix_and_rewards = get_transition_matrix_and_rewards(
                self.n_states,
                self.n_actions,
                self.G,
                self.get_info_class,
                self.get_reward_distribution,
                self.node_to_index,
                self._force_sparse_transition
            )
        return self._transition_matrix_and_rewards

    @property
    def graph_layout(self) -&gt; Dict[&#34;NODE_TYPE&#34;, Tuple[float, float]]:
        &#34;&#34;&#34;
        returns a graph layout for the MDP. It can be customized by implementing the custom_graph_layout function.
        &#34;&#34;&#34;
        if self._graph_layout is None:
            self._graph_layout = (
                self.custom_graph_layout()
                if hasattr(self, &#34;custom_graph_layout&#34;)
                else nx.nx_agraph.graphviz_layout(self.G)
            )
        return self._graph_layout

    @property
    def graph_metrics(self) -&gt; dict:
        &#34;&#34;&#34;
        returns a dictionary containing graph metric for the MDP graph.
        &#34;&#34;&#34;
        if self._graph_metrics is None:
            G = self.get_episodic_graph(True) if self.is_episodic() else self.G
            self._graph_metrics = dict()
            self._graph_metrics[&#34;# nodes&#34;] = len(G.nodes)
            self._graph_metrics[&#34;# edges&#34;] = len(G.edges)
        return self._graph_metrics

    @property
    def diameter(self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]) -&gt; float:
        &#34;&#34;&#34;
        returns the diameter of the MDP.
        &#34;&#34;&#34;
        if self._diameter is None:
            if self.hardness_report:
                self._diameter = self.hardness_report[&#34;MDP measure of hardness&#34;][
                    &#34;diameter&#34;
                ]
            else:
                self._diameter = get_diameter(
                    self.episodic_transition_matrix_and_rewards[0]
                    if self.is_episodic()
                    else self.T,
                    self.is_episodic(),
                )
        return self._diameter

    @property
    def sum_reciprocals_suboptimality_gaps(
        self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
    ) -&gt; float:
        &#34;&#34;&#34;
        returns the sum of the reciprocals of the sub-optimality gaps.
        &#34;&#34;&#34;
        if self._sum_reciprocals_suboptimality_gaps is None:
            if self.hardness_report:
                self._sum_reciprocals_suboptimality_gaps = self.hardness_report[
                    &#34;MDP measure of hardness&#34;
                ][&#34;suboptimal_gaps&#34;]
            else:
                self._sum_reciprocals_suboptimality_gaps = (
                    get_sum_reciprocals_suboptimality_gaps(
                        *self.optimal_value,
                        self.reachable_states if self.is_episodic() else None,
                    )
                )
        return self._sum_reciprocals_suboptimality_gaps

    def _compute_value_norm(self, discounted: bool) -&gt; float:
        &#34;&#34;&#34;
        returns the environmental value norm in the undiscounted or undiscounted setting.
        &#34;&#34;&#34;
        T, R = (self.T_cf, self.R_cf) if self.is_episodic() else (self.T, self.R)
        V = (
            self.optimal_value_continuous_form[1]
            if self.is_episodic()
            else self.optimal_value[1]
        )
        if discounted:
            return calculate_norm_discounted(T, V)
        return calculate_norm_average(
            T, self.optimal_transition_probabilities, self.optimal_average_rewards
        )

    @property
    def discounted_value_norm(self) -&gt; float:
        &#34;&#34;&#34;
        returns the discounted environmental value norm.
        &#34;&#34;&#34;
        if True not in self._optimal_value_norm:
            if (
                self._are_all_transition_deterministic
                and self._are_all_rewards_deterministic
            ):
                self._optimal_value_norm[True] = 0.0
            elif self.hardness_report:
                self._optimal_value_norm[True] = self.hardness_report[
                    &#34;MDP measure of hardness&#34;
                ][&#34;value_norm&#34;]
            else:
                self._optimal_value_norm[True] = self._compute_value_norm(True)
        return self._optimal_value_norm[True]

    @property
    def undiscounted_value_norm(self) -&gt; float:
        &#34;&#34;&#34;
        returns the undiscounted environmental value norm.
        &#34;&#34;&#34;
        if False not in self._optimal_value_norm:
            self._optimal_value_norm[False] = self._compute_value_norm(False)
        return self._optimal_value_norm[False]

    @property
    def value_norm(self):
        &#34;&#34;&#34;
        is an alias for the discounted value norm.
        &#34;&#34;&#34;
        return self.discounted_value_norm

    @property
    def measures_of_hardness(self) -&gt; Dict[str, float]:
        &#34;&#34;&#34;
        returns a dictionary containing all the measures of hardness availble.
        &#34;&#34;&#34;
        return dict(
            diameter=self.diameter,
            suboptimal_gaps=self.sum_reciprocals_suboptimality_gaps,
            value_norm=self.value_norm,
        )

    @property
    def summary(self) -&gt; Dict[str, Dict[str, Any]]:
        &#34;&#34;&#34;
        returns a dictionary with information about the parameters, the measures of hardness and the graph metrics.
        &#34;&#34;&#34;
        if self._summary is None:
            self._summary = {
                &#34;Parameters&#34;: clean_for_storing(self.parameters),
                &#34;Measure of hardness&#34;: clean_for_storing(self.measures_of_hardness),
                &#34;Graph metrics&#34;: clean_for_storing(self.graph_metrics),
            }
        return self._summary

    @property
    def hardness_report(self) -&gt; Union[Dict, None]:
        &#34;&#34;&#34;
        looks for a cached hardness report in the folder given in input when the MDP was created. It returns None if it
        is not able to find it.
        &#34;&#34;&#34;
        if self._hr is None:
            report_file = find_hardness_report_file(self, self._hardness_reports_folder)
            if report_file:
                with open(report_file, &#34;r&#34;) as f:
                    report = yaml.load(f, yaml.Loader)
                self._hr = report
            else:
                self._hr = False
        if self._hr:
            return self._hr
        return None

    def get_info_class(self, n: &#34;NODE_TYPE&#34;) -&gt; NodeInfoClass:
        &#34;&#34;&#34;
        returns the container class (NodeInfoClass) associated with node n.
        &#34;&#34;&#34;
        return self.G.nodes[n][&#34;info_class&#34;]

    def get_transition_distributions(
        self, node: &#34;NODE_TYPE&#34;
    ) -&gt; Dict[int, Union[rv_continuous, NextStateSampler]]:
        &#34;&#34;&#34;
        returns a dictionary containing the transition distributions for any action at the node given in input.
        &#34;&#34;&#34;
        return self.get_info_class(node).transition_distributions

    def get_reward_distribution(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ):
        &#34;&#34;&#34;
        returns the reward distribution for transitioning in next_node when selecting action from node.
        &#34;&#34;&#34;
        if (node, action, next_node) not in self._cached_reward_distributions:
            self._cached_reward_distributions[
                node, action, next_node
            ] = self._get_reward_distribution(
                node, self._inverse_action_mapping(node, action), next_node
            )
        return self._cached_reward_distributions[node, action, next_node]

    def sample_reward(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ) -&gt; float:
        &#34;&#34;&#34;
        returns a sample from the reward distribution when transitioning in next_node when selecting action from node.
        &#34;&#34;&#34;
        if (node, action, next_node) not in self._cached_rewards or len(
            self._cached_rewards[node, action, next_node]
        ) == 0:
            self._cached_rewards[node, action, next_node] = (
                self.get_reward_distribution(node, action, next_node)
                .rvs(5000, random_state=self._rng)
                .tolist()
            )
        r = self._cached_rewards[node, action, next_node].pop(0)
        return (
            r * (self.rewards_range[1] - self.rewards_range[0]) - self.rewards_range[0]
        )

    def get_measure_from_name(self, measure_name: str) -&gt; float:
        &#34;&#34;&#34;
        returns the value of the measure given in input.
        &#34;&#34;&#34;
        if measure_name == &#34;diameter&#34;:
            return self.diameter
        elif measure_name in [&#34;value_norm&#34;, &#34;environmental_value_norm&#34;]:
            return self.value_norm
        elif measure_name == &#34;suboptimal_gaps&#34;:
            return self.sum_reciprocals_suboptimality_gaps
        else:
            raise ValueError(
                f&#34;{measure_name} is not a valid hardness measure name: available ones are &#34;
                + str(self.get_available_hardness_measures())
            )

    def action_spec(self) -&gt; DiscreteArray:
        &#34;&#34;&#34;
        returns the action spec of the environment.
        &#34;&#34;&#34;
        return DiscreteArray(self.n_actions, name=&#34;action&#34;)

    def observation_spec(self) -&gt; Array:
        &#34;&#34;&#34;
        returns the observation spec of the environment.
        &#34;&#34;&#34;
        if self._representation_mapping is None:
            return DiscreteArray(self.n_states, name=&#34;observation&#34;)
        obs = self.get_observation(self.starting_nodes[0], 0)
        return BoundedArray(obs.shape, obs.dtype, -np.inf, np.inf, &#34;observation&#34;)

    def get_observation(self, node: &#34;NODE_TYPE&#34;, h: int = None):
        &#34;&#34;&#34;
        returns the representation corresponding to the node given in input. Episodic MDPs also requires the current
        in-episode time step.
        &#34;&#34;&#34;
        if self._representation_mapping is None:
            return self.node_to_index[self.cur_node]
        return self._representation_mapping.get_observation(node, h)

    def reset(self) -&gt; dm_env.TimeStep:
        &#34;&#34;&#34;
        resets the environment to a newly sampled starting node.
        &#34;&#34;&#34;
        self.necessary_reset = False
        self.h = 0
        self.cur_node = self.last_starting_node = self._starting_node_sampler.sample()
        node_info_class = self.get_info_class(self.cur_node)
        node_info_class.update_visitation_counts()
        return dm_env.restart(self.get_observation(self.cur_node, self.h))

    def step(self, action: int, auto_reset=False) -&gt; dm_env.TimeStep:
        &#34;&#34;&#34;
        takes a step in the MDP for the given action. When auto_reset is set to True then it automatically reset the
        at the end of the episodes.
        &#34;&#34;&#34;
        if auto_reset and self.necessary_reset:
            return self.reset()
        assert not self.necessary_reset

        # This can be the in episode time step (episodic setting) or the total numuber of time steps (continuous setting)
        self.h += 1

        # In case the action is a numpy array
        action = int(action)

        # Moving the current node according to the action played
        old_node = self.cur_node
        self.cur_node = self.get_info_class(old_node).sample_next_state(action)
        self.last_edge = old_node, self.cur_node
        node_info_class = self.get_info_class(self.cur_node)
        node_info_class.update_visitation_counts(action)

        # Calculate reward and observation
        reward = self.sample_reward(old_node, action, self.cur_node)
        observation = self.get_observation(self.cur_node, self.h)

        # Wrapping the time step in a dm_env.TimeStep
        if self.is_episodic() and self.h &gt;= self.H:
            self.necessary_reset = True
            if self._representation_mapping is None:
                observation = -1
            else:
                observation = np.zeros_like(self.observation_spec().generate_value())
            return dm_env.termination(reward=reward, observation=observation)
        return dm_env.transition(reward=reward, observation=observation)

    def random_step(self, auto_reset=False) -&gt; Tuple[dm_env.TimeStep, int]:
        &#34;&#34;&#34;
        takes a step with a random action and returns both the next step and the random action. If auto_reset is set to
        True than it automatically resets episodic MDPs.
        &#34;&#34;&#34;
        action = int(self._rng.randint(self.action_spec().num_values))
        ts = self.step(action, auto_reset)
        return ts, action

    def get_visitation_counts(
        self, state_only=True
    ) -&gt; Dict[Union[&#34;NODE_TYPE&#34;, Tuple[&#34;NODE_TYPE&#34;, &#34;ACTION_TYPE&#34;]], int]:
        &#34;&#34;&#34;
        when state_only is True it returns the visitation counts for the states and when it is False it returns the
        visitation counts for state action pairs.
        &#34;&#34;&#34;
        if state_only:
            return {
                node: self.get_info_class(node).state_visitation_count
                for node in self.G.nodes
            }
        return {
            (node, a): self.get_info_class(node).actions_visitation_count[a]
            for node in self.G.nodes
            for a in range(self.n_actions)
        }

    def reset_visitation_counts(self):
        &#34;&#34;&#34;
        resets the visitation counts to zero for all states and state action pairs.
        &#34;&#34;&#34;
        for node in self.G.nodes:
            self.get_info_class(node).state_visitation_count = 0
            for a in range(self.n_actions):
                self.get_info_class(node).actions_visitation_count[a] = 0

    def get_value_node_labels(
        self: Union[&#34;ContinuousMDP&#34;, &#34;EpisodicMDP&#34;], V: np.ndarray = None
    ) -&gt; Dict[&#34;NODE_TYPE&#34;, float]:
        &#34;&#34;&#34;
        returns a mapping from node to state values. By default, it uses the optimal values.
        &#34;&#34;&#34;
        if V is None:
            _, V = self.optimal_value
        else:
            if isinstance(self, EpisodicMDP):
                h, d = V.shape
                assert h == self.H and d == self.n_states
            else:
                assert len(V) == self.n_states
        return {
            node: np.round(
                V[0, self.node_to_index[node]]
                if self.is_episodic()
                else V[self.node_to_index[node]],
                2,
            )
            for node in self.G.nodes
        }</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>dm_env._environment.Environment</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="colosseum.mdp.base_finite.EpisodicMDP" href="base_finite.html#colosseum.mdp.base_finite.EpisodicMDP">EpisodicMDP</a></li>
<li><a title="colosseum.mdp.base_infinite.ContinuousMDP" href="base_infinite.html#colosseum.mdp.base_infinite.ContinuousMDP">ContinuousMDP</a></li>
<li><a title="colosseum.mdp.custom_mdp.CustomMDP" href="custom_mdp.html#colosseum.mdp.custom_mdp.CustomMDP">CustomMDP</a></li>
<li><a title="colosseum.mdp.deep_sea.base.DeepSeaMDP" href="deep_sea/base.html#colosseum.mdp.deep_sea.base.DeepSeaMDP">DeepSeaMDP</a></li>
<li><a title="colosseum.mdp.frozen_lake.base.FrozenLakeMDP" href="frozen_lake/base.html#colosseum.mdp.frozen_lake.base.FrozenLakeMDP">FrozenLakeMDP</a></li>
<li><a title="colosseum.mdp.minigrid_empty.base.MiniGridEmptyMDP" href="minigrid_empty/base.html#colosseum.mdp.minigrid_empty.base.MiniGridEmptyMDP">MiniGridEmptyMDP</a></li>
<li><a title="colosseum.mdp.minigrid_rooms.base.MiniGridRoomsMDP" href="minigrid_rooms/base.html#colosseum.mdp.minigrid_rooms.base.MiniGridRoomsMDP">MiniGridRoomsMDP</a></li>
<li><a title="colosseum.mdp.river_swim.base.RiverSwimMDP" href="river_swim/base.html#colosseum.mdp.river_swim.base.RiverSwimMDP">RiverSwimMDP</a></li>
<li><a title="colosseum.mdp.simple_grid.base.SimpleGridMDP" href="simple_grid/base.html#colosseum.mdp.simple_grid.base.SimpleGridMDP">SimpleGridMDP</a></li>
<li><a title="colosseum.mdp.taxi.base.TaxiMDP" href="taxi/base.html#colosseum.mdp.taxi.base.TaxiMDP">TaxiMDP</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure"><code class="name flex">
<span>def <span class="ident">does_seed_change_MDP_structure</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>returns True if when changing the seed the transition matrix and/or rewards matrix change. This for example may
happen when there are fewer starting states that possible one and the effective starting states are picked
randomly based on the seed.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
@abc.abstractmethod
def does_seed_change_MDP_structure() -&gt; bool:
    &#34;&#34;&#34;
    returns True if when changing the seed the transition matrix and/or rewards matrix change. This for example may
    happen when there are fewer starting states that possible one and the effective starting states are picked
    randomly based on the seed.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_available_hardness_measures"><code class="name flex">
<span>def <span class="ident">get_available_hardness_measures</span></span>(<span>) ‑> List[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def get_available_hardness_measures() -&gt; List[str]:
    return [&#34;diameter&#34;, &#34;value_norm&#34;, &#34;suboptimal_gaps&#34;]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_node_class"><code class="name flex">
<span>def <span class="ident">get_node_class</span></span>(<span>) ‑> Type[NODE_TYPE]</span>
</code></dt>
<dd>
<div class="desc"><p>returns the class of the nodes of the MDP.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
@abc.abstractmethod
def get_node_class() -&gt; Type[&#34;NODE_TYPE&#34;]:
    &#34;&#34;&#34;
    returns the class of the nodes of the MDP.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.is_episodic"><code class="name flex">
<span>def <span class="ident">is_episodic</span></span>(<span>) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>returns whether the MDP is episodic.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
@abc.abstractmethod
def is_episodic() -&gt; bool:
    &#34;&#34;&#34;
    returns whether the MDP is episodic.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.sample_parameters"><code class="name flex">
<span>def <span class="ident">sample_parameters</span></span>(<span>n: int, seed: int = None) ‑> List[Dict[str, Any]]</span>
</code></dt>
<dd>
<div class="desc"><p>returns n sampled parameters that can be used to construct an MDP in a reasonable amount of time.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
@abc.abstractmethod
def sample_parameters(n: int, seed: int = None) -&gt; List[Dict[str, Any]]:
    &#34;&#34;&#34;
    returns n sampled parameters that can be used to construct an MDP in a reasonable amount of time.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="colosseum.mdp.base.BaseMDP.R"><code class="name">var <span class="ident">R</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>is an alias for the rewards matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def R(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    is an alias for the rewards matrix.
    &#34;&#34;&#34;
    return self.transition_matrix_and_rewards[1]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.T"><code class="name">var <span class="ident">T</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>is an alias for the transition matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def T(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    is an alias for the transition matrix.
    &#34;&#34;&#34;
    return self.transition_matrix_and_rewards[0]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.communication_class"><code class="name">var <span class="ident">communication_class</span> : <a title="colosseum.mdp.utils.communication_class.MDPCommunicationClass" href="utils/communication_class.html#colosseum.mdp.utils.communication_class.MDPCommunicationClass">MDPCommunicationClass</a></code></dt>
<dd>
<div class="desc"><p>returns the communication class.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def communication_class(
    self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
) -&gt; MDPCommunicationClass:
    &#34;&#34;&#34;
    returns the communication class.
    &#34;&#34;&#34;
    if self._communication_class is None:
        self._communication_class = get_communication_class(
            self.T, self.get_episodic_graph(True) if self.is_episodic() else self.G
        )
    return self._communication_class</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.diameter"><code class="name">var <span class="ident">diameter</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the diameter of the MDP.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def diameter(self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]) -&gt; float:
    &#34;&#34;&#34;
    returns the diameter of the MDP.
    &#34;&#34;&#34;
    if self._diameter is None:
        if self.hardness_report:
            self._diameter = self.hardness_report[&#34;MDP measure of hardness&#34;][
                &#34;diameter&#34;
            ]
        else:
            self._diameter = get_diameter(
                self.episodic_transition_matrix_and_rewards[0]
                if self.is_episodic()
                else self.T,
                self.is_episodic(),
            )
    return self._diameter</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.discounted_value_norm"><code class="name">var <span class="ident">discounted_value_norm</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the discounted environmental value norm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def discounted_value_norm(self) -&gt; float:
    &#34;&#34;&#34;
    returns the discounted environmental value norm.
    &#34;&#34;&#34;
    if True not in self._optimal_value_norm:
        if (
            self._are_all_transition_deterministic
            and self._are_all_rewards_deterministic
        ):
            self._optimal_value_norm[True] = 0.0
        elif self.hardness_report:
            self._optimal_value_norm[True] = self.hardness_report[
                &#34;MDP measure of hardness&#34;
            ][&#34;value_norm&#34;]
        else:
            self._optimal_value_norm[True] = self._compute_value_norm(True)
    return self._optimal_value_norm[True]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.graph_layout"><code class="name">var <span class="ident">graph_layout</span> : Dict[NODE_TYPE, Tuple[float, float]]</code></dt>
<dd>
<div class="desc"><p>returns a graph layout for the MDP. It can be customized by implementing the custom_graph_layout function.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def graph_layout(self) -&gt; Dict[&#34;NODE_TYPE&#34;, Tuple[float, float]]:
    &#34;&#34;&#34;
    returns a graph layout for the MDP. It can be customized by implementing the custom_graph_layout function.
    &#34;&#34;&#34;
    if self._graph_layout is None:
        self._graph_layout = (
            self.custom_graph_layout()
            if hasattr(self, &#34;custom_graph_layout&#34;)
            else nx.nx_agraph.graphviz_layout(self.G)
        )
    return self._graph_layout</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.graph_metrics"><code class="name">var <span class="ident">graph_metrics</span> : dict</code></dt>
<dd>
<div class="desc"><p>returns a dictionary containing graph metric for the MDP graph.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def graph_metrics(self) -&gt; dict:
    &#34;&#34;&#34;
    returns a dictionary containing graph metric for the MDP graph.
    &#34;&#34;&#34;
    if self._graph_metrics is None:
        G = self.get_episodic_graph(True) if self.is_episodic() else self.G
        self._graph_metrics = dict()
        self._graph_metrics[&#34;# nodes&#34;] = len(G.nodes)
        self._graph_metrics[&#34;# edges&#34;] = len(G.edges)
    return self._graph_metrics</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.hardness_report"><code class="name">var <span class="ident">hardness_report</span> : Optional[Dict[~KT, ~VT]]</code></dt>
<dd>
<div class="desc"><p>looks for a cached hardness report in the folder given in input when the MDP was created. It returns None if it
is not able to find it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def hardness_report(self) -&gt; Union[Dict, None]:
    &#34;&#34;&#34;
    looks for a cached hardness report in the folder given in input when the MDP was created. It returns None if it
    is not able to find it.
    &#34;&#34;&#34;
    if self._hr is None:
        report_file = find_hardness_report_file(self, self._hardness_reports_folder)
        if report_file:
            with open(report_file, &#34;r&#34;) as f:
                report = yaml.load(f, yaml.Loader)
            self._hr = report
        else:
            self._hr = False
    if self._hr:
        return self._hr
    return None</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.hash"><code class="name">var <span class="ident">hash</span> : str</code></dt>
<dd>
<div class="desc"><p>returns a hash value based on the parameters of the MDP. This can be use to create cache files.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def hash(self) -&gt; str:
    &#34;&#34;&#34;
    returns a hash value based on the parameters of the MDP. This can be use to create cache files.
    &#34;&#34;&#34;
    s = &#34;_&#34;.join(map(str, clean_for_storing(list(self.parameters.values()))))
    return f&#34;mdp_{type(self).__name__}_&#34; + clean_for_file_path(s)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.measures_of_hardness"><code class="name">var <span class="ident">measures_of_hardness</span> : Dict[str, float]</code></dt>
<dd>
<div class="desc"><p>returns a dictionary containing all the measures of hardness availble.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def measures_of_hardness(self) -&gt; Dict[str, float]:
    &#34;&#34;&#34;
    returns a dictionary containing all the measures of hardness availble.
    &#34;&#34;&#34;
    return dict(
        diameter=self.diameter,
        suboptimal_gaps=self.sum_reciprocals_suboptimality_gaps,
        value_norm=self.value_norm,
    )</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.n_actions"><code class="name">var <span class="ident">n_actions</span> : int</code></dt>
<dd>
<div class="desc"><p>returns the number of available actions.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
@abc.abstractmethod
def n_actions(self) -&gt; int:
    &#34;&#34;&#34;
    returns the number of available actions.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.optimal_average_reward"><code class="name">var <span class="ident">optimal_average_reward</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the expected average reward obtained when following the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_average_reward(self) -&gt; float:
    &#34;&#34;&#34;
    returns the expected average reward obtained when following the optimal policy.
    &#34;&#34;&#34;
    if self._oar is None:
        self._oar = sum(
            self.optimal_stationary_distribution * self.optimal_average_rewards
        )
    return self._oar</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.optimal_average_rewards"><code class="name">var <span class="ident">optimal_average_rewards</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the expected rewards obtained at each state when following the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_average_rewards(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the expected rewards obtained at each state when following the optimal policy.
    &#34;&#34;&#34;
    if self._oars is None:
        R = self.R_cf if self.is_episodic() else self.R
        pi = (
            self.get_optimal_policy_continuous_form(True)
            if self.is_episodic()
            else self.get_optimal_policy(True)
        )
        self._oars = get_average_rewards(R, pi)
    return self._oars</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.optimal_markov_chain"><code class="name">var <span class="ident">optimal_markov_chain</span> : pydtmc.markov_chain.MarkovChain</code></dt>
<dd>
<div class="desc"><p>returns the Markov chain corresponding to the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_markov_chain(self) -&gt; MarkovChain:
    &#34;&#34;&#34;
    returns the Markov chain corresponding to the optimal policy.
    &#34;&#34;&#34;
    if self._omc is None:
        self._omc = get_markov_chain(self.optimal_transition_probabilities)
    return self._omc</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.optimal_stationary_distribution"><code class="name">var <span class="ident">optimal_stationary_distribution</span> : <built-in function array></code></dt>
<dd>
<div class="desc"><p>returns the stationary distribution yielded by the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_stationary_distribution(self) -&gt; np.array:
    &#34;&#34;&#34;
    returns the stationary distribution yielded by the optimal policy.
    &#34;&#34;&#34;
    if self._osd is None:
        self._osd = get_stationary_distribution(
            self.optimal_transition_probabilities,
            self.starting_states_and_probs,
        )
    return self._osd</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.optimal_transition_probabilities"><code class="name">var <span class="ident">optimal_transition_probabilities</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the transition probabilities corresponding to the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_transition_probabilities(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the transition probabilities corresponding to the optimal policy.
    &#34;&#34;&#34;
    if self._otp is None:
        T = self.T_cf if self.is_episodic() else self.T
        pi = (
            self.get_optimal_policy_continuous_form(True)
            if self.is_episodic()
            else self.get_optimal_policy(True)
        )
        self._otp = get_transition_probabilities(T, pi)
    return self._otp</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.optimal_value"><code class="name">var <span class="ident">optimal_value</span> : Tuple[numpy.ndarray, numpy.ndarray]</code></dt>
<dd>
<div class="desc"><p>returns the optimal q and state values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    returns the optimal q and state values.
    &#34;&#34;&#34;
    if self._optimal_value is None:
        self._optimal_value = self._vi(*self.transition_matrix_and_rewards)
    return self._optimal_value</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.parameters"><code class="name">var <span class="ident">parameters</span> : Dict[str, Any]</code></dt>
<dd>
<div class="desc"><p>returns the parameters of the MDP.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
@abc.abstractmethod
def parameters(self) -&gt; Dict[str, Any]:
    &#34;&#34;&#34;
    returns the parameters of the MDP.
    &#34;&#34;&#34;
    return dict(
        seed=self._seed,
        randomize_actions=self._randomize_actions,
        p_lazy=self._p_lazy,
        p_rand=self._p_rand,
        rewards_range=self._rewards_range,
        make_reward_stochastic=self._make_reward_stochastic,
        variance_multipliers=self._variance_multipliers,
    )</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_average_reward"><code class="name">var <span class="ident">random_average_reward</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the expected average reward obtained when following the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_average_reward(self) -&gt; float:
    &#34;&#34;&#34;
    returns the expected average reward obtained when following the randomly acting policy.
    &#34;&#34;&#34;
    if self._rar is None:
        self._rar = sum(
            self.random_stationary_distribution * self.random_average_rewards
        )
    return self._rar</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_average_rewards"><code class="name">var <span class="ident">random_average_rewards</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the expected rewards obtained at each state when following the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_average_rewards(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the expected rewards obtained at each state when following the randomly acting policy.
    &#34;&#34;&#34;
    if self._rars is None:
        R = self.R_cf if self.is_episodic() else self.R
        pi = self.random_policy_cf if self.is_episodic() else self.random_policy
        self._rars = get_average_rewards(R, pi)
    return self._rars</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_markov_chain"><code class="name">var <span class="ident">random_markov_chain</span> : pydtmc.markov_chain.MarkovChain</code></dt>
<dd>
<div class="desc"><p>returns the Markov chain corresponding to the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_markov_chain(self) -&gt; MarkovChain:
    &#34;&#34;&#34;
    returns the Markov chain corresponding to the randomly acting policy.
    &#34;&#34;&#34;
    if self._rmc is None:
        self._rmc = get_markov_chain(self.random_transition_probabilities)
    return self._rmc</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_stationary_distribution"><code class="name">var <span class="ident">random_stationary_distribution</span> : <built-in function array></code></dt>
<dd>
<div class="desc"><p>returns the stationary distribution yielded by the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_stationary_distribution(self) -&gt; np.array:
    &#34;&#34;&#34;
    returns the stationary distribution yielded by the randomly acting policy.
    &#34;&#34;&#34;
    if self._rsd is None:
        self._rsd = get_stationary_distribution(
            self.random_transition_probabilities,
            None,
        )
    return self._rsd</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_transition_probabilities"><code class="name">var <span class="ident">random_transition_probabilities</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the transition probabilities corresponding to the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_transition_probabilities(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the transition probabilities corresponding to the randomly acting policy.
    &#34;&#34;&#34;
    if self._rtp is None:
        T = self.T_cf if self.is_episodic() else self.T
        pi = self.random_policy_cf if self.is_episodic() else self.random_policy
        self._rtp = get_transition_probabilities(T, pi)
    return self._rtp</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_value"><code class="name">var <span class="ident">random_value</span> : Tuple[numpy.ndarray, numpy.ndarray]</code></dt>
<dd>
<div class="desc"><p>returns the q and state values for the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    returns the q and state values for the randomly acting policy.
    &#34;&#34;&#34;
    if self._random_value is None:
        self._random_value = self._pe(
            *self.transition_matrix_and_rewards, self.random_policy
        )
    return self._random_value</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.recurrent_nodes_set"><code class="name">var <span class="ident">recurrent_nodes_set</span> : Iterable[NODE_TYPE]</code></dt>
<dd>
<div class="desc"><p>returns the recurrent node set.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def recurrent_nodes_set(self) -&gt; Iterable[&#34;NODE_TYPE&#34;]:
    &#34;&#34;&#34;
    returns the recurrent node set.
    &#34;&#34;&#34;
    if self._recurrent_nodes_set is None:
        self._recurrent_nodes_set = get_recurrent_nodes_set(
            self.communication_class, self.G
        )
    return self._recurrent_nodes_set</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps"><code class="name">var <span class="ident">sum_reciprocals_suboptimality_gaps</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the sum of the reciprocals of the sub-optimality gaps.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def sum_reciprocals_suboptimality_gaps(
    self: Union[&#34;EpisodicMDP&#34;, &#34;ContinuousMDP&#34;]
) -&gt; float:
    &#34;&#34;&#34;
    returns the sum of the reciprocals of the sub-optimality gaps.
    &#34;&#34;&#34;
    if self._sum_reciprocals_suboptimality_gaps is None:
        if self.hardness_report:
            self._sum_reciprocals_suboptimality_gaps = self.hardness_report[
                &#34;MDP measure of hardness&#34;
            ][&#34;suboptimal_gaps&#34;]
        else:
            self._sum_reciprocals_suboptimality_gaps = (
                get_sum_reciprocals_suboptimality_gaps(
                    *self.optimal_value,
                    self.reachable_states if self.is_episodic() else None,
                )
            )
    return self._sum_reciprocals_suboptimality_gaps</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.summary"><code class="name">var <span class="ident">summary</span> : Dict[str, Dict[str, Any]]</code></dt>
<dd>
<div class="desc"><p>returns a dictionary with information about the parameters, the measures of hardness and the graph metrics.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def summary(self) -&gt; Dict[str, Dict[str, Any]]:
    &#34;&#34;&#34;
    returns a dictionary with information about the parameters, the measures of hardness and the graph metrics.
    &#34;&#34;&#34;
    if self._summary is None:
        self._summary = {
            &#34;Parameters&#34;: clean_for_storing(self.parameters),
            &#34;Measure of hardness&#34;: clean_for_storing(self.measures_of_hardness),
            &#34;Graph metrics&#34;: clean_for_storing(self.graph_metrics),
        }
    return self._summary</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards"><code class="name">var <span class="ident">transition_matrix_and_rewards</span> : Tuple[numpy.ndarray, numpy.ndarray]</code></dt>
<dd>
<div class="desc"><p>returns the transition probabilities matrix and the reward matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def transition_matrix_and_rewards(self) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    returns the transition probabilities matrix and the reward matrix.
    &#34;&#34;&#34;
    if self._transition_matrix_and_rewards is None:
        self._transition_matrix_and_rewards = get_transition_matrix_and_rewards(
            self.n_states,
            self.n_actions,
            self.G,
            self.get_info_class,
            self.get_reward_distribution,
            self.node_to_index,
            self._force_sparse_transition
        )
    return self._transition_matrix_and_rewards</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.undiscounted_value_norm"><code class="name">var <span class="ident">undiscounted_value_norm</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the undiscounted environmental value norm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def undiscounted_value_norm(self) -&gt; float:
    &#34;&#34;&#34;
    returns the undiscounted environmental value norm.
    &#34;&#34;&#34;
    if False not in self._optimal_value_norm:
        self._optimal_value_norm[False] = self._compute_value_norm(False)
    return self._optimal_value_norm[False]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.value_norm"><code class="name">var <span class="ident">value_norm</span></code></dt>
<dd>
<div class="desc"><p>is an alias for the discounted value norm.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def value_norm(self):
    &#34;&#34;&#34;
    is an alias for the discounted value norm.
    &#34;&#34;&#34;
    return self.discounted_value_norm</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.worst_average_reward"><code class="name">var <span class="ident">worst_average_reward</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the expected average reward obtained when following the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_average_reward(self) -&gt; float:
    &#34;&#34;&#34;
    returns the expected average reward obtained when following the worst performing policy.
    &#34;&#34;&#34;
    if self._war is None:
        self._war = sum(
            self.worst_stationary_distribution * self.worst_average_rewards
        )
    return self._war</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.worst_average_rewards"><code class="name">var <span class="ident">worst_average_rewards</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the expected rewards obtained at each state when following the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_average_rewards(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the expected rewards obtained at each state when following the worst performing policy.
    &#34;&#34;&#34;
    if self._wars is None:
        R = self.R_cf if self.is_episodic() else self.R
        pi = (
            self.get_worst_policy_continuous_form(True)
            if self.is_episodic()
            else self.get_worst_policy(True)
        )
        self._wars = get_average_rewards(R, pi)
    return self._wars</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.worst_markov_chain"><code class="name">var <span class="ident">worst_markov_chain</span> : pydtmc.markov_chain.MarkovChain</code></dt>
<dd>
<div class="desc"><p>returns the Markov chain corresponding to the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_markov_chain(self) -&gt; MarkovChain:
    &#34;&#34;&#34;
    returns the Markov chain corresponding to the worst performing policy.
    &#34;&#34;&#34;
    if self._wmc is None:
        self._wmc = get_markov_chain(self.worst_transition_probabilities)
    return self._wmc</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.worst_stationary_distribution"><code class="name">var <span class="ident">worst_stationary_distribution</span> : <built-in function array></code></dt>
<dd>
<div class="desc"><p>returns the stationary distribution yielded by the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_stationary_distribution(self) -&gt; np.array:
    &#34;&#34;&#34;
    returns the stationary distribution yielded by the worst performing policy.
    &#34;&#34;&#34;
    if self._wsd is None:
        self._wsd = get_stationary_distribution(
            self.worst_transition_probabilities,
            self.starting_states_and_probs,
        )
        assert not np.isnan(self._wsd).any()
    return self._wsd</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.worst_transition_probabilities"><code class="name">var <span class="ident">worst_transition_probabilities</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the transition probabilities corresponding to the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_transition_probabilities(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the transition probabilities corresponding to the worst performing policy.
    &#34;&#34;&#34;
    if self._wtp is None:
        T = self.T_cf if self.is_episodic() else self.T
        pi = (
            self.get_worst_policy_continuous_form(True)
            if self.is_episodic()
            else self.get_worst_policy(True)
        )
        self._wtp = get_transition_probabilities(T, pi)
    return self._wtp</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.worst_value"><code class="name">var <span class="ident">worst_value</span> : Tuple[numpy.ndarray, numpy.ndarray]</code></dt>
<dd>
<div class="desc"><p>returns the q and state values for the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_value(self) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    returns the q and state values for the worst performing policy.
    &#34;&#34;&#34;
    if self._worst_value is None:
        self._worst_value = self._pe(
            *self.transition_matrix_and_rewards, self.get_worst_policy(True)
        )
    return self._worst_value</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="colosseum.mdp.base.BaseMDP.action_spec"><code class="name flex">
<span>def <span class="ident">action_spec</span></span>(<span>self) ‑> dm_env.specs.DiscreteArray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the action spec of the environment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def action_spec(self) -&gt; DiscreteArray:
    &#34;&#34;&#34;
    returns the action spec of the environment.
    &#34;&#34;&#34;
    return DiscreteArray(self.n_actions, name=&#34;action&#34;)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_grid_representation"><code class="name flex">
<span>def <span class="ident">get_grid_representation</span></span>(<span>self, node: NODE_TYPE, h: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>produces an ASCII representation of the node given in input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@abc.abstractmethod
def get_grid_representation(self, node: &#34;NODE_TYPE&#34;, h: int = None):
    &#34;&#34;&#34;
    produces an ASCII representation of the node given in input.
    &#34;&#34;&#34;</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_info_class"><code class="name flex">
<span>def <span class="ident">get_info_class</span></span>(<span>self, n: NODE_TYPE) ‑> <a title="colosseum.mdp.utils.mdp_creation.NodeInfoClass" href="utils/mdp_creation.html#colosseum.mdp.utils.mdp_creation.NodeInfoClass">NodeInfoClass</a></span>
</code></dt>
<dd>
<div class="desc"><p>returns the container class (NodeInfoClass) associated with node n.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_info_class(self, n: &#34;NODE_TYPE&#34;) -&gt; NodeInfoClass:
    &#34;&#34;&#34;
    returns the container class (NodeInfoClass) associated with node n.
    &#34;&#34;&#34;
    return self.G.nodes[n][&#34;info_class&#34;]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_measure_from_name"><code class="name flex">
<span>def <span class="ident">get_measure_from_name</span></span>(<span>self, measure_name: str) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>returns the value of the measure given in input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_measure_from_name(self, measure_name: str) -&gt; float:
    &#34;&#34;&#34;
    returns the value of the measure given in input.
    &#34;&#34;&#34;
    if measure_name == &#34;diameter&#34;:
        return self.diameter
    elif measure_name in [&#34;value_norm&#34;, &#34;environmental_value_norm&#34;]:
        return self.value_norm
    elif measure_name == &#34;suboptimal_gaps&#34;:
        return self.sum_reciprocals_suboptimality_gaps
    else:
        raise ValueError(
            f&#34;{measure_name} is not a valid hardness measure name: available ones are &#34;
            + str(self.get_available_hardness_measures())
        )</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_observation"><code class="name flex">
<span>def <span class="ident">get_observation</span></span>(<span>self, node: NODE_TYPE, h: int = None)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the representation corresponding to the node given in input. Episodic MDPs also requires the current
in-episode time step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_observation(self, node: &#34;NODE_TYPE&#34;, h: int = None):
    &#34;&#34;&#34;
    returns the representation corresponding to the node given in input. Episodic MDPs also requires the current
    in-episode time step.
    &#34;&#34;&#34;
    if self._representation_mapping is None:
        return self.node_to_index[self.cur_node]
    return self._representation_mapping.get_observation(node, h)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_optimal_policy"><code class="name flex">
<span>def <span class="ident">get_optimal_policy</span></span>(<span>self, stochastic_form: bool) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the optimal policy. It can be either in the stochastic form, so in the form of dirac probabaility
distribution or as a simple mapping to integer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_optimal_policy(self, stochastic_form: bool) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the optimal policy. It can be either in the stochastic form, so in the form of dirac probabaility
    distribution or as a simple mapping to integer.
    &#34;&#34;&#34;
    if stochastic_form not in self._optimal_policy:
        self._optimal_policy[stochastic_form] = get_policy_from_q_values(
            self.optimal_value[0], stochastic_form
        )
    return self._optimal_policy[stochastic_form]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_reward_distribution"><code class="name flex">
<span>def <span class="ident">get_reward_distribution</span></span>(<span>self, node: NODE_TYPE, action: ACTION_TYPE, next_node: NODE_TYPE)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the reward distribution for transitioning in next_node when selecting action from node.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_reward_distribution(
    self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
):
    &#34;&#34;&#34;
    returns the reward distribution for transitioning in next_node when selecting action from node.
    &#34;&#34;&#34;
    if (node, action, next_node) not in self._cached_reward_distributions:
        self._cached_reward_distributions[
            node, action, next_node
        ] = self._get_reward_distribution(
            node, self._inverse_action_mapping(node, action), next_node
        )
    return self._cached_reward_distributions[node, action, next_node]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_transition_distributions"><code class="name flex">
<span>def <span class="ident">get_transition_distributions</span></span>(<span>self, node: NODE_TYPE) ‑> Dict[int, Union[scipy.stats._distn_infrastructure.rv_continuous, <a title="colosseum.mdp.utils.custom_samplers.NextStateSampler" href="utils/custom_samplers.html#colosseum.mdp.utils.custom_samplers.NextStateSampler">NextStateSampler</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>returns a dictionary containing the transition distributions for any action at the node given in input.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_transition_distributions(
    self, node: &#34;NODE_TYPE&#34;
) -&gt; Dict[int, Union[rv_continuous, NextStateSampler]]:
    &#34;&#34;&#34;
    returns a dictionary containing the transition distributions for any action at the node given in input.
    &#34;&#34;&#34;
    return self.get_info_class(node).transition_distributions</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_value_node_labels"><code class="name flex">
<span>def <span class="ident">get_value_node_labels</span></span>(<span>self: Union[ForwardRef('ContinuousMDP'), ForwardRef('EpisodicMDP')], V: numpy.ndarray = None) ‑> Dict[NODE_TYPE, float]</span>
</code></dt>
<dd>
<div class="desc"><p>returns a mapping from node to state values. By default, it uses the optimal values.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_value_node_labels(
    self: Union[&#34;ContinuousMDP&#34;, &#34;EpisodicMDP&#34;], V: np.ndarray = None
) -&gt; Dict[&#34;NODE_TYPE&#34;, float]:
    &#34;&#34;&#34;
    returns a mapping from node to state values. By default, it uses the optimal values.
    &#34;&#34;&#34;
    if V is None:
        _, V = self.optimal_value
    else:
        if isinstance(self, EpisodicMDP):
            h, d = V.shape
            assert h == self.H and d == self.n_states
        else:
            assert len(V) == self.n_states
    return {
        node: np.round(
            V[0, self.node_to_index[node]]
            if self.is_episodic()
            else V[self.node_to_index[node]],
            2,
        )
        for node in self.G.nodes
    }</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_visitation_counts"><code class="name flex">
<span>def <span class="ident">get_visitation_counts</span></span>(<span>self, state_only=True) ‑> Dict[Union[NODE_TYPE, Tuple[NODE_TYPE, ACTION_TYPE]], int]</span>
</code></dt>
<dd>
<div class="desc"><p>when state_only is True it returns the visitation counts for the states and when it is False it returns the
visitation counts for state action pairs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_visitation_counts(
    self, state_only=True
) -&gt; Dict[Union[&#34;NODE_TYPE&#34;, Tuple[&#34;NODE_TYPE&#34;, &#34;ACTION_TYPE&#34;]], int]:
    &#34;&#34;&#34;
    when state_only is True it returns the visitation counts for the states and when it is False it returns the
    visitation counts for state action pairs.
    &#34;&#34;&#34;
    if state_only:
        return {
            node: self.get_info_class(node).state_visitation_count
            for node in self.G.nodes
        }
    return {
        (node, a): self.get_info_class(node).actions_visitation_count[a]
        for node in self.G.nodes
        for a in range(self.n_actions)
    }</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.get_worst_policy"><code class="name flex">
<span>def <span class="ident">get_worst_policy</span></span>(<span>self, stochastic_form) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the worst performing policy. It can be either in the stochastic form, so in the form of dirac probabaility
distribution or as a simple mapping to integer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_worst_policy(self, stochastic_form) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the worst performing policy. It can be either in the stochastic form, so in the form of dirac probabaility
    distribution or as a simple mapping to integer.
    &#34;&#34;&#34;
    if stochastic_form not in self._worst_policy:
        self._worst_policy[stochastic_form] = get_policy_from_q_values(
            self._vi(self.T, -self.R)[0], stochastic_form
        )
    return self._worst_policy[stochastic_form]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.instantiate_MDP"><code class="name flex">
<span>def <span class="ident">instantiate_MDP</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def instantiate_MDP(self):
    # Instantiating the MDP
    self._check_parameters_in_input()
    self._starting_node_sampler = self._get_starting_node_sampler()
    self.starting_nodes = self._starting_node_sampler.next_nodes
    self.G = nx.DiGraph()
    self._instantiate_mdp()
    self.n_states = len(self.G.nodes)

    # Some shortcuts variables
    if not self.is_episodic():
        self._vi, self._pe = (
            discounted_value_iteration,
            discounted_policy_evaluation,
        )
    self.random_policy = (
        np.ones((self.n_states, self.n_actions), dtype=np.float32) / self.n_actions
    )

    # Cache node to index mapping
    mapping = self._rng.rand(self.n_states, self.n_actions).argsort(1)
    self.node_to_index = dict()
    self.index_to_node = dict()
    for i, node in enumerate(self.G.nodes):
        self.node_to_index[node] = i
        self.index_to_node[i] = node

    # Compute the starting state distribution
    self.starting_distribution = np.zeros(self.n_states)
    self.starting_states = []
    for n, p in self._starting_node_sampler.next_nodes_and_probs:
        s = self.node_to_index[n]
        self.starting_distribution[s] = p
        self.starting_states.append(s)
    self.starting_states_and_probs = list(
        zip(self.starting_states, self._starting_node_sampler.probs)
    )</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.observation_spec"><code class="name flex">
<span>def <span class="ident">observation_spec</span></span>(<span>self) ‑> dm_env.specs.Array</span>
</code></dt>
<dd>
<div class="desc"><p>returns the observation spec of the environment.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def observation_spec(self) -&gt; Array:
    &#34;&#34;&#34;
    returns the observation spec of the environment.
    &#34;&#34;&#34;
    if self._representation_mapping is None:
        return DiscreteArray(self.n_states, name=&#34;observation&#34;)
    obs = self.get_observation(self.starting_nodes[0], 0)
    return BoundedArray(obs.shape, obs.dtype, -np.inf, np.inf, &#34;observation&#34;)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.random_step"><code class="name flex">
<span>def <span class="ident">random_step</span></span>(<span>self, auto_reset=False) ‑> Tuple[dm_env._environment.TimeStep, int]</span>
</code></dt>
<dd>
<div class="desc"><p>takes a step with a random action and returns both the next step and the random action. If auto_reset is set to
True than it automatically resets episodic MDPs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_step(self, auto_reset=False) -&gt; Tuple[dm_env.TimeStep, int]:
    &#34;&#34;&#34;
    takes a step with a random action and returns both the next step and the random action. If auto_reset is set to
    True than it automatically resets episodic MDPs.
    &#34;&#34;&#34;
    action = int(self._rng.randint(self.action_spec().num_values))
    ts = self.step(action, auto_reset)
    return ts, action</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.reset"><code class="name flex">
<span>def <span class="ident">reset</span></span>(<span>self) ‑> dm_env._environment.TimeStep</span>
</code></dt>
<dd>
<div class="desc"><p>resets the environment to a newly sampled starting node.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset(self) -&gt; dm_env.TimeStep:
    &#34;&#34;&#34;
    resets the environment to a newly sampled starting node.
    &#34;&#34;&#34;
    self.necessary_reset = False
    self.h = 0
    self.cur_node = self.last_starting_node = self._starting_node_sampler.sample()
    node_info_class = self.get_info_class(self.cur_node)
    node_info_class.update_visitation_counts()
    return dm_env.restart(self.get_observation(self.cur_node, self.h))</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.reset_visitation_counts"><code class="name flex">
<span>def <span class="ident">reset_visitation_counts</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>resets the visitation counts to zero for all states and state action pairs.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_visitation_counts(self):
    &#34;&#34;&#34;
    resets the visitation counts to zero for all states and state action pairs.
    &#34;&#34;&#34;
    for node in self.G.nodes:
        self.get_info_class(node).state_visitation_count = 0
        for a in range(self.n_actions):
            self.get_info_class(node).actions_visitation_count[a] = 0</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.sample_reward"><code class="name flex">
<span>def <span class="ident">sample_reward</span></span>(<span>self, node: NODE_TYPE, action: ACTION_TYPE, next_node: NODE_TYPE) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>returns a sample from the reward distribution when transitioning in next_node when selecting action from node.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sample_reward(
    self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
) -&gt; float:
    &#34;&#34;&#34;
    returns a sample from the reward distribution when transitioning in next_node when selecting action from node.
    &#34;&#34;&#34;
    if (node, action, next_node) not in self._cached_rewards or len(
        self._cached_rewards[node, action, next_node]
    ) == 0:
        self._cached_rewards[node, action, next_node] = (
            self.get_reward_distribution(node, action, next_node)
            .rvs(5000, random_state=self._rng)
            .tolist()
        )
    r = self._cached_rewards[node, action, next_node].pop(0)
    return (
        r * (self.rewards_range[1] - self.rewards_range[0]) - self.rewards_range[0]
    )</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base.BaseMDP.step"><code class="name flex">
<span>def <span class="ident">step</span></span>(<span>self, action: int, auto_reset=False) ‑> dm_env._environment.TimeStep</span>
</code></dt>
<dd>
<div class="desc"><p>takes a step in the MDP for the given action. When auto_reset is set to True then it automatically reset the
at the end of the episodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step(self, action: int, auto_reset=False) -&gt; dm_env.TimeStep:
    &#34;&#34;&#34;
    takes a step in the MDP for the given action. When auto_reset is set to True then it automatically reset the
    at the end of the episodes.
    &#34;&#34;&#34;
    if auto_reset and self.necessary_reset:
        return self.reset()
    assert not self.necessary_reset

    # This can be the in episode time step (episodic setting) or the total numuber of time steps (continuous setting)
    self.h += 1

    # In case the action is a numpy array
    action = int(action)

    # Moving the current node according to the action played
    old_node = self.cur_node
    self.cur_node = self.get_info_class(old_node).sample_next_state(action)
    self.last_edge = old_node, self.cur_node
    node_info_class = self.get_info_class(self.cur_node)
    node_info_class.update_visitation_counts(action)

    # Calculate reward and observation
    reward = self.sample_reward(old_node, action, self.cur_node)
    observation = self.get_observation(self.cur_node, self.h)

    # Wrapping the time step in a dm_env.TimeStep
    if self.is_episodic() and self.h &gt;= self.H:
        self.necessary_reset = True
        if self._representation_mapping is None:
            observation = -1
        else:
            observation = np.zeros_like(self.observation_spec().generate_value())
        return dm_env.termination(reward=reward, observation=observation)
    return dm_env.transition(reward=reward, observation=observation)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="colosseum.mdp" href="index.html">colosseum.mdp</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="colosseum.mdp.base.BaseMDP" href="#colosseum.mdp.base.BaseMDP">BaseMDP</a></code></h4>
<ul class="">
<li><code><a title="colosseum.mdp.base.BaseMDP.R" href="#colosseum.mdp.base.BaseMDP.R">R</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.T" href="#colosseum.mdp.base.BaseMDP.T">T</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.action_spec" href="#colosseum.mdp.base.BaseMDP.action_spec">action_spec</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.communication_class" href="#colosseum.mdp.base.BaseMDP.communication_class">communication_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.diameter" href="#colosseum.mdp.base.BaseMDP.diameter">diameter</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.discounted_value_norm" href="#colosseum.mdp.base.BaseMDP.discounted_value_norm">discounted_value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure" href="#colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure">does_seed_change_MDP_structure</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_available_hardness_measures" href="#colosseum.mdp.base.BaseMDP.get_available_hardness_measures">get_available_hardness_measures</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_grid_representation" href="#colosseum.mdp.base.BaseMDP.get_grid_representation">get_grid_representation</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_info_class" href="#colosseum.mdp.base.BaseMDP.get_info_class">get_info_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_measure_from_name" href="#colosseum.mdp.base.BaseMDP.get_measure_from_name">get_measure_from_name</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_node_class" href="#colosseum.mdp.base.BaseMDP.get_node_class">get_node_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_observation" href="#colosseum.mdp.base.BaseMDP.get_observation">get_observation</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_optimal_policy" href="#colosseum.mdp.base.BaseMDP.get_optimal_policy">get_optimal_policy</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_reward_distribution" href="#colosseum.mdp.base.BaseMDP.get_reward_distribution">get_reward_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_transition_distributions" href="#colosseum.mdp.base.BaseMDP.get_transition_distributions">get_transition_distributions</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_value_node_labels" href="#colosseum.mdp.base.BaseMDP.get_value_node_labels">get_value_node_labels</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_visitation_counts" href="#colosseum.mdp.base.BaseMDP.get_visitation_counts">get_visitation_counts</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_worst_policy" href="#colosseum.mdp.base.BaseMDP.get_worst_policy">get_worst_policy</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.graph_layout" href="#colosseum.mdp.base.BaseMDP.graph_layout">graph_layout</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.graph_metrics" href="#colosseum.mdp.base.BaseMDP.graph_metrics">graph_metrics</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.hardness_report" href="#colosseum.mdp.base.BaseMDP.hardness_report">hardness_report</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.hash" href="#colosseum.mdp.base.BaseMDP.hash">hash</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.instantiate_MDP" href="#colosseum.mdp.base.BaseMDP.instantiate_MDP">instantiate_MDP</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.is_episodic" href="#colosseum.mdp.base.BaseMDP.is_episodic">is_episodic</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.measures_of_hardness" href="#colosseum.mdp.base.BaseMDP.measures_of_hardness">measures_of_hardness</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.n_actions" href="#colosseum.mdp.base.BaseMDP.n_actions">n_actions</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.observation_spec" href="#colosseum.mdp.base.BaseMDP.observation_spec">observation_spec</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_average_reward" href="#colosseum.mdp.base.BaseMDP.optimal_average_reward">optimal_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_average_rewards" href="#colosseum.mdp.base.BaseMDP.optimal_average_rewards">optimal_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_markov_chain" href="#colosseum.mdp.base.BaseMDP.optimal_markov_chain">optimal_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_stationary_distribution" href="#colosseum.mdp.base.BaseMDP.optimal_stationary_distribution">optimal_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_transition_probabilities" href="#colosseum.mdp.base.BaseMDP.optimal_transition_probabilities">optimal_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_value" href="#colosseum.mdp.base.BaseMDP.optimal_value">optimal_value</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.parameters" href="#colosseum.mdp.base.BaseMDP.parameters">parameters</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_average_reward" href="#colosseum.mdp.base.BaseMDP.random_average_reward">random_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_average_rewards" href="#colosseum.mdp.base.BaseMDP.random_average_rewards">random_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_markov_chain" href="#colosseum.mdp.base.BaseMDP.random_markov_chain">random_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_stationary_distribution" href="#colosseum.mdp.base.BaseMDP.random_stationary_distribution">random_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_step" href="#colosseum.mdp.base.BaseMDP.random_step">random_step</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_transition_probabilities" href="#colosseum.mdp.base.BaseMDP.random_transition_probabilities">random_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_value" href="#colosseum.mdp.base.BaseMDP.random_value">random_value</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.recurrent_nodes_set" href="#colosseum.mdp.base.BaseMDP.recurrent_nodes_set">recurrent_nodes_set</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.reset" href="#colosseum.mdp.base.BaseMDP.reset">reset</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.reset_visitation_counts" href="#colosseum.mdp.base.BaseMDP.reset_visitation_counts">reset_visitation_counts</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sample_parameters" href="#colosseum.mdp.base.BaseMDP.sample_parameters">sample_parameters</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sample_reward" href="#colosseum.mdp.base.BaseMDP.sample_reward">sample_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.step" href="#colosseum.mdp.base.BaseMDP.step">step</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps" href="#colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps">sum_reciprocals_suboptimality_gaps</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.summary" href="#colosseum.mdp.base.BaseMDP.summary">summary</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards" href="#colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards">transition_matrix_and_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.undiscounted_value_norm" href="#colosseum.mdp.base.BaseMDP.undiscounted_value_norm">undiscounted_value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.value_norm" href="#colosseum.mdp.base.BaseMDP.value_norm">value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_average_reward" href="#colosseum.mdp.base.BaseMDP.worst_average_reward">worst_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_average_rewards" href="#colosseum.mdp.base.BaseMDP.worst_average_rewards">worst_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_markov_chain" href="#colosseum.mdp.base.BaseMDP.worst_markov_chain">worst_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_stationary_distribution" href="#colosseum.mdp.base.BaseMDP.worst_stationary_distribution">worst_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_transition_probabilities" href="#colosseum.mdp.base.BaseMDP.worst_transition_probabilities">worst_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_value" href="#colosseum.mdp.base.BaseMDP.worst_value">worst_value</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>