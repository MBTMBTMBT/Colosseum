<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>colosseum.mdp.utils.markov_chain API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>colosseum.mdp.utils.markov_chain</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import os
from typing import Iterable, List, Optional, Tuple

import networkx as nx
import numba
import numpy as np
import scipy
from pydtmc import MarkovChain
from scipy.sparse import coo_matrix, csr_matrix, lil_matrix


def get_average_reward(
    T: np.ndarray,
    R: np.ndarray,
    policy: np.ndarray,
    next_states_and_probs: Optional,
    sparse_threshold_size: int = 500 * 500,
) -&gt; float:
    &#34;&#34;&#34;
    returns the expected average reward when following policy for the MDP defined by the given transition matrix and
    rewards matrix.
    &#34;&#34;&#34;
    average_rewards = get_average_rewards(R, policy)
    tps = get_transition_probabilities(T, policy)
    sd = get_stationary_distribution(tps, next_states_and_probs, sparse_threshold_size)
    return (average_rewards * sd).sum()


def get_average_rewards(R: np.ndarray, policy: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the expected rewards for each state when following the given policy.
    &#34;&#34;&#34;
    return np.einsum(&#34;sa,sa-&gt;s&#34;, R, policy)


def get_transition_probabilities(T: np.ndarray, policy: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the transition probability matrix of the Markov chain yielded by the given policy.
    &#34;&#34;&#34;
    return np.minimum(1.0, np.einsum(&#34;saj,sa-&gt;sj&#34;, T, policy))


def get_markov_chain(transition_probabilities: np.ndarray) -&gt; MarkovChain:
    &#34;&#34;&#34;
    returns a Markov chain object from the pydtmc package.
    &#34;&#34;&#34;
    return MarkovChain(transition_probabilities)


def get_stationary_distribution(
    tps: np.ndarray,
    starting_states_and_probs: Iterable[Tuple[int, float]],
    sparse_threshold_size: int = 500 * 500,
) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the stationary distribution of the transition matrix. If there are multiple recurrent classes, and so
    multiple stationary distribution, the return stationary distribution is the average of the stationary distributions
    weighted using the starting state distribution.

    Parameters
    ----------
    tps : np.ndarray
        is the transition probabilities matrix.
    starting_states_and_probs : List[Tuple[int, float]]
        is an iterable over the starting states and their corresponding probabilities.
    sparse_threshold_size : int
        is a threshold for the size of the transition probabilities matrix that flags whether it is better to use sparse
        matrices.
    &#34;&#34;&#34;
    if tps.size &gt; sparse_threshold_size:
        G = nx.DiGraph(coo_matrix(tps))
    else:
        G = nx.DiGraph(tps)

    # Obtain the recurrent classes
    recurrent_classes = list(map(tuple, nx.attracting_components(G)))
    if len(recurrent_classes) == 1 and len(recurrent_classes[0]) &lt; len(tps):
        sd = np.zeros(len(tps), np.float32)
        if len(recurrent_classes[0]) == 1:
            sd[recurrent_classes[0][0]] = 1
        else:
            sd[list(recurrent_classes[0])] = _get_stationary_distribution(
                tps[np.ix_(recurrent_classes[0], recurrent_classes[0])],
                sparse_threshold_size,
            )
        return sd

    elif len(recurrent_classes) &gt; 1 and len(recurrent_classes[0]) &lt; len(tps):

        # Calculate the stationary distribution for each recurrent class
        recurrent_classes_sds = dict()
        for recurrent_class in recurrent_classes:
            recurrent_classes_sds[recurrent_class] = _get_stationary_distribution(
                tps[np.ix_(recurrent_class, recurrent_class)], sparse_threshold_size
            )

        sd = np.zeros(len(tps))
        if len(recurrent_classes) &gt; 1:
            # Weight the stationary distribution of the recurrent classes with starting states distribution
            for ss, p in starting_states_and_probs:
                for recurrent_class in recurrent_classes:
                    try:
                        # this means that the starting state ss is connected to recurrent_class
                        nx.shortest_path_length(G, ss, recurrent_class[0])
                        sd[list(recurrent_class)] += (
                            p * recurrent_classes_sds[recurrent_class]
                        )
                        break
                    except nx.exception.NetworkXNoPath:
                        pass
        else:
            # No need to weight with the starting state distribution since there is only one recurrent class
            sd[list(recurrent_class)] += recurrent_classes_sds[recurrent_class]

        return sd

    sd = _get_stationary_distribution(tps)
    return sd


@numba.njit()
def _gth_solve_numba(tps: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the stationary distribution of a transition probabilities matrix with a single recurrent class using the
    GTH method.
    &#34;&#34;&#34;
    a = np.copy(tps).astype(np.float64)
    n = a.shape[0]

    for i in range(n - 1):
        scale = np.sum(a[i, i + 1 : n])

        if scale &lt;= 0.0:  # pragma: no cover
            n = i + 1
            break

        a[i + 1 : n, i] /= scale
        a[i + 1 : n, i + 1 : n] += np.outer(
            a[i + 1 : n, i : i + 1], a[i : i + 1, i + 1 : n]
        )

    x = np.zeros(n, np.float64)
    x[n - 1] = 1.0
    x[n - 2] = a[n - 1, n - 2]
    for i in range(n - 3, -1, -1):
        x[i] = np.sum(x[i + 1 : n] * a[i + 1 : n, i])
    x /= np.sum(x)
    return x


def _convertToRateMatrix(tps: csr_matrix):
    &#34;&#34;&#34;
    converts the initial matrix to a rate matrix. We make all rows in Q sum to zero by subtracting the row sums from the
    diagonal.
    &#34;&#34;&#34;
    rowSums = tps.sum(axis=1).getA1()
    idxRange = np.arange(tps.shape[0])
    Qdiag = coo_matrix((rowSums, (idxRange, idxRange)), shape=tps.shape).tocsr()
    return tps - Qdiag


def _eigen_method(tps, tol=1e-8, maxiter=1e5):
    &#34;&#34;&#34;
    returns the stationary distribution of a transition probabilities matrix with a single recurrent class using the
    eigenvalue method.
    &#34;&#34;&#34;
    Q = _convertToRateMatrix(tps)
    size = Q.shape[0]
    guess = np.ones(size, dtype=float)
    w, v = scipy.sparse.linalg.eigs(
        Q.T, k=1, v0=guess, sigma=1e-6, which=&#34;LM&#34;, tol=tol, maxiter=maxiter
    )
    pi = v[:, 0].real
    pi /= pi.sum()
    return np.maximum(pi, 0.0)


def _get_stationary_distribution(
    tps: np.ndarray, sparse_threshold_size: int = 500 * 500
) -&gt; np.ndarray:
    if tps.size &gt; sparse_threshold_size:
        sd = _eigen_method(csr_matrix(tps))
        if np.isnan(sd).any() or not np.isclose(sd.sum(), 1.0):
            # sometimes the eigen method fails so we use gth that is slower but more reliable
            os.makedirs(&#34;tmp/sd_failures&#34;, exist_ok=True)
            for i in range(1000):
                if not os.path.isfile(f&#34;tmp/sd_failures/tps{i}.npy&#34;):
                    np.save(f&#34;tmp/sd_failures/tps{i}.npy&#34;, tps)
                    break

            sd = _gth_solve_numba(tps)
            assert not (np.isnan(sd).any() or not np.isclose(sd.sum(), 1.0)), np.save(
                &#34;tmp/tps.npy&#34;, tps
            )
    else:
        sd = _gth_solve_numba(tps)
        assert not (np.isnan(sd).any() or not np.isclose(sd.sum(), 1.0)), np.save(
            &#34;tmp/tps.npy&#34;, tps
        )
    return sd</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="colosseum.mdp.utils.markov_chain.get_average_reward"><code class="name flex">
<span>def <span class="ident">get_average_reward</span></span>(<span>T: numpy.ndarray, R: numpy.ndarray, policy: numpy.ndarray, next_states_and_probs: Optional, sparse_threshold_size: int = 250000) ‑> float</span>
</code></dt>
<dd>
<div class="desc"><p>returns the expected average reward when following policy for the MDP defined by the given transition matrix and
rewards matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_average_reward(
    T: np.ndarray,
    R: np.ndarray,
    policy: np.ndarray,
    next_states_and_probs: Optional,
    sparse_threshold_size: int = 500 * 500,
) -&gt; float:
    &#34;&#34;&#34;
    returns the expected average reward when following policy for the MDP defined by the given transition matrix and
    rewards matrix.
    &#34;&#34;&#34;
    average_rewards = get_average_rewards(R, policy)
    tps = get_transition_probabilities(T, policy)
    sd = get_stationary_distribution(tps, next_states_and_probs, sparse_threshold_size)
    return (average_rewards * sd).sum()</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.utils.markov_chain.get_average_rewards"><code class="name flex">
<span>def <span class="ident">get_average_rewards</span></span>(<span>R: numpy.ndarray, policy: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the expected rewards for each state when following the given policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_average_rewards(R: np.ndarray, policy: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the expected rewards for each state when following the given policy.
    &#34;&#34;&#34;
    return np.einsum(&#34;sa,sa-&gt;s&#34;, R, policy)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.utils.markov_chain.get_markov_chain"><code class="name flex">
<span>def <span class="ident">get_markov_chain</span></span>(<span>transition_probabilities: numpy.ndarray) ‑> pydtmc.markov_chain.MarkovChain</span>
</code></dt>
<dd>
<div class="desc"><p>returns a Markov chain object from the pydtmc package.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_markov_chain(transition_probabilities: np.ndarray) -&gt; MarkovChain:
    &#34;&#34;&#34;
    returns a Markov chain object from the pydtmc package.
    &#34;&#34;&#34;
    return MarkovChain(transition_probabilities)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.utils.markov_chain.get_stationary_distribution"><code class="name flex">
<span>def <span class="ident">get_stationary_distribution</span></span>(<span>tps: numpy.ndarray, starting_states_and_probs: Iterable[Tuple[int, float]], sparse_threshold_size: int = 250000) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the stationary distribution of the transition matrix. If there are multiple recurrent classes, and so
multiple stationary distribution, the return stationary distribution is the average of the stationary distributions
weighted using the starting state distribution.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>tps</code></strong> :&ensp;<code>np.ndarray</code></dt>
<dd>is the transition probabilities matrix.</dd>
<dt><strong><code>starting_states_and_probs</code></strong> :&ensp;<code>List[Tuple[int, float]]</code></dt>
<dd>is an iterable over the starting states and their corresponding probabilities.</dd>
<dt><strong><code>sparse_threshold_size</code></strong> :&ensp;<code>int</code></dt>
<dd>is a threshold for the size of the transition probabilities matrix that flags whether it is better to use sparse
matrices.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_stationary_distribution(
    tps: np.ndarray,
    starting_states_and_probs: Iterable[Tuple[int, float]],
    sparse_threshold_size: int = 500 * 500,
) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the stationary distribution of the transition matrix. If there are multiple recurrent classes, and so
    multiple stationary distribution, the return stationary distribution is the average of the stationary distributions
    weighted using the starting state distribution.

    Parameters
    ----------
    tps : np.ndarray
        is the transition probabilities matrix.
    starting_states_and_probs : List[Tuple[int, float]]
        is an iterable over the starting states and their corresponding probabilities.
    sparse_threshold_size : int
        is a threshold for the size of the transition probabilities matrix that flags whether it is better to use sparse
        matrices.
    &#34;&#34;&#34;
    if tps.size &gt; sparse_threshold_size:
        G = nx.DiGraph(coo_matrix(tps))
    else:
        G = nx.DiGraph(tps)

    # Obtain the recurrent classes
    recurrent_classes = list(map(tuple, nx.attracting_components(G)))
    if len(recurrent_classes) == 1 and len(recurrent_classes[0]) &lt; len(tps):
        sd = np.zeros(len(tps), np.float32)
        if len(recurrent_classes[0]) == 1:
            sd[recurrent_classes[0][0]] = 1
        else:
            sd[list(recurrent_classes[0])] = _get_stationary_distribution(
                tps[np.ix_(recurrent_classes[0], recurrent_classes[0])],
                sparse_threshold_size,
            )
        return sd

    elif len(recurrent_classes) &gt; 1 and len(recurrent_classes[0]) &lt; len(tps):

        # Calculate the stationary distribution for each recurrent class
        recurrent_classes_sds = dict()
        for recurrent_class in recurrent_classes:
            recurrent_classes_sds[recurrent_class] = _get_stationary_distribution(
                tps[np.ix_(recurrent_class, recurrent_class)], sparse_threshold_size
            )

        sd = np.zeros(len(tps))
        if len(recurrent_classes) &gt; 1:
            # Weight the stationary distribution of the recurrent classes with starting states distribution
            for ss, p in starting_states_and_probs:
                for recurrent_class in recurrent_classes:
                    try:
                        # this means that the starting state ss is connected to recurrent_class
                        nx.shortest_path_length(G, ss, recurrent_class[0])
                        sd[list(recurrent_class)] += (
                            p * recurrent_classes_sds[recurrent_class]
                        )
                        break
                    except nx.exception.NetworkXNoPath:
                        pass
        else:
            # No need to weight with the starting state distribution since there is only one recurrent class
            sd[list(recurrent_class)] += recurrent_classes_sds[recurrent_class]

        return sd

    sd = _get_stationary_distribution(tps)
    return sd</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.utils.markov_chain.get_transition_probabilities"><code class="name flex">
<span>def <span class="ident">get_transition_probabilities</span></span>(<span>T: numpy.ndarray, policy: numpy.ndarray) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the transition probability matrix of the Markov chain yielded by the given policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_transition_probabilities(T: np.ndarray, policy: np.ndarray) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the transition probability matrix of the Markov chain yielded by the given policy.
    &#34;&#34;&#34;
    return np.minimum(1.0, np.einsum(&#34;saj,sa-&gt;sj&#34;, T, policy))</code></pre>
</details>
</dd>
</dl>
</section>
<section>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="colosseum.mdp.utils" href="index.html">colosseum.mdp.utils</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="colosseum.mdp.utils.markov_chain.get_average_reward" href="#colosseum.mdp.utils.markov_chain.get_average_reward">get_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.utils.markov_chain.get_average_rewards" href="#colosseum.mdp.utils.markov_chain.get_average_rewards">get_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.utils.markov_chain.get_markov_chain" href="#colosseum.mdp.utils.markov_chain.get_markov_chain">get_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.utils.markov_chain.get_stationary_distribution" href="#colosseum.mdp.utils.markov_chain.get_stationary_distribution">get_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.utils.markov_chain.get_transition_probabilities" href="#colosseum.mdp.utils.markov_chain.get_transition_probabilities">get_transition_probabilities</a></code></li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>