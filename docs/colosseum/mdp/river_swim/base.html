<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>colosseum.mdp.river_swim.base API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>colosseum.mdp.river_swim.base</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
from dataclasses import dataclass
from enum import IntEnum
from typing import TYPE_CHECKING, Any, Dict, List, Tuple, Type, Union

import numpy as np
from scipy.stats import beta, rv_continuous

from colosseum.mdp import BaseMDP
from colosseum.mdp.utils.custom_samplers import NextStateSampler
from colosseum.utils.miscellanea import check_distributions, deterministic, get_dist

if TYPE_CHECKING:
    from colosseum.mdp import ACTION_TYPE, NODE_TYPE


@dataclass(frozen=True)
class RiverSwimNode:
    X: int

    def __str__(self):
        return f&#34;X={self.X}&#34;

    def __iter__(self):
        return iter((self.X, self.X))


class RiverSwimAction(IntEnum):
    LEFT = 0
    RIGHT = 1


class RiverSwimMDP(BaseMDP, abc.ABC):
    @staticmethod
    def does_seed_change_MDP_structure() -&gt; bool:
        return False

    @staticmethod
    def _sample_parameters(
        n: int, is_episodic: bool, seed: int = None
    ) -&gt; List[Dict[str, Any]]:
        rng = np.random.RandomState(np.random.randint(10_000) if seed is None else seed)
        samples = []
        for _ in range(n):
            p_rand, p_lazy, _ = 0.9 * np.random.dirichlet([0.2, 0.2, 5])
            sample = dict(
                size=np.minimum(2.5 + (200 / (45 * np.random.random() + 11)), 25)
                if is_episodic
                else int((6 * rng.random() + 2) ** 2.2),
                make_reward_stochastic=rng.choice([True, False]),
                p_rand=p_rand,
                p_lazy=p_lazy,
                variance_multipliers=2 * rng.random() + 0.005,
            )
            sample[&#34;p_rand&#34;] = None if sample[&#34;p_rand&#34;] &lt; 0.01 else sample[&#34;p_rand&#34;]
            sample[&#34;p_lazy&#34;] = None if sample[&#34;p_lazy&#34;] &lt; 0.01 else sample[&#34;p_lazy&#34;]

            if sample[&#34;make_reward_stochastic&#34;]:
                sample[&#34;sub_optimal_distribution&#34;] = beta(
                    sample[&#34;variance_multipliers&#34;],
                    sample[&#34;variance_multipliers&#34;] * (1 / 0.2 - 1),
                )
                sample[&#34;optimal_distribution&#34;] = beta(
                    sample[&#34;variance_multipliers&#34;],
                    sample[&#34;variance_multipliers&#34;] * (1 / 0.9 - 1),
                )
                sample[&#34;other_distribution&#34;] = beta(
                    sample[&#34;variance_multipliers&#34;],
                    sample[&#34;variance_multipliers&#34;] * (10 / 0.2 - 1),
                )
            else:
                sample[&#34;sub_optimal_distribution&#34;] = deterministic(5 / 1000)
                sample[&#34;optimal_distribution&#34;] = deterministic(1.0)
                sample[&#34;other_distribution&#34;] = deterministic(0.0)
            samples.append(sample)
        return samples

    @staticmethod
    def get_node_class() -&gt; Type[RiverSwimNode]:
        return RiverSwimNode

    @property
    def n_actions(self) -&gt; int:
        return len(RiverSwimAction)

    def __init__(
        self,
        seed: int,
        size: int,
        optimal_mean_reward: float = 0.9,
        sub_optimal_mean_reward: float = 0.2,
        sub_optimal_distribution: Union[Tuple, rv_continuous] = None,
        optimal_distribution: Union[Tuple, rv_continuous] = None,
        other_distribution: Union[Tuple, rv_continuous] = None,
        make_reward_stochastic=False,
        variance_multipliers: float = 1.0,
        **kwargs,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        seed : int
            the seed used for sampling rewards and next states.
        randomize_actions : bool, optional
            whether the effect of the actions changes for every node. It is particularly important to set this value to
             true when doing experiments to avoid immediately reaching highly rewarding states in some MDPs by just
             selecting the same action repeatedly. By default, it is set to true.
        lazy : float
            the probability of an action not producing any effect on the MDP.
        variance_multipliers : float, optional
            A constant that can be used to increase the variance of the reward distributions without changing their means.
            The lower the value, the higher the variance. By default, it is set to 1.
        size : int
            the size of the chain.
        make_reward_stochastic : bool, optional
            checks whether the rewards are to be made stochastic. By default, it is set to False.
        optimal_mean_reward : float, optional
            if the rewards are made stochastic, this parameter controls the mean reward for the highly rewarding states.
            By default, it is set to 0.9.
        sub_optimal_mean_reward: float, optional
            if the rewards are made stochastic, this parameter controls the mean reward for the suboptimal states.
            By default, it is set to 0.2.
        sub_optimal_distribution : Union[Tuple, rv_continuous], optional
            The distribution of the suboptimal rewarding states. It can be either passed as a tuple containing Beta
            parameters or as a rv_continuous object.
        optimal_distribution : Union[Tuple, rv_continuous], optional
            The distribution of the highly rewarding state. It can be either passed as a tuple containing Beta parameters
            or as a rv_continuous object.
        other_distribution : Union[Tuple, rv_continuous], optional
            The distribution of the other states. It can be either passed as a tuple containing Beta parameters or as a
            rv_continuous object.
        &#34;&#34;&#34;
        if type(sub_optimal_distribution) == tuple:
            sub_optimal_distribution = get_dist(
                sub_optimal_distribution[0], sub_optimal_distribution[1:]
            )
        if type(optimal_distribution) == tuple:
            optimal_distribution = get_dist(
                optimal_distribution[0], optimal_distribution[1:]
            )
        if type(other_distribution) == tuple:
            other_distribution = get_dist(other_distribution[0], other_distribution[1:])

        self._size = size
        self._optimal_mean_reward = optimal_mean_reward
        self._sub_optimal_mean_reward = sub_optimal_mean_reward
        self._optimal_distribution = optimal_distribution
        self._sub_optimal_distribution = sub_optimal_distribution
        self._other_distribution = other_distribution

        dists = [
            sub_optimal_distribution,
            optimal_distribution,
            other_distribution,
        ]
        if dists.count(None) == 0:
            self._sub_optimal_distribution = sub_optimal_distribution
            self._optimal_distribution = optimal_distribution
            self._other_distribution = other_distribution
        else:
            if make_reward_stochastic:
                if self.is_episodic():
                    sub_optimal_mean_reward /= self._size
                self._sub_optimal_distribution = beta(
                    variance_multipliers,
                    variance_multipliers * (1 / sub_optimal_mean_reward - 1),
                )
                self._optimal_distribution = beta(
                    variance_multipliers,
                    variance_multipliers * (1 / optimal_mean_reward - 1),
                )
                self._other_distribution = beta(
                    variance_multipliers,
                    variance_multipliers * (10 / sub_optimal_mean_reward - 1),
                )
            else:
                self._sub_optimal_distribution = deterministic(5 / 1000)
                self._optimal_distribution = deterministic(1.0)
                self._other_distribution = deterministic(0.0)

        super(RiverSwimMDP, self).__init__(
            seed=seed,
            variance_multipliers=variance_multipliers,
            make_reward_stochastic=make_reward_stochastic,
            **kwargs,
        )

    def _get_next_nodes_parameters(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;
    ) -&gt; Tuple[Tuple[dict, float], ...]:
        return (
            (
                dict(
                    X=min(node.X + 1, self._size - 1)
                    if action == RiverSwimAction.RIGHT
                    else max(node.X - 1, 0),
                ),
                1.0,
            ),
        )

    def _get_reward_distribution(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ) -&gt; rv_continuous:
        return (
            self._optimal_distribution
            if node.X == self._size - 1 and action == RiverSwimAction.RIGHT
            else (
                self._sub_optimal_distribution
                if node.X == 0 and action == RiverSwimAction.LEFT
                else self._other_distribution
            )
        )

    def _get_starting_node_sampler(self) -&gt; NextStateSampler:
        return NextStateSampler(next_nodes=self._possible_starting_nodes)

    def _check_parameters_in_input(self):
        super(RiverSwimMDP, self)._check_parameters_in_input()

        assert self._size &gt; 1
        assert self._optimal_mean_reward - 0.1 &gt; self._sub_optimal_mean_reward

        dists = [
            self._sub_optimal_distribution,
            self._optimal_distribution,
            self._other_distribution,
        ]
        check_distributions(
            dists,
            self._make_reward_stochastic,
        )

    def _get_grid_representation(self, node: &#34;NODE_TYPE&#34;):
        grid = np.zeros((1, self._size), dtype=str)
        grid[:, :] = &#34; &#34;
        grid[0, 0] = &#34;S&#34;
        grid[0, -1] = &#34;G&#34;
        grid[0, node.X] = &#34;A&#34;
        return grid

    @property
    def _possible_starting_nodes(self) -&gt; List[&#34;NODE_TYPE&#34;]:
        return [RiverSwimNode(0)]

    @property
    def parameters(self) -&gt; Dict[str, Any]:
        return {
            **super(RiverSwimMDP, self).parameters,
            **dict(
                size=self._size,
                optimal_mean_reward=self._optimal_mean_reward,
                sub_optimal_mean_reward=self._sub_optimal_mean_reward,
                optimal_distribution=self._optimal_distribution,
                sub_optimal_distribution=self._sub_optimal_distribution,
                other_distribution=self._other_distribution,
            ),
        }</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="colosseum.mdp.river_swim.base.RiverSwimAction"><code class="flex name class">
<span>class <span class="ident">RiverSwimAction</span></span>
<span>(</span><span>value, names=None, *, module=None, qualname=None, type=None, start=1)</span>
</code></dt>
<dd>
<div class="desc"><p>An enumeration.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RiverSwimAction(IntEnum):
    LEFT = 0
    RIGHT = 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>enum.IntEnum</li>
<li>builtins.int</li>
<li>enum.Enum</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="colosseum.mdp.river_swim.base.RiverSwimAction.LEFT"><code class="name">var <span class="ident">LEFT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="colosseum.mdp.river_swim.base.RiverSwimAction.RIGHT"><code class="name">var <span class="ident">RIGHT</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
<dt id="colosseum.mdp.river_swim.base.RiverSwimMDP"><code class="flex name class">
<span>class <span class="ident">RiverSwimMDP</span></span>
<span>(</span><span>seed: int, size: int, optimal_mean_reward: float = 0.9, sub_optimal_mean_reward: float = 0.2, sub_optimal_distribution: Union[Tuple[], scipy.stats._distn_infrastructure.rv_continuous] = None, optimal_distribution: Union[Tuple[], scipy.stats._distn_infrastructure.rv_continuous] = None, other_distribution: Union[Tuple[], scipy.stats._distn_infrastructure.rv_continuous] = None, make_reward_stochastic=False, variance_multipliers: float = 1.0, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for Python RL environments.</p>
<p>Observations and valid actions are described with <code>Array</code> specs, defined in
the <code>specs</code> module.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>the seed used for sampling rewards and next states.</dd>
<dt><strong><code>randomize_actions</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>whether the effect of the actions changes for every node. It is particularly important to set this value to
true when doing experiments to avoid immediately reaching highly rewarding states in some MDPs by just
selecting the same action repeatedly. By default, it is set to true.</dd>
<dt><strong><code>lazy</code></strong> :&ensp;<code>float</code></dt>
<dd>the probability of an action not producing any effect on the MDP.</dd>
<dt><strong><code>variance_multipliers</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>A constant that can be used to increase the variance of the reward distributions without changing their means.
The lower the value, the higher the variance. By default, it is set to 1.</dd>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>the size of the chain.</dd>
<dt><strong><code>make_reward_stochastic</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>checks whether the rewards are to be made stochastic. By default, it is set to False.</dd>
<dt><strong><code>optimal_mean_reward</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>if the rewards are made stochastic, this parameter controls the mean reward for the highly rewarding states.
By default, it is set to 0.9.</dd>
<dt><strong><code>sub_optimal_mean_reward</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>if the rewards are made stochastic, this parameter controls the mean reward for the suboptimal states.
By default, it is set to 0.2.</dd>
<dt><strong><code>sub_optimal_distribution</code></strong> :&ensp;<code>Union[Tuple, rv_continuous]</code>, optional</dt>
<dd>The distribution of the suboptimal rewarding states. It can be either passed as a tuple containing Beta
parameters or as a rv_continuous object.</dd>
<dt><strong><code>optimal_distribution</code></strong> :&ensp;<code>Union[Tuple, rv_continuous]</code>, optional</dt>
<dd>The distribution of the highly rewarding state. It can be either passed as a tuple containing Beta parameters
or as a rv_continuous object.</dd>
<dt><strong><code>other_distribution</code></strong> :&ensp;<code>Union[Tuple, rv_continuous]</code>, optional</dt>
<dd>The distribution of the other states. It can be either passed as a tuple containing Beta parameters or as a
rv_continuous object.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RiverSwimMDP(BaseMDP, abc.ABC):
    @staticmethod
    def does_seed_change_MDP_structure() -&gt; bool:
        return False

    @staticmethod
    def _sample_parameters(
        n: int, is_episodic: bool, seed: int = None
    ) -&gt; List[Dict[str, Any]]:
        rng = np.random.RandomState(np.random.randint(10_000) if seed is None else seed)
        samples = []
        for _ in range(n):
            p_rand, p_lazy, _ = 0.9 * np.random.dirichlet([0.2, 0.2, 5])
            sample = dict(
                size=np.minimum(2.5 + (200 / (45 * np.random.random() + 11)), 25)
                if is_episodic
                else int((6 * rng.random() + 2) ** 2.2),
                make_reward_stochastic=rng.choice([True, False]),
                p_rand=p_rand,
                p_lazy=p_lazy,
                variance_multipliers=2 * rng.random() + 0.005,
            )
            sample[&#34;p_rand&#34;] = None if sample[&#34;p_rand&#34;] &lt; 0.01 else sample[&#34;p_rand&#34;]
            sample[&#34;p_lazy&#34;] = None if sample[&#34;p_lazy&#34;] &lt; 0.01 else sample[&#34;p_lazy&#34;]

            if sample[&#34;make_reward_stochastic&#34;]:
                sample[&#34;sub_optimal_distribution&#34;] = beta(
                    sample[&#34;variance_multipliers&#34;],
                    sample[&#34;variance_multipliers&#34;] * (1 / 0.2 - 1),
                )
                sample[&#34;optimal_distribution&#34;] = beta(
                    sample[&#34;variance_multipliers&#34;],
                    sample[&#34;variance_multipliers&#34;] * (1 / 0.9 - 1),
                )
                sample[&#34;other_distribution&#34;] = beta(
                    sample[&#34;variance_multipliers&#34;],
                    sample[&#34;variance_multipliers&#34;] * (10 / 0.2 - 1),
                )
            else:
                sample[&#34;sub_optimal_distribution&#34;] = deterministic(5 / 1000)
                sample[&#34;optimal_distribution&#34;] = deterministic(1.0)
                sample[&#34;other_distribution&#34;] = deterministic(0.0)
            samples.append(sample)
        return samples

    @staticmethod
    def get_node_class() -&gt; Type[RiverSwimNode]:
        return RiverSwimNode

    @property
    def n_actions(self) -&gt; int:
        return len(RiverSwimAction)

    def __init__(
        self,
        seed: int,
        size: int,
        optimal_mean_reward: float = 0.9,
        sub_optimal_mean_reward: float = 0.2,
        sub_optimal_distribution: Union[Tuple, rv_continuous] = None,
        optimal_distribution: Union[Tuple, rv_continuous] = None,
        other_distribution: Union[Tuple, rv_continuous] = None,
        make_reward_stochastic=False,
        variance_multipliers: float = 1.0,
        **kwargs,
    ):
        &#34;&#34;&#34;
        Parameters
        ----------
        seed : int
            the seed used for sampling rewards and next states.
        randomize_actions : bool, optional
            whether the effect of the actions changes for every node. It is particularly important to set this value to
             true when doing experiments to avoid immediately reaching highly rewarding states in some MDPs by just
             selecting the same action repeatedly. By default, it is set to true.
        lazy : float
            the probability of an action not producing any effect on the MDP.
        variance_multipliers : float, optional
            A constant that can be used to increase the variance of the reward distributions without changing their means.
            The lower the value, the higher the variance. By default, it is set to 1.
        size : int
            the size of the chain.
        make_reward_stochastic : bool, optional
            checks whether the rewards are to be made stochastic. By default, it is set to False.
        optimal_mean_reward : float, optional
            if the rewards are made stochastic, this parameter controls the mean reward for the highly rewarding states.
            By default, it is set to 0.9.
        sub_optimal_mean_reward: float, optional
            if the rewards are made stochastic, this parameter controls the mean reward for the suboptimal states.
            By default, it is set to 0.2.
        sub_optimal_distribution : Union[Tuple, rv_continuous], optional
            The distribution of the suboptimal rewarding states. It can be either passed as a tuple containing Beta
            parameters or as a rv_continuous object.
        optimal_distribution : Union[Tuple, rv_continuous], optional
            The distribution of the highly rewarding state. It can be either passed as a tuple containing Beta parameters
            or as a rv_continuous object.
        other_distribution : Union[Tuple, rv_continuous], optional
            The distribution of the other states. It can be either passed as a tuple containing Beta parameters or as a
            rv_continuous object.
        &#34;&#34;&#34;
        if type(sub_optimal_distribution) == tuple:
            sub_optimal_distribution = get_dist(
                sub_optimal_distribution[0], sub_optimal_distribution[1:]
            )
        if type(optimal_distribution) == tuple:
            optimal_distribution = get_dist(
                optimal_distribution[0], optimal_distribution[1:]
            )
        if type(other_distribution) == tuple:
            other_distribution = get_dist(other_distribution[0], other_distribution[1:])

        self._size = size
        self._optimal_mean_reward = optimal_mean_reward
        self._sub_optimal_mean_reward = sub_optimal_mean_reward
        self._optimal_distribution = optimal_distribution
        self._sub_optimal_distribution = sub_optimal_distribution
        self._other_distribution = other_distribution

        dists = [
            sub_optimal_distribution,
            optimal_distribution,
            other_distribution,
        ]
        if dists.count(None) == 0:
            self._sub_optimal_distribution = sub_optimal_distribution
            self._optimal_distribution = optimal_distribution
            self._other_distribution = other_distribution
        else:
            if make_reward_stochastic:
                if self.is_episodic():
                    sub_optimal_mean_reward /= self._size
                self._sub_optimal_distribution = beta(
                    variance_multipliers,
                    variance_multipliers * (1 / sub_optimal_mean_reward - 1),
                )
                self._optimal_distribution = beta(
                    variance_multipliers,
                    variance_multipliers * (1 / optimal_mean_reward - 1),
                )
                self._other_distribution = beta(
                    variance_multipliers,
                    variance_multipliers * (10 / sub_optimal_mean_reward - 1),
                )
            else:
                self._sub_optimal_distribution = deterministic(5 / 1000)
                self._optimal_distribution = deterministic(1.0)
                self._other_distribution = deterministic(0.0)

        super(RiverSwimMDP, self).__init__(
            seed=seed,
            variance_multipliers=variance_multipliers,
            make_reward_stochastic=make_reward_stochastic,
            **kwargs,
        )

    def _get_next_nodes_parameters(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;
    ) -&gt; Tuple[Tuple[dict, float], ...]:
        return (
            (
                dict(
                    X=min(node.X + 1, self._size - 1)
                    if action == RiverSwimAction.RIGHT
                    else max(node.X - 1, 0),
                ),
                1.0,
            ),
        )

    def _get_reward_distribution(
        self, node: &#34;NODE_TYPE&#34;, action: &#34;ACTION_TYPE&#34;, next_node: &#34;NODE_TYPE&#34;
    ) -&gt; rv_continuous:
        return (
            self._optimal_distribution
            if node.X == self._size - 1 and action == RiverSwimAction.RIGHT
            else (
                self._sub_optimal_distribution
                if node.X == 0 and action == RiverSwimAction.LEFT
                else self._other_distribution
            )
        )

    def _get_starting_node_sampler(self) -&gt; NextStateSampler:
        return NextStateSampler(next_nodes=self._possible_starting_nodes)

    def _check_parameters_in_input(self):
        super(RiverSwimMDP, self)._check_parameters_in_input()

        assert self._size &gt; 1
        assert self._optimal_mean_reward - 0.1 &gt; self._sub_optimal_mean_reward

        dists = [
            self._sub_optimal_distribution,
            self._optimal_distribution,
            self._other_distribution,
        ]
        check_distributions(
            dists,
            self._make_reward_stochastic,
        )

    def _get_grid_representation(self, node: &#34;NODE_TYPE&#34;):
        grid = np.zeros((1, self._size), dtype=str)
        grid[:, :] = &#34; &#34;
        grid[0, 0] = &#34;S&#34;
        grid[0, -1] = &#34;G&#34;
        grid[0, node.X] = &#34;A&#34;
        return grid

    @property
    def _possible_starting_nodes(self) -&gt; List[&#34;NODE_TYPE&#34;]:
        return [RiverSwimNode(0)]

    @property
    def parameters(self) -&gt; Dict[str, Any]:
        return {
            **super(RiverSwimMDP, self).parameters,
            **dict(
                size=self._size,
                optimal_mean_reward=self._optimal_mean_reward,
                sub_optimal_mean_reward=self._sub_optimal_mean_reward,
                optimal_distribution=self._optimal_distribution,
                sub_optimal_distribution=self._sub_optimal_distribution,
                other_distribution=self._other_distribution,
            ),
        }</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="colosseum.mdp.base.BaseMDP" href="../base.html#colosseum.mdp.base.BaseMDP">BaseMDP</a></li>
<li>dm_env._environment.Environment</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="colosseum.mdp.river_swim.finite_horizon.RiverSwimEpisodic" href="finite_horizon.html#colosseum.mdp.river_swim.finite_horizon.RiverSwimEpisodic">RiverSwimEpisodic</a></li>
<li><a title="colosseum.mdp.river_swim.infinite_horizon.RiverSwimContinuous" href="infinite_horizon.html#colosseum.mdp.river_swim.infinite_horizon.RiverSwimContinuous">RiverSwimContinuous</a></li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="colosseum.mdp.base.BaseMDP" href="../base.html#colosseum.mdp.base.BaseMDP">BaseMDP</a></b></code>:
<ul class="hlist">
<li><code><a title="colosseum.mdp.base.BaseMDP.R" href="../base.html#colosseum.mdp.base.BaseMDP.R">R</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.T" href="../base.html#colosseum.mdp.base.BaseMDP.T">T</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.action_spec" href="../base.html#colosseum.mdp.base.BaseMDP.action_spec">action_spec</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.communication_class" href="../base.html#colosseum.mdp.base.BaseMDP.communication_class">communication_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.diameter" href="../base.html#colosseum.mdp.base.BaseMDP.diameter">diameter</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.discounted_value_norm" href="../base.html#colosseum.mdp.base.BaseMDP.discounted_value_norm">discounted_value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure" href="../base.html#colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure">does_seed_change_MDP_structure</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_grid_representation" href="../base.html#colosseum.mdp.base.BaseMDP.get_grid_representation">get_grid_representation</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_info_class" href="../base.html#colosseum.mdp.base.BaseMDP.get_info_class">get_info_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_measure_from_name" href="../base.html#colosseum.mdp.base.BaseMDP.get_measure_from_name">get_measure_from_name</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_node_class" href="../base.html#colosseum.mdp.base.BaseMDP.get_node_class">get_node_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_observation" href="../base.html#colosseum.mdp.base.BaseMDP.get_observation">get_observation</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_optimal_policy" href="../base.html#colosseum.mdp.base.BaseMDP.get_optimal_policy">get_optimal_policy</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_reward_distribution" href="../base.html#colosseum.mdp.base.BaseMDP.get_reward_distribution">get_reward_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_transition_distributions" href="../base.html#colosseum.mdp.base.BaseMDP.get_transition_distributions">get_transition_distributions</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_value_node_labels" href="../base.html#colosseum.mdp.base.BaseMDP.get_value_node_labels">get_value_node_labels</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_visitation_counts" href="../base.html#colosseum.mdp.base.BaseMDP.get_visitation_counts">get_visitation_counts</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_worst_policy" href="../base.html#colosseum.mdp.base.BaseMDP.get_worst_policy">get_worst_policy</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.graph_layout" href="../base.html#colosseum.mdp.base.BaseMDP.graph_layout">graph_layout</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.graph_metrics" href="../base.html#colosseum.mdp.base.BaseMDP.graph_metrics">graph_metrics</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.hardness_report" href="../base.html#colosseum.mdp.base.BaseMDP.hardness_report">hardness_report</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.hash" href="../base.html#colosseum.mdp.base.BaseMDP.hash">hash</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.is_episodic" href="../base.html#colosseum.mdp.base.BaseMDP.is_episodic">is_episodic</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.measures_of_hardness" href="../base.html#colosseum.mdp.base.BaseMDP.measures_of_hardness">measures_of_hardness</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.n_actions" href="../base.html#colosseum.mdp.base.BaseMDP.n_actions">n_actions</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.observation_spec" href="../base.html#colosseum.mdp.base.BaseMDP.observation_spec">observation_spec</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_average_reward" href="../base.html#colosseum.mdp.base.BaseMDP.optimal_average_reward">optimal_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_average_rewards" href="../base.html#colosseum.mdp.base.BaseMDP.optimal_average_rewards">optimal_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_markov_chain" href="../base.html#colosseum.mdp.base.BaseMDP.optimal_markov_chain">optimal_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_stationary_distribution" href="../base.html#colosseum.mdp.base.BaseMDP.optimal_stationary_distribution">optimal_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_transition_probabilities" href="../base.html#colosseum.mdp.base.BaseMDP.optimal_transition_probabilities">optimal_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_value" href="../base.html#colosseum.mdp.base.BaseMDP.optimal_value">optimal_value</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.parameters" href="../base.html#colosseum.mdp.base.BaseMDP.parameters">parameters</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_average_reward" href="../base.html#colosseum.mdp.base.BaseMDP.random_average_reward">random_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_average_rewards" href="../base.html#colosseum.mdp.base.BaseMDP.random_average_rewards">random_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_markov_chain" href="../base.html#colosseum.mdp.base.BaseMDP.random_markov_chain">random_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_stationary_distribution" href="../base.html#colosseum.mdp.base.BaseMDP.random_stationary_distribution">random_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_step" href="../base.html#colosseum.mdp.base.BaseMDP.random_step">random_step</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_transition_probabilities" href="../base.html#colosseum.mdp.base.BaseMDP.random_transition_probabilities">random_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_value" href="../base.html#colosseum.mdp.base.BaseMDP.random_value">random_value</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.recurrent_nodes_set" href="../base.html#colosseum.mdp.base.BaseMDP.recurrent_nodes_set">recurrent_nodes_set</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.reset" href="../base.html#colosseum.mdp.base.BaseMDP.reset">reset</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.reset_visitation_counts" href="../base.html#colosseum.mdp.base.BaseMDP.reset_visitation_counts">reset_visitation_counts</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sample_parameters" href="../base.html#colosseum.mdp.base.BaseMDP.sample_parameters">sample_parameters</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sample_reward" href="../base.html#colosseum.mdp.base.BaseMDP.sample_reward">sample_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.step" href="../base.html#colosseum.mdp.base.BaseMDP.step">step</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps" href="../base.html#colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps">sum_reciprocals_suboptimality_gaps</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.summary" href="../base.html#colosseum.mdp.base.BaseMDP.summary">summary</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards" href="../base.html#colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards">transition_matrix_and_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.undiscounted_value_norm" href="../base.html#colosseum.mdp.base.BaseMDP.undiscounted_value_norm">undiscounted_value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.value_norm" href="../base.html#colosseum.mdp.base.BaseMDP.value_norm">value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_average_reward" href="../base.html#colosseum.mdp.base.BaseMDP.worst_average_reward">worst_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_average_rewards" href="../base.html#colosseum.mdp.base.BaseMDP.worst_average_rewards">worst_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_markov_chain" href="../base.html#colosseum.mdp.base.BaseMDP.worst_markov_chain">worst_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_stationary_distribution" href="../base.html#colosseum.mdp.base.BaseMDP.worst_stationary_distribution">worst_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_transition_probabilities" href="../base.html#colosseum.mdp.base.BaseMDP.worst_transition_probabilities">worst_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_value" href="../base.html#colosseum.mdp.base.BaseMDP.worst_value">worst_value</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="colosseum.mdp.river_swim.base.RiverSwimNode"><code class="flex name class">
<span>class <span class="ident">RiverSwimNode</span></span>
<span>(</span><span>X: int)</span>
</code></dt>
<dd>
<div class="desc"><p>RiverSwimNode(X: int)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RiverSwimNode:
    X: int

    def __str__(self):
        return f&#34;X={self.X}&#34;

    def __iter__(self):
        return iter((self.X, self.X))</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="colosseum.mdp.river_swim.base.RiverSwimNode.X"><code class="name">var <span class="ident">X</span> : int</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="colosseum.mdp.river_swim" href="index.html">colosseum.mdp.river_swim</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="colosseum.mdp.river_swim.base.RiverSwimAction" href="#colosseum.mdp.river_swim.base.RiverSwimAction">RiverSwimAction</a></code></h4>
<ul class="">
<li><code><a title="colosseum.mdp.river_swim.base.RiverSwimAction.LEFT" href="#colosseum.mdp.river_swim.base.RiverSwimAction.LEFT">LEFT</a></code></li>
<li><code><a title="colosseum.mdp.river_swim.base.RiverSwimAction.RIGHT" href="#colosseum.mdp.river_swim.base.RiverSwimAction.RIGHT">RIGHT</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="colosseum.mdp.river_swim.base.RiverSwimMDP" href="#colosseum.mdp.river_swim.base.RiverSwimMDP">RiverSwimMDP</a></code></h4>
</li>
<li>
<h4><code><a title="colosseum.mdp.river_swim.base.RiverSwimNode" href="#colosseum.mdp.river_swim.base.RiverSwimNode">RiverSwimNode</a></code></h4>
<ul class="">
<li><code><a title="colosseum.mdp.river_swim.base.RiverSwimNode.X" href="#colosseum.mdp.river_swim.base.RiverSwimNode.X">X</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>