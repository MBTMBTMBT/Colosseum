<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>colosseum.mdp.base_finite API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>colosseum.mdp.base_finite</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import abc
from typing import TYPE_CHECKING, Any, Dict, List, Tuple

import networkx as nx
import numpy as np

from colosseum.dynamic_programming import (
    discounted_policy_iteration,
    discounted_value_iteration,
    episodic_policy_evaluation,
    episodic_value_iteration,
)
from colosseum.dynamic_programming.utils import get_policy_from_q_values
from colosseum.mdp import BaseMDP
from colosseum.mdp.utils.mdp_creation import (
    get_continuous_form_episodic_transition_matrix_and_rewards,
    get_episodic_graph,
    get_episodic_transition_matrix_and_rewards,
)

if TYPE_CHECKING:
    from colosseum.mdp import NODE_TYPE


class EpisodicMDP(BaseMDP, abc.ABC):
    @staticmethod
    def is_episodic() -&gt; bool:
        return True

    def __init__(self, H: int = None, **kwargs):
        super(EpisodicMDP, self).__init__(**kwargs)

        # Computing the time horizon
        self._set_time_horizon(H)

        # Episodic setting specific caching variables
        self._reachable_states = None
        self._episodic_graph = dict()
        self._continuous_form_episodic_transition_matrix_and_rewards = None
        self._episodic_transition_matrix_and_rewards = None
        self._optimal_policy_cf = dict()
        self._worst_policy_cf = dict()
        self._optimal_value_cf = None
        self._worst_value_cf = None
        self._random_value_cf = None
        self._eoar = None
        self._woar = None
        self._roar = None
        self.random_policy_cf = (
            np.ones(
                (len(self.get_episodic_graph(True).nodes), self.n_actions), np.float32
            )
            / self.n_actions
        )
        self.random_policy = (
            np.ones((self.H, self.n_states, self.n_actions), np.float32)
            / self.n_actions
        )

    def _set_time_horizon(self, H: int):
        &#34;&#34;&#34;
        calculates a meaningful minimal horizon for the MDP.
        &#34;&#34;&#34;
        if &#34;Taxi&#34; in str(type(self)):
            # it is complicated to give the same horizon to different seed of the same MDP instance
            # for the Taxi MDP
            minimal_H = int(1.5 * self._size ** 2)
        else:
            minimal_H = (
                max(
                    max(nx.shortest_path_length(self.G, sn).values())
                    for sn in self._possible_starting_nodes
                )
                + 1
            )
        if H is None:
            self.H = self._H = minimal_H
        else:
            self.H = self._H = max(minimal_H, H)

    def _vi(self, *args):
        return episodic_value_iteration(self.H, *args)

    def _pe(self, *args):
        return episodic_policy_evaluation(self.H, *args)

    @property
    def parameters(self) -&gt; Dict[str, Any]:
        parameters = super(EpisodicMDP, self).parameters
        parameters[&#34;H&#34;] = self.H
        return parameters

    @property
    def reachable_states(self) -&gt; List[Tuple[int, &#34;NODE_TYPE&#34;]]:
        &#34;&#34;&#34;
        returns the pairs of in episode time step and state that are feasible.
        &#34;&#34;&#34;
        if self._reachable_states is None:
            self._reachable_states = [
                (h, self.node_to_index[n])
                for h, n in self.get_episodic_graph(False).nodes
            ]
        return self._reachable_states

    @property
    def T_cf(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the continuous form of the transition matrix.
        &#34;&#34;&#34;
        return self.continuous_form_episodic_transition_matrix_and_rewards[0]

    @property
    def R_cf(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the continuous form of the rewards matrix.
        &#34;&#34;&#34;
        return self.continuous_form_episodic_transition_matrix_and_rewards[1]

    @property
    def optimal_value_continuous_form(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the optimal q and state values computed for the continuous form.
        &#34;&#34;&#34;
        if self._optimal_value_cf is None:
            self._optimal_value_cf = discounted_value_iteration(self.T_cf, self.R_cf)
        return self._optimal_value_cf

    @property
    def worst_value_continuous_form(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the q and state values for the worst performing policy computed for the continuous form.
        &#34;&#34;&#34;
        if self._worst_value_cf is None:
            self._worst_value_cf = discounted_value_iteration(self.T_cf, -self.R_cf)
        return self._worst_value_cf

    @property
    def random_value_continuous_form(self):
        &#34;&#34;&#34;
        returns the q and state values for the randomly acting policy computed for the continuous form.
        &#34;&#34;&#34;
        if self._random_value_cf is None:
            self._random_value_cf = discounted_policy_iteration(
                self.T_cf, self.R_cf, self.random_policy_cf
            )
        return self._random_value_cf

    @property
    def episodic_optimal_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected value of time step zero under the starting state distribution for the optimal policy.
        &#34;&#34;&#34;
        if self._eoar is None:
            _eoar = 0.0
            for sn, p in self._starting_node_sampler.next_nodes_and_probs:
                _eoar += p * self.get_optimal_policy_starting_value(sn)
            self._eoar = _eoar / self.H
        return self._eoar

    @property
    def episodic_worst_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected value of time step zero under the starting state distribution for the worst performing policy.
        &#34;&#34;&#34;
        if self._woar is None:
            _woar = 0.0
            for sn, p in self._starting_node_sampler.next_nodes_and_probs:
                _woar += p * self.get_worst_policy_starting_value(sn)
            self._woar = _woar / self.H
        return self._woar

    @property
    def episodic_random_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected value of time step zero under the starting state distribution for the randomly acting policy.
        &#34;&#34;&#34;
        if self._roar is None:
            _roar = 0.0
            for sn, p in self._starting_node_sampler.next_nodes_and_probs:
                _roar += p * self.get_random_policy_starting_value(sn)
            self._roar = _roar / self.H
        return self._roar

    @property
    def continuous_form_episodic_transition_matrix_and_rewards(self):
        &#34;&#34;&#34;
        returns the transition matrix and rewards matrix for the continous form, i.e. when the state space is augmented
        with the in episode time step.
        &#34;&#34;&#34;
        if self._continuous_form_episodic_transition_matrix_and_rewards is None:
            self._continuous_form_episodic_transition_matrix_and_rewards = (
                get_continuous_form_episodic_transition_matrix_and_rewards(
                    self.H,
                    self.get_episodic_graph(True),
                    *self.transition_matrix_and_rewards,
                    self._starting_node_sampler,
                    self.node_to_index,
                )
            )
        return self._continuous_form_episodic_transition_matrix_and_rewards

    @property
    def episodic_transition_matrix_and_rewards(self):
        &#34;&#34;&#34;
        returns the episodic transition matrix and episodic rewards matrix, i.e. with an additional dimensional to
        account for the in episode time step.
        &#34;&#34;&#34;
        if self._episodic_transition_matrix_and_rewards is None:
            self._episodic_transition_matrix_and_rewards = (
                get_episodic_transition_matrix_and_rewards(
                    self.H,
                    *self.transition_matrix_and_rewards,
                    self._starting_node_sampler,
                    self.node_to_index,
                )
            )
        return self._episodic_transition_matrix_and_rewards

    def get_optimal_policy_continuous_form(self, stochastic_form: bool) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the optimal policy computed for the continuous form.
        &#34;&#34;&#34;
        if stochastic_form not in self._optimal_policy_cf:
            self._optimal_policy_cf[stochastic_form] = get_policy_from_q_values(
                self.optimal_value_continuous_form[0], stochastic_form
            )
        return self._optimal_policy_cf[stochastic_form]

    def get_worst_policy_continuous_form(self, stochastic_form) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the worst policy computed for the continuous form.
        &#34;&#34;&#34;
        if stochastic_form not in self._worst_policy_cf:
            self._worst_policy_cf[stochastic_form] = get_policy_from_q_values(
                self.worst_value_continuous_form[0], stochastic_form
            )
        return self._worst_policy_cf[stochastic_form]

    def get_minimal_regret_for_starting_node(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the minimal possible regret obtained from the given starting state.
        &#34;&#34;&#34;
        return self.get_optimal_policy_starting_value(
            node
        ) - self.get_worst_policy_starting_value(node)

    def get_optimal_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the value of the given node at in episode time step zero for the optimal policy.
        &#34;&#34;&#34;
        return self.optimal_value[1][0, self.node_to_index[node]]

    def get_worst_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the value of the given node at in episode time step zero for the worst performing policy.
        &#34;&#34;&#34;
        return self.worst_value[1][0, self.node_to_index[node]]

    def get_random_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the value of the given node at in episode time step zero for the randomly acting policy.
        &#34;&#34;&#34;
        return self.random_value[1][0, self.node_to_index[node]]

    def get_episodic_graph(self, remove_labels: bool) -&gt; nx.DiGraph:
        &#34;&#34;&#34;
        returns the graph corresponding the state space augmented with the in episode time step. It is possible to remove
        the labels that mark the nodes.
        &#34;&#34;&#34;
        if remove_labels not in self._episodic_graph:
            self._episodic_graph[remove_labels] = get_episodic_graph(
                self.G, self.H, self.node_to_index, self.starting_nodes, remove_labels
            )
        return self._episodic_graph[remove_labels]

    def get_grid_representation(self, node: &#34;NODE_TYPE&#34;, h : int = None) -&gt; np.array:
        if h is None:
            h = self.h
        grid = self._get_grid_representation(node)
        while grid.shape[1] &lt; 2 + len(str(self.h)):
            adder = np.zeros((grid.shape[1], 1), dtype=str)
            adder[:] = &#34;X&#34;
            grid = np.hstack((grid, adder))
        title = np.array(
            [&#34; &#34;] * grid.shape[1] + [&#34;_&#34;] * grid.shape[1], dtype=str
        ).reshape(2, -1)
        title[0, 0] = &#34;H&#34;
        title[0, 1] = &#34;=&#34;
        for i, l in enumerate(str(h)):
            title[0, 2 + i] = l
        return np.vstack((title, grid))</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="colosseum.mdp.base_finite.EpisodicMDP"><code class="flex name class">
<span>class <span class="ident">EpisodicMDP</span></span>
<span>(</span><span>H: int = None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Abstract base class for Python RL environments.</p>
<p>Observations and valid actions are described with <code>Array</code> specs, defined in
the <code>specs</code> module.</p>
<p>instantiates the MDP.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>seed</code></strong> :&ensp;<code>int</code></dt>
<dd>is the random seed.</dd>
<dt><strong><code>randomize_actions</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>checks whether to apply a random mapping to the actions for each state. This avoids issues linked to
the possible bias of the agents to always take action zero at the beginning of the interactions.
By default, it is set to true.</dd>
<dt><strong><code>variance_multipliers</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>A constant that can be used to increase the variance of the reward distributions without changing their means.
The lower the value, the higher the variance. By default, it is set to 1.</dd>
<dt><strong><code>p_lazy</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>is the probability of an action not producing any effect on the MDP.
By default, it is set to zero.</dd>
<dt><strong><code>p_rand</code></strong> :&ensp;<code>float</code>, optional</dt>
<dd>is the probability of selecting an action at random instead of the one specified by the agent.
By default, it is set to zero.</dd>
<dt><strong><code>rewards_range</code></strong> :&ensp;<code>Tuple[float, float]</code>, optional</dt>
<dd>is the maximum value of the reward.
By default, it is set to the zero one interval.</dd>
<dt><strong><code>representation_mapping</code></strong> :&ensp;<code>RepresentationMapping</code></dt>
<dd>the representation mapping assigned to each state. By default, no representation mapping is used.</dd>
<dt><strong><code>hardness_reports_folder</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>is the path where the MDP looks for previously cached hardness reports.</dd>
<dt><strong><code>instantiate_mdp</code></strong> :&ensp;<code>bool</code></dt>
<dd>checks whether to immediately instantiate the MDP.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EpisodicMDP(BaseMDP, abc.ABC):
    @staticmethod
    def is_episodic() -&gt; bool:
        return True

    def __init__(self, H: int = None, **kwargs):
        super(EpisodicMDP, self).__init__(**kwargs)

        # Computing the time horizon
        self._set_time_horizon(H)

        # Episodic setting specific caching variables
        self._reachable_states = None
        self._episodic_graph = dict()
        self._continuous_form_episodic_transition_matrix_and_rewards = None
        self._episodic_transition_matrix_and_rewards = None
        self._optimal_policy_cf = dict()
        self._worst_policy_cf = dict()
        self._optimal_value_cf = None
        self._worst_value_cf = None
        self._random_value_cf = None
        self._eoar = None
        self._woar = None
        self._roar = None
        self.random_policy_cf = (
            np.ones(
                (len(self.get_episodic_graph(True).nodes), self.n_actions), np.float32
            )
            / self.n_actions
        )
        self.random_policy = (
            np.ones((self.H, self.n_states, self.n_actions), np.float32)
            / self.n_actions
        )

    def _set_time_horizon(self, H: int):
        &#34;&#34;&#34;
        calculates a meaningful minimal horizon for the MDP.
        &#34;&#34;&#34;
        if &#34;Taxi&#34; in str(type(self)):
            # it is complicated to give the same horizon to different seed of the same MDP instance
            # for the Taxi MDP
            minimal_H = int(1.5 * self._size ** 2)
        else:
            minimal_H = (
                max(
                    max(nx.shortest_path_length(self.G, sn).values())
                    for sn in self._possible_starting_nodes
                )
                + 1
            )
        if H is None:
            self.H = self._H = minimal_H
        else:
            self.H = self._H = max(minimal_H, H)

    def _vi(self, *args):
        return episodic_value_iteration(self.H, *args)

    def _pe(self, *args):
        return episodic_policy_evaluation(self.H, *args)

    @property
    def parameters(self) -&gt; Dict[str, Any]:
        parameters = super(EpisodicMDP, self).parameters
        parameters[&#34;H&#34;] = self.H
        return parameters

    @property
    def reachable_states(self) -&gt; List[Tuple[int, &#34;NODE_TYPE&#34;]]:
        &#34;&#34;&#34;
        returns the pairs of in episode time step and state that are feasible.
        &#34;&#34;&#34;
        if self._reachable_states is None:
            self._reachable_states = [
                (h, self.node_to_index[n])
                for h, n in self.get_episodic_graph(False).nodes
            ]
        return self._reachable_states

    @property
    def T_cf(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the continuous form of the transition matrix.
        &#34;&#34;&#34;
        return self.continuous_form_episodic_transition_matrix_and_rewards[0]

    @property
    def R_cf(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        is an alias for the continuous form of the rewards matrix.
        &#34;&#34;&#34;
        return self.continuous_form_episodic_transition_matrix_and_rewards[1]

    @property
    def optimal_value_continuous_form(self) -&gt; Tuple[np.ndarray, np.ndarray]:
        &#34;&#34;&#34;
        returns the optimal q and state values computed for the continuous form.
        &#34;&#34;&#34;
        if self._optimal_value_cf is None:
            self._optimal_value_cf = discounted_value_iteration(self.T_cf, self.R_cf)
        return self._optimal_value_cf

    @property
    def worst_value_continuous_form(self) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the q and state values for the worst performing policy computed for the continuous form.
        &#34;&#34;&#34;
        if self._worst_value_cf is None:
            self._worst_value_cf = discounted_value_iteration(self.T_cf, -self.R_cf)
        return self._worst_value_cf

    @property
    def random_value_continuous_form(self):
        &#34;&#34;&#34;
        returns the q and state values for the randomly acting policy computed for the continuous form.
        &#34;&#34;&#34;
        if self._random_value_cf is None:
            self._random_value_cf = discounted_policy_iteration(
                self.T_cf, self.R_cf, self.random_policy_cf
            )
        return self._random_value_cf

    @property
    def episodic_optimal_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected value of time step zero under the starting state distribution for the optimal policy.
        &#34;&#34;&#34;
        if self._eoar is None:
            _eoar = 0.0
            for sn, p in self._starting_node_sampler.next_nodes_and_probs:
                _eoar += p * self.get_optimal_policy_starting_value(sn)
            self._eoar = _eoar / self.H
        return self._eoar

    @property
    def episodic_worst_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected value of time step zero under the starting state distribution for the worst performing policy.
        &#34;&#34;&#34;
        if self._woar is None:
            _woar = 0.0
            for sn, p in self._starting_node_sampler.next_nodes_and_probs:
                _woar += p * self.get_worst_policy_starting_value(sn)
            self._woar = _woar / self.H
        return self._woar

    @property
    def episodic_random_average_reward(self) -&gt; float:
        &#34;&#34;&#34;
        returns the expected value of time step zero under the starting state distribution for the randomly acting policy.
        &#34;&#34;&#34;
        if self._roar is None:
            _roar = 0.0
            for sn, p in self._starting_node_sampler.next_nodes_and_probs:
                _roar += p * self.get_random_policy_starting_value(sn)
            self._roar = _roar / self.H
        return self._roar

    @property
    def continuous_form_episodic_transition_matrix_and_rewards(self):
        &#34;&#34;&#34;
        returns the transition matrix and rewards matrix for the continous form, i.e. when the state space is augmented
        with the in episode time step.
        &#34;&#34;&#34;
        if self._continuous_form_episodic_transition_matrix_and_rewards is None:
            self._continuous_form_episodic_transition_matrix_and_rewards = (
                get_continuous_form_episodic_transition_matrix_and_rewards(
                    self.H,
                    self.get_episodic_graph(True),
                    *self.transition_matrix_and_rewards,
                    self._starting_node_sampler,
                    self.node_to_index,
                )
            )
        return self._continuous_form_episodic_transition_matrix_and_rewards

    @property
    def episodic_transition_matrix_and_rewards(self):
        &#34;&#34;&#34;
        returns the episodic transition matrix and episodic rewards matrix, i.e. with an additional dimensional to
        account for the in episode time step.
        &#34;&#34;&#34;
        if self._episodic_transition_matrix_and_rewards is None:
            self._episodic_transition_matrix_and_rewards = (
                get_episodic_transition_matrix_and_rewards(
                    self.H,
                    *self.transition_matrix_and_rewards,
                    self._starting_node_sampler,
                    self.node_to_index,
                )
            )
        return self._episodic_transition_matrix_and_rewards

    def get_optimal_policy_continuous_form(self, stochastic_form: bool) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the optimal policy computed for the continuous form.
        &#34;&#34;&#34;
        if stochastic_form not in self._optimal_policy_cf:
            self._optimal_policy_cf[stochastic_form] = get_policy_from_q_values(
                self.optimal_value_continuous_form[0], stochastic_form
            )
        return self._optimal_policy_cf[stochastic_form]

    def get_worst_policy_continuous_form(self, stochastic_form) -&gt; np.ndarray:
        &#34;&#34;&#34;
        returns the worst policy computed for the continuous form.
        &#34;&#34;&#34;
        if stochastic_form not in self._worst_policy_cf:
            self._worst_policy_cf[stochastic_form] = get_policy_from_q_values(
                self.worst_value_continuous_form[0], stochastic_form
            )
        return self._worst_policy_cf[stochastic_form]

    def get_minimal_regret_for_starting_node(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the minimal possible regret obtained from the given starting state.
        &#34;&#34;&#34;
        return self.get_optimal_policy_starting_value(
            node
        ) - self.get_worst_policy_starting_value(node)

    def get_optimal_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the value of the given node at in episode time step zero for the optimal policy.
        &#34;&#34;&#34;
        return self.optimal_value[1][0, self.node_to_index[node]]

    def get_worst_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the value of the given node at in episode time step zero for the worst performing policy.
        &#34;&#34;&#34;
        return self.worst_value[1][0, self.node_to_index[node]]

    def get_random_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
        &#34;&#34;&#34;
        returns the value of the given node at in episode time step zero for the randomly acting policy.
        &#34;&#34;&#34;
        return self.random_value[1][0, self.node_to_index[node]]

    def get_episodic_graph(self, remove_labels: bool) -&gt; nx.DiGraph:
        &#34;&#34;&#34;
        returns the graph corresponding the state space augmented with the in episode time step. It is possible to remove
        the labels that mark the nodes.
        &#34;&#34;&#34;
        if remove_labels not in self._episodic_graph:
            self._episodic_graph[remove_labels] = get_episodic_graph(
                self.G, self.H, self.node_to_index, self.starting_nodes, remove_labels
            )
        return self._episodic_graph[remove_labels]

    def get_grid_representation(self, node: &#34;NODE_TYPE&#34;, h : int = None) -&gt; np.array:
        if h is None:
            h = self.h
        grid = self._get_grid_representation(node)
        while grid.shape[1] &lt; 2 + len(str(self.h)):
            adder = np.zeros((grid.shape[1], 1), dtype=str)
            adder[:] = &#34;X&#34;
            grid = np.hstack((grid, adder))
        title = np.array(
            [&#34; &#34;] * grid.shape[1] + [&#34;_&#34;] * grid.shape[1], dtype=str
        ).reshape(2, -1)
        title[0, 0] = &#34;H&#34;
        title[0, 1] = &#34;=&#34;
        for i, l in enumerate(str(h)):
            title[0, 2 + i] = l
        return np.vstack((title, grid))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="colosseum.mdp.base.BaseMDP" href="base.html#colosseum.mdp.base.BaseMDP">BaseMDP</a></li>
<li>dm_env._environment.Environment</li>
<li>abc.ABC</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="colosseum.mdp.custom_mdp.CustomEpisodic" href="custom_mdp.html#colosseum.mdp.custom_mdp.CustomEpisodic">CustomEpisodic</a></li>
<li><a title="colosseum.mdp.deep_sea.finite_horizon.DeepSeaEpisodic" href="deep_sea/finite_horizon.html#colosseum.mdp.deep_sea.finite_horizon.DeepSeaEpisodic">DeepSeaEpisodic</a></li>
<li><a title="colosseum.mdp.frozen_lake.finite_horizon.FrozenLakeEpisodic" href="frozen_lake/finite_horizon.html#colosseum.mdp.frozen_lake.finite_horizon.FrozenLakeEpisodic">FrozenLakeEpisodic</a></li>
<li><a title="colosseum.mdp.minigrid_empty.finite_horizon.MiniGridEmptyEpisodic" href="minigrid_empty/finite_horizon.html#colosseum.mdp.minigrid_empty.finite_horizon.MiniGridEmptyEpisodic">MiniGridEmptyEpisodic</a></li>
<li><a title="colosseum.mdp.minigrid_rooms.finite_horizon.MiniGridRoomsEpisodic" href="minigrid_rooms/finite_horizon.html#colosseum.mdp.minigrid_rooms.finite_horizon.MiniGridRoomsEpisodic">MiniGridRoomsEpisodic</a></li>
<li><a title="colosseum.mdp.river_swim.finite_horizon.RiverSwimEpisodic" href="river_swim/finite_horizon.html#colosseum.mdp.river_swim.finite_horizon.RiverSwimEpisodic">RiverSwimEpisodic</a></li>
<li><a title="colosseum.mdp.simple_grid.finite_horizon.SimpleGridEpisodic" href="simple_grid/finite_horizon.html#colosseum.mdp.simple_grid.finite_horizon.SimpleGridEpisodic">SimpleGridEpisodic</a></li>
<li><a title="colosseum.mdp.taxi.finite_horizon.TaxiEpisodic" href="taxi/finite_horizon.html#colosseum.mdp.taxi.finite_horizon.TaxiEpisodic">TaxiEpisodic</a></li>
</ul>
<h3>Instance variables</h3>
<dl>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.R_cf"><code class="name">var <span class="ident">R_cf</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>is an alias for the continuous form of the rewards matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def R_cf(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    is an alias for the continuous form of the rewards matrix.
    &#34;&#34;&#34;
    return self.continuous_form_episodic_transition_matrix_and_rewards[1]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.T_cf"><code class="name">var <span class="ident">T_cf</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>is an alias for the continuous form of the transition matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def T_cf(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    is an alias for the continuous form of the transition matrix.
    &#34;&#34;&#34;
    return self.continuous_form_episodic_transition_matrix_and_rewards[0]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.continuous_form_episodic_transition_matrix_and_rewards"><code class="name">var <span class="ident">continuous_form_episodic_transition_matrix_and_rewards</span></code></dt>
<dd>
<div class="desc"><p>returns the transition matrix and rewards matrix for the continous form, i.e. when the state space is augmented
with the in episode time step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def continuous_form_episodic_transition_matrix_and_rewards(self):
    &#34;&#34;&#34;
    returns the transition matrix and rewards matrix for the continous form, i.e. when the state space is augmented
    with the in episode time step.
    &#34;&#34;&#34;
    if self._continuous_form_episodic_transition_matrix_and_rewards is None:
        self._continuous_form_episodic_transition_matrix_and_rewards = (
            get_continuous_form_episodic_transition_matrix_and_rewards(
                self.H,
                self.get_episodic_graph(True),
                *self.transition_matrix_and_rewards,
                self._starting_node_sampler,
                self.node_to_index,
            )
        )
    return self._continuous_form_episodic_transition_matrix_and_rewards</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.episodic_optimal_average_reward"><code class="name">var <span class="ident">episodic_optimal_average_reward</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the expected value of time step zero under the starting state distribution for the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def episodic_optimal_average_reward(self) -&gt; float:
    &#34;&#34;&#34;
    returns the expected value of time step zero under the starting state distribution for the optimal policy.
    &#34;&#34;&#34;
    if self._eoar is None:
        _eoar = 0.0
        for sn, p in self._starting_node_sampler.next_nodes_and_probs:
            _eoar += p * self.get_optimal_policy_starting_value(sn)
        self._eoar = _eoar / self.H
    return self._eoar</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.episodic_random_average_reward"><code class="name">var <span class="ident">episodic_random_average_reward</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the expected value of time step zero under the starting state distribution for the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def episodic_random_average_reward(self) -&gt; float:
    &#34;&#34;&#34;
    returns the expected value of time step zero under the starting state distribution for the randomly acting policy.
    &#34;&#34;&#34;
    if self._roar is None:
        _roar = 0.0
        for sn, p in self._starting_node_sampler.next_nodes_and_probs:
            _roar += p * self.get_random_policy_starting_value(sn)
        self._roar = _roar / self.H
    return self._roar</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.episodic_transition_matrix_and_rewards"><code class="name">var <span class="ident">episodic_transition_matrix_and_rewards</span></code></dt>
<dd>
<div class="desc"><p>returns the episodic transition matrix and episodic rewards matrix, i.e. with an additional dimensional to
account for the in episode time step.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def episodic_transition_matrix_and_rewards(self):
    &#34;&#34;&#34;
    returns the episodic transition matrix and episodic rewards matrix, i.e. with an additional dimensional to
    account for the in episode time step.
    &#34;&#34;&#34;
    if self._episodic_transition_matrix_and_rewards is None:
        self._episodic_transition_matrix_and_rewards = (
            get_episodic_transition_matrix_and_rewards(
                self.H,
                *self.transition_matrix_and_rewards,
                self._starting_node_sampler,
                self.node_to_index,
            )
        )
    return self._episodic_transition_matrix_and_rewards</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.episodic_worst_average_reward"><code class="name">var <span class="ident">episodic_worst_average_reward</span> : float</code></dt>
<dd>
<div class="desc"><p>returns the expected value of time step zero under the starting state distribution for the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def episodic_worst_average_reward(self) -&gt; float:
    &#34;&#34;&#34;
    returns the expected value of time step zero under the starting state distribution for the worst performing policy.
    &#34;&#34;&#34;
    if self._woar is None:
        _woar = 0.0
        for sn, p in self._starting_node_sampler.next_nodes_and_probs:
            _woar += p * self.get_worst_policy_starting_value(sn)
        self._woar = _woar / self.H
    return self._woar</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.optimal_value_continuous_form"><code class="name">var <span class="ident">optimal_value_continuous_form</span> : Tuple[numpy.ndarray, numpy.ndarray]</code></dt>
<dd>
<div class="desc"><p>returns the optimal q and state values computed for the continuous form.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def optimal_value_continuous_form(self) -&gt; Tuple[np.ndarray, np.ndarray]:
    &#34;&#34;&#34;
    returns the optimal q and state values computed for the continuous form.
    &#34;&#34;&#34;
    if self._optimal_value_cf is None:
        self._optimal_value_cf = discounted_value_iteration(self.T_cf, self.R_cf)
    return self._optimal_value_cf</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.random_value_continuous_form"><code class="name">var <span class="ident">random_value_continuous_form</span></code></dt>
<dd>
<div class="desc"><p>returns the q and state values for the randomly acting policy computed for the continuous form.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def random_value_continuous_form(self):
    &#34;&#34;&#34;
    returns the q and state values for the randomly acting policy computed for the continuous form.
    &#34;&#34;&#34;
    if self._random_value_cf is None:
        self._random_value_cf = discounted_policy_iteration(
            self.T_cf, self.R_cf, self.random_policy_cf
        )
    return self._random_value_cf</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.reachable_states"><code class="name">var <span class="ident">reachable_states</span> : List[Tuple[int, NODE_TYPE]]</code></dt>
<dd>
<div class="desc"><p>returns the pairs of in episode time step and state that are feasible.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def reachable_states(self) -&gt; List[Tuple[int, &#34;NODE_TYPE&#34;]]:
    &#34;&#34;&#34;
    returns the pairs of in episode time step and state that are feasible.
    &#34;&#34;&#34;
    if self._reachable_states is None:
        self._reachable_states = [
            (h, self.node_to_index[n])
            for h, n in self.get_episodic_graph(False).nodes
        ]
    return self._reachable_states</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.worst_value_continuous_form"><code class="name">var <span class="ident">worst_value_continuous_form</span> : numpy.ndarray</code></dt>
<dd>
<div class="desc"><p>returns the q and state values for the worst performing policy computed for the continuous form.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def worst_value_continuous_form(self) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the q and state values for the worst performing policy computed for the continuous form.
    &#34;&#34;&#34;
    if self._worst_value_cf is None:
        self._worst_value_cf = discounted_value_iteration(self.T_cf, -self.R_cf)
    return self._worst_value_cf</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_episodic_graph"><code class="name flex">
<span>def <span class="ident">get_episodic_graph</span></span>(<span>self, remove_labels: bool) ‑> networkx.classes.digraph.DiGraph</span>
</code></dt>
<dd>
<div class="desc"><p>returns the graph corresponding the state space augmented with the in episode time step. It is possible to remove
the labels that mark the nodes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_episodic_graph(self, remove_labels: bool) -&gt; nx.DiGraph:
    &#34;&#34;&#34;
    returns the graph corresponding the state space augmented with the in episode time step. It is possible to remove
    the labels that mark the nodes.
    &#34;&#34;&#34;
    if remove_labels not in self._episodic_graph:
        self._episodic_graph[remove_labels] = get_episodic_graph(
            self.G, self.H, self.node_to_index, self.starting_nodes, remove_labels
        )
    return self._episodic_graph[remove_labels]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_minimal_regret_for_starting_node"><code class="name flex">
<span>def <span class="ident">get_minimal_regret_for_starting_node</span></span>(<span>self, node: NODE_TYPE)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the minimal possible regret obtained from the given starting state.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_minimal_regret_for_starting_node(self, node: &#34;NODE_TYPE&#34;):
    &#34;&#34;&#34;
    returns the minimal possible regret obtained from the given starting state.
    &#34;&#34;&#34;
    return self.get_optimal_policy_starting_value(
        node
    ) - self.get_worst_policy_starting_value(node)</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_optimal_policy_continuous_form"><code class="name flex">
<span>def <span class="ident">get_optimal_policy_continuous_form</span></span>(<span>self, stochastic_form: bool) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the optimal policy computed for the continuous form.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_optimal_policy_continuous_form(self, stochastic_form: bool) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the optimal policy computed for the continuous form.
    &#34;&#34;&#34;
    if stochastic_form not in self._optimal_policy_cf:
        self._optimal_policy_cf[stochastic_form] = get_policy_from_q_values(
            self.optimal_value_continuous_form[0], stochastic_form
        )
    return self._optimal_policy_cf[stochastic_form]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_optimal_policy_starting_value"><code class="name flex">
<span>def <span class="ident">get_optimal_policy_starting_value</span></span>(<span>self, node: NODE_TYPE)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the value of the given node at in episode time step zero for the optimal policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_optimal_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
    &#34;&#34;&#34;
    returns the value of the given node at in episode time step zero for the optimal policy.
    &#34;&#34;&#34;
    return self.optimal_value[1][0, self.node_to_index[node]]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_random_policy_starting_value"><code class="name flex">
<span>def <span class="ident">get_random_policy_starting_value</span></span>(<span>self, node: NODE_TYPE)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the value of the given node at in episode time step zero for the randomly acting policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_random_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
    &#34;&#34;&#34;
    returns the value of the given node at in episode time step zero for the randomly acting policy.
    &#34;&#34;&#34;
    return self.random_value[1][0, self.node_to_index[node]]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_worst_policy_continuous_form"><code class="name flex">
<span>def <span class="ident">get_worst_policy_continuous_form</span></span>(<span>self, stochastic_form) ‑> numpy.ndarray</span>
</code></dt>
<dd>
<div class="desc"><p>returns the worst policy computed for the continuous form.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_worst_policy_continuous_form(self, stochastic_form) -&gt; np.ndarray:
    &#34;&#34;&#34;
    returns the worst policy computed for the continuous form.
    &#34;&#34;&#34;
    if stochastic_form not in self._worst_policy_cf:
        self._worst_policy_cf[stochastic_form] = get_policy_from_q_values(
            self.worst_value_continuous_form[0], stochastic_form
        )
    return self._worst_policy_cf[stochastic_form]</code></pre>
</details>
</dd>
<dt id="colosseum.mdp.base_finite.EpisodicMDP.get_worst_policy_starting_value"><code class="name flex">
<span>def <span class="ident">get_worst_policy_starting_value</span></span>(<span>self, node: NODE_TYPE)</span>
</code></dt>
<dd>
<div class="desc"><p>returns the value of the given node at in episode time step zero for the worst performing policy.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_worst_policy_starting_value(self, node: &#34;NODE_TYPE&#34;):
    &#34;&#34;&#34;
    returns the value of the given node at in episode time step zero for the worst performing policy.
    &#34;&#34;&#34;
    return self.worst_value[1][0, self.node_to_index[node]]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="colosseum.mdp.base.BaseMDP" href="base.html#colosseum.mdp.base.BaseMDP">BaseMDP</a></b></code>:
<ul class="hlist">
<li><code><a title="colosseum.mdp.base.BaseMDP.R" href="base.html#colosseum.mdp.base.BaseMDP.R">R</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.T" href="base.html#colosseum.mdp.base.BaseMDP.T">T</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.action_spec" href="base.html#colosseum.mdp.base.BaseMDP.action_spec">action_spec</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.communication_class" href="base.html#colosseum.mdp.base.BaseMDP.communication_class">communication_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.diameter" href="base.html#colosseum.mdp.base.BaseMDP.diameter">diameter</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.discounted_value_norm" href="base.html#colosseum.mdp.base.BaseMDP.discounted_value_norm">discounted_value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure" href="base.html#colosseum.mdp.base.BaseMDP.does_seed_change_MDP_structure">does_seed_change_MDP_structure</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_grid_representation" href="base.html#colosseum.mdp.base.BaseMDP.get_grid_representation">get_grid_representation</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_info_class" href="base.html#colosseum.mdp.base.BaseMDP.get_info_class">get_info_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_measure_from_name" href="base.html#colosseum.mdp.base.BaseMDP.get_measure_from_name">get_measure_from_name</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_node_class" href="base.html#colosseum.mdp.base.BaseMDP.get_node_class">get_node_class</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_observation" href="base.html#colosseum.mdp.base.BaseMDP.get_observation">get_observation</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_optimal_policy" href="base.html#colosseum.mdp.base.BaseMDP.get_optimal_policy">get_optimal_policy</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_reward_distribution" href="base.html#colosseum.mdp.base.BaseMDP.get_reward_distribution">get_reward_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_transition_distributions" href="base.html#colosseum.mdp.base.BaseMDP.get_transition_distributions">get_transition_distributions</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_value_node_labels" href="base.html#colosseum.mdp.base.BaseMDP.get_value_node_labels">get_value_node_labels</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_visitation_counts" href="base.html#colosseum.mdp.base.BaseMDP.get_visitation_counts">get_visitation_counts</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.get_worst_policy" href="base.html#colosseum.mdp.base.BaseMDP.get_worst_policy">get_worst_policy</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.graph_layout" href="base.html#colosseum.mdp.base.BaseMDP.graph_layout">graph_layout</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.graph_metrics" href="base.html#colosseum.mdp.base.BaseMDP.graph_metrics">graph_metrics</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.hardness_report" href="base.html#colosseum.mdp.base.BaseMDP.hardness_report">hardness_report</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.hash" href="base.html#colosseum.mdp.base.BaseMDP.hash">hash</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.is_episodic" href="base.html#colosseum.mdp.base.BaseMDP.is_episodic">is_episodic</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.measures_of_hardness" href="base.html#colosseum.mdp.base.BaseMDP.measures_of_hardness">measures_of_hardness</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.n_actions" href="base.html#colosseum.mdp.base.BaseMDP.n_actions">n_actions</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.observation_spec" href="base.html#colosseum.mdp.base.BaseMDP.observation_spec">observation_spec</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_average_reward" href="base.html#colosseum.mdp.base.BaseMDP.optimal_average_reward">optimal_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_average_rewards" href="base.html#colosseum.mdp.base.BaseMDP.optimal_average_rewards">optimal_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_markov_chain" href="base.html#colosseum.mdp.base.BaseMDP.optimal_markov_chain">optimal_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_stationary_distribution" href="base.html#colosseum.mdp.base.BaseMDP.optimal_stationary_distribution">optimal_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_transition_probabilities" href="base.html#colosseum.mdp.base.BaseMDP.optimal_transition_probabilities">optimal_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.optimal_value" href="base.html#colosseum.mdp.base.BaseMDP.optimal_value">optimal_value</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.parameters" href="base.html#colosseum.mdp.base.BaseMDP.parameters">parameters</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_average_reward" href="base.html#colosseum.mdp.base.BaseMDP.random_average_reward">random_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_average_rewards" href="base.html#colosseum.mdp.base.BaseMDP.random_average_rewards">random_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_markov_chain" href="base.html#colosseum.mdp.base.BaseMDP.random_markov_chain">random_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_stationary_distribution" href="base.html#colosseum.mdp.base.BaseMDP.random_stationary_distribution">random_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_step" href="base.html#colosseum.mdp.base.BaseMDP.random_step">random_step</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_transition_probabilities" href="base.html#colosseum.mdp.base.BaseMDP.random_transition_probabilities">random_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.random_value" href="base.html#colosseum.mdp.base.BaseMDP.random_value">random_value</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.recurrent_nodes_set" href="base.html#colosseum.mdp.base.BaseMDP.recurrent_nodes_set">recurrent_nodes_set</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.reset" href="base.html#colosseum.mdp.base.BaseMDP.reset">reset</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.reset_visitation_counts" href="base.html#colosseum.mdp.base.BaseMDP.reset_visitation_counts">reset_visitation_counts</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sample_parameters" href="base.html#colosseum.mdp.base.BaseMDP.sample_parameters">sample_parameters</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sample_reward" href="base.html#colosseum.mdp.base.BaseMDP.sample_reward">sample_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.step" href="base.html#colosseum.mdp.base.BaseMDP.step">step</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps" href="base.html#colosseum.mdp.base.BaseMDP.sum_reciprocals_suboptimality_gaps">sum_reciprocals_suboptimality_gaps</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.summary" href="base.html#colosseum.mdp.base.BaseMDP.summary">summary</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards" href="base.html#colosseum.mdp.base.BaseMDP.transition_matrix_and_rewards">transition_matrix_and_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.undiscounted_value_norm" href="base.html#colosseum.mdp.base.BaseMDP.undiscounted_value_norm">undiscounted_value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.value_norm" href="base.html#colosseum.mdp.base.BaseMDP.value_norm">value_norm</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_average_reward" href="base.html#colosseum.mdp.base.BaseMDP.worst_average_reward">worst_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_average_rewards" href="base.html#colosseum.mdp.base.BaseMDP.worst_average_rewards">worst_average_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_markov_chain" href="base.html#colosseum.mdp.base.BaseMDP.worst_markov_chain">worst_markov_chain</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_stationary_distribution" href="base.html#colosseum.mdp.base.BaseMDP.worst_stationary_distribution">worst_stationary_distribution</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_transition_probabilities" href="base.html#colosseum.mdp.base.BaseMDP.worst_transition_probabilities">worst_transition_probabilities</a></code></li>
<li><code><a title="colosseum.mdp.base.BaseMDP.worst_value" href="base.html#colosseum.mdp.base.BaseMDP.worst_value">worst_value</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="colosseum.mdp" href="index.html">colosseum.mdp</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="colosseum.mdp.base_finite.EpisodicMDP" href="#colosseum.mdp.base_finite.EpisodicMDP">EpisodicMDP</a></code></h4>
<ul class="">
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.R_cf" href="#colosseum.mdp.base_finite.EpisodicMDP.R_cf">R_cf</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.T_cf" href="#colosseum.mdp.base_finite.EpisodicMDP.T_cf">T_cf</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.continuous_form_episodic_transition_matrix_and_rewards" href="#colosseum.mdp.base_finite.EpisodicMDP.continuous_form_episodic_transition_matrix_and_rewards">continuous_form_episodic_transition_matrix_and_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.episodic_optimal_average_reward" href="#colosseum.mdp.base_finite.EpisodicMDP.episodic_optimal_average_reward">episodic_optimal_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.episodic_random_average_reward" href="#colosseum.mdp.base_finite.EpisodicMDP.episodic_random_average_reward">episodic_random_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.episodic_transition_matrix_and_rewards" href="#colosseum.mdp.base_finite.EpisodicMDP.episodic_transition_matrix_and_rewards">episodic_transition_matrix_and_rewards</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.episodic_worst_average_reward" href="#colosseum.mdp.base_finite.EpisodicMDP.episodic_worst_average_reward">episodic_worst_average_reward</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_episodic_graph" href="#colosseum.mdp.base_finite.EpisodicMDP.get_episodic_graph">get_episodic_graph</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_minimal_regret_for_starting_node" href="#colosseum.mdp.base_finite.EpisodicMDP.get_minimal_regret_for_starting_node">get_minimal_regret_for_starting_node</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_optimal_policy_continuous_form" href="#colosseum.mdp.base_finite.EpisodicMDP.get_optimal_policy_continuous_form">get_optimal_policy_continuous_form</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_optimal_policy_starting_value" href="#colosseum.mdp.base_finite.EpisodicMDP.get_optimal_policy_starting_value">get_optimal_policy_starting_value</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_random_policy_starting_value" href="#colosseum.mdp.base_finite.EpisodicMDP.get_random_policy_starting_value">get_random_policy_starting_value</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_worst_policy_continuous_form" href="#colosseum.mdp.base_finite.EpisodicMDP.get_worst_policy_continuous_form">get_worst_policy_continuous_form</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.get_worst_policy_starting_value" href="#colosseum.mdp.base_finite.EpisodicMDP.get_worst_policy_starting_value">get_worst_policy_starting_value</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.optimal_value_continuous_form" href="#colosseum.mdp.base_finite.EpisodicMDP.optimal_value_continuous_form">optimal_value_continuous_form</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.random_value_continuous_form" href="#colosseum.mdp.base_finite.EpisodicMDP.random_value_continuous_form">random_value_continuous_form</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.reachable_states" href="#colosseum.mdp.base_finite.EpisodicMDP.reachable_states">reachable_states</a></code></li>
<li><code><a title="colosseum.mdp.base_finite.EpisodicMDP.worst_value_continuous_form" href="#colosseum.mdp.base_finite.EpisodicMDP.worst_value_continuous_form">worst_value_continuous_form</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>