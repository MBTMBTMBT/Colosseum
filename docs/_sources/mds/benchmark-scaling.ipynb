{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca01c228",
   "metadata": {},
   "source": [
    "# Scale Benchmarking to a Cluster\n",
    "\n",
    "`````{margin}\n",
    "````{dropdown} Necessary imports\n",
    "```{code-block} python\n",
    "import os\n",
    "\n",
    "from colosseum import config\n",
    "from colosseum.agent.agents.episodic import QLearningEpisodic\n",
    "from colosseum.agent.agents.infinite_horizon import QLearningContinuous\n",
    "from colosseum.benchmark import ColosseumDefaultBenchmark\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks_for_hyperopt\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks\n",
    "from colosseum.experiment.experiment_instances import save_instances_to_folder\n",
    "from colosseum.hyperopt import SMALL_HYPEROPT_CONF\n",
    "from colosseum.hyperopt.selection import retrieve_best_agent_config_from_hp_folder\n",
    "from colosseum.hyperopt.utils import sample_agent_configs_and_benchmarks_for_hyperopt\n",
    "\n",
    "# Set an experiment name that briefly describes the aim of the experiments\n",
    "experiments_folder = \"experiments\" + os.sep + \"tabular\"\n",
    "experiment_name = \"tutorial\"\n",
    "\n",
    "exp_instances_hpo_folder = config.get_hyperopt_folder() + \"experiment_instances\"\n",
    "\n",
    "config.set_experiments_folder(experiments_folder, experiment_name)\n",
    "config.set_hyperopt_folder(experiments_folder, experiment_name)\n",
    "```\n",
    "````\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3298133",
   "metadata": {
    "tags": [
     "remove-output",
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 20:10:02.626118: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-23 20:10:02.705826: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-23 20:10:03.052577: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-23 20:10:03.052617: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-23 20:10:03.052621: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from colosseum import config\n",
    "from colosseum.agent.agents.episodic import QLearningEpisodic\n",
    "from colosseum.agent.agents.infinite_horizon import QLearningContinuous\n",
    "from colosseum.benchmark import ColosseumDefaultBenchmark\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks_for_hyperopt\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks\n",
    "from colosseum.experiment.experiment_instances import save_instances_to_folder\n",
    "from colosseum.hyperopt import SMALL_HYPEROPT_CONF\n",
    "from colosseum.hyperopt.selection import retrieve_best_agent_config_from_hp_folder\n",
    "from colosseum.hyperopt.utils import sample_agent_configs_and_benchmarks_for_hyperopt\n",
    "\n",
    "agent_cls = [QLearningContinuous, QLearningEpisodic]\n",
    "\n",
    "# Set an experiment name that briefly describes the aim of the experiments\n",
    "experiments_folder = \"experiments\" + os.sep + \"tabular\"\n",
    "experiment_name = \"tutorial\"\n",
    "\n",
    "exp_instances_hpo_folder = config.get_hyperopt_folder() + \"experiment_instances\"\n",
    "exp_instances_bench_folder = config.get_experiments_folder() + \"experiment_instances\"\n",
    "\n",
    "config.set_experiments_folder(experiments_folder, experiment_name)\n",
    "config.set_hyperopt_folder(experiments_folder, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fe149",
   "metadata": {},
   "source": [
    "Scaling up {{col}} benchmarking to run on a cluster is particularly straightforward.\n",
    "Each agent/MDP interaction configuration can be stored as a [`ExperimentInstance`](../pdoc_files/colosseum/experiment/experiment_instance.html#ExperimentInstance) object, which can be easily pickled, uploaded to the cluster server, and run.\n",
    "To properly execute the instances, it is also necessary to upload the benchmark folders containing the gin configurations to the cluster as shown below.\n",
    "\n",
    "**Cluster jobs**  \n",
    "The main task of the cluster jobs is to run the experiment instances using the following functions.\n",
    "\n",
    "- [`run_experiment_instance`](../pdoc_files/colosseum/experiment/experiment_instances.html#run_experiment_instance) takes as input a ``ExperimentInstance`` object or a string containing a path to a file of a pickled ``ExperimentInstance``object, and runs the corresponding agent/MDP interaction.\n",
    "- [`run_experiment_instances`](../pdoc_files/colosseum/experiment/experiment_instances.html#run_experiment_instances) takes as input a list of ``ExperimentInstance`` or a list of strings containing paths to the pickled ``ExperimentInstance`` objects. This function allows to group and to run multiples experiment instances using a single core or multiple cores depending on whether the multiprocessing is enabled or not.\n",
    "\n",
    "<h4> Step 1: Hyperparameters optimization </h4>\n",
    "\n",
    "```{code-block} python\n",
    "# Assume we want to benchmark the following agent classes\n",
    "agent_cls = [QLearningContinuous, QLearningEpisodic]\n",
    "\n",
    "# Obtain the MDP configuration files and instantiate them locally\n",
    "hyperopt_benchmarks = sample_agent_configs_and_benchmarks_for_hyperopt(agent_cls, SMALL_HYPEROPT_CONF)\n",
    "\n",
    "# Create the corresponding ExperimentInstance objects\n",
    "hp_exp_instances = instantiate_and_get_exp_instances_from_agents_and_benchmarks_for_hyperopt(\n",
    "    hyperopt_benchmarks\n",
    ")\n",
    "\n",
    "# Pickle the experiment instances\n",
    "exp_instance_paths = save_instances_to_folder(hp_exp_instances, exp_instances_hpo_folder)\n",
    "```\n",
    "\n",
    "We have now instantiated locally all the files we need to run the hyperparameters optimization procedure: the agents gin configurations, the MDPs gin configurations, and the pickled ExperimentInstances.\n",
    "Note that, to simplify the entire procedure, it is important that the logging files are downloaded in the folders that were previously created when instantiating the agents and MDPs gin configuration files.\n",
    "\n",
    "A suggestion of how to upload/download the necessary directories to the cluster server is reported below,\n",
    "where `upload_folder` recursively copies a folder to the same path in the cluster and\n",
    "`download_folder` recursively downloads a folder from the cluster to the same path in your local machine.\n",
    "\n",
    "```{code-block} python\n",
    "# Upload\n",
    "for _, b in hyperopt_benchmarks:\n",
    "    upload_folder(b.get_hyperopt_benchmark_log_folder())\n",
    "upload_folder(exp_instances_hpo_folder)\n",
    "\n",
    "# Let the cluster jobs run\n",
    "\n",
    "# Download the results\n",
    "for _, b in hyperopt_benchmarks:\n",
    "    download_folder(cluster_ssh_path + b.get_hyperopt_benchmark_log_folder())\n",
    "```\n",
    "\n",
    "After the logging files have been downloaded and are available locally, we can proceed to the hyperparameters selection, which, by default, minimises the average normalized cumulative regret.\n",
    "```{code-block} python\n",
    "# Obtain the best hyperparameters given the performances stored in the loggings\n",
    "agents_configs = retrieve_best_agent_config_from_hp_folder(agent_cls)\n",
    "```\n",
    "\n",
    "<h4> Step 2: Agents benchmarking </h4>\n",
    "\n",
    "````{margin}\n",
    "```{tip}\n",
    "You can substitute the default benchmark with custom benchmarks here (see [Create Custom Bechmark tutorial](../mds/benchmark-custom.md)).\n",
    "```\n",
    "````\n",
    "\n",
    "The first step of the {{col}} benchmarking procedure is completed, we now proceed to benchmark the best agent configurations on the default benchmark.\n",
    "\n",
    "```{code-block} python\n",
    "# Store the episodic and continuous agents configs separately.\n",
    "agents_configs_episodic = {cl : agents_configs[cl] for cl in agents_configs if cl.is_episodic()}\n",
    "agents_configs_continuous = {cl : agents_configs[cl] for cl in agents_configs if not cl.is_episodic()}\n",
    "\n",
    "# Instantiate the benchmark for the different settings\n",
    "b_cc = ColosseumDefaultBenchmark.CONTINUOUS_COMMUNICATING.get_benchmark()\n",
    "b_ce = ColosseumDefaultBenchmark.CONTINUOUS_ERGODIC.get_benchmark()\n",
    "b_ec = ColosseumDefaultBenchmark.EPISODIC_COMMUNICATING.get_benchmark()\n",
    "b_ee = ColosseumDefaultBenchmark.EPISODIC_ERGODIC.get_benchmark()\n",
    "\n",
    "# Prepare the input for the ExperimentInstance creator function\n",
    "agents_and_benchmarks = [\n",
    "    (agents_configs_continuous, b_cc),\n",
    "    (agents_configs_continuous, b_ce),\n",
    "    (agents_configs_episodic, b_ec),\n",
    "    (agents_configs_episodic, b_ee),\n",
    "]\n",
    "\n",
    "# Instantiate the experiment instances (note the different function compared to the hyperoptimzation procedure)\n",
    "experiment_instances = instantiate_and_get_exp_instances_from_agents_and_benchmarks(agents_and_benchmarks)\n",
    "experiment_instances_paths = save_instances_to_folder(\n",
    "    experiment_instances, exp_instances_bench_folder\n",
    ")\n",
    "```\n",
    "\n",
    "Uploading and running the instances to the cluster should be done in the same way as we did for the hyperparameters optimization procedure.\n",
    "Note the different function used to obtain the folder of the benchmark.\n",
    "```{code-block} python\n",
    "for _, b in agents_and_benchmarks:\n",
    "    upload_folder(b.get_experiments_benchmark_log_folder())\n",
    "upload_folder(exp_instances_bench_folder)\n",
    "```\n",
    "\n",
    "After downloading the results of the benchmarking procedure, you can proceed to analyse the results as explained in the\n",
    "[Analyse Benchmarking Results tutorial](../mds/benchmark-analysis.md)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "source_map": [
   11,
   41,
   67
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}