{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "422f06ce",
   "metadata": {},
   "source": [
    "# Scaling up Colosseum\n",
    "\n",
    "`````{margin}\n",
    "````{dropdown} Necessary imports\n",
    "```{code-block} python\n",
    "import os\n",
    "\n",
    "from colosseum import config\n",
    "from colosseum.agent.agents.episodic import QLearningEpisodic\n",
    "from colosseum.agent.agents.infinite_horizon import QLearningContinuous\n",
    "from colosseum.benchmark import ColosseumDefaultBenchmark\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks_for_hyperopt\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks\n",
    "from colosseum.experiment.experiment_instances import save_instances_to_folder\n",
    "from colosseum.hyperopt import SMALL_HYPEROPT_CONF\n",
    "from colosseum.hyperopt.selection import retrieve_best_agent_config_from_hp_folder\n",
    "from colosseum.hyperopt.utils import sample_agent_configs_and_benchmarks_for_hyperopt\n",
    "\n",
    "# Set an experiment name that briefly describes the aim of the experiments\n",
    "experiments_folder = \"experiments\" + os.sep + \"tabular\"\n",
    "experiment_name = \"tutorial\"\n",
    "\n",
    "exp_instances_hpo_folder = config.get_hyperopt_folder() + \"experiment_instances\"\n",
    "\n",
    "config.set_experiments_folder(experiments_folder, experiment_name)\n",
    "config.set_hyperopt_folder(experiments_folder, experiment_name)\n",
    "```\n",
    "````\n",
    "`````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b049fe",
   "metadata": {
    "tags": [
     "remove-output",
     "remove-input"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 16:42:45.948293: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-10-15 16:42:46.033062: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-15 16:42:46.391347: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-15 16:42:46.391386: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-10-15 16:42:46.391391: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from colosseum import config\n",
    "from colosseum.agent.agents.episodic import QLearningEpisodic\n",
    "from colosseum.agent.agents.infinite_horizon import QLearningContinuous\n",
    "from colosseum.benchmark import ColosseumDefaultBenchmark\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks_for_hyperopt\n",
    "from colosseum.benchmark.run import instantiate_and_get_exp_instances_from_agents_and_benchmarks\n",
    "from colosseum.experiment.experiment_instances import save_instances_to_folder\n",
    "from colosseum.hyperopt import SMALL_HYPEROPT_CONF\n",
    "from colosseum.hyperopt.selection import retrieve_best_agent_config_from_hp_folder\n",
    "from colosseum.hyperopt.utils import sample_agent_configs_and_benchmarks_for_hyperopt\n",
    "\n",
    "agent_cls = [QLearningContinuous, QLearningEpisodic]\n",
    "\n",
    "# Set an experiment name that briefly describes the aim of the experiments\n",
    "experiments_folder = \"experiments\" + os.sep + \"tabular\"\n",
    "experiment_name = \"tutorial\"\n",
    "\n",
    "exp_instances_hpo_folder = config.get_hyperopt_folder() + \"experiment_instances\"\n",
    "exp_instances_bench_folder = config.get_experiments_folder() + \"experiment_instances\"\n",
    "\n",
    "config.set_experiments_folder(experiments_folder, experiment_name)\n",
    "config.set_hyperopt_folder(experiments_folder, experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244252f4",
   "metadata": {},
   "source": [
    "Scaling up {{col}} benchmarking to run on a cluster is particularly straightforward.\n",
    "Each agent/MDP interaction configuration can be stored as [`ExperimentInstance`](../pdoc_files/colosseum/experiment/experiment_instance.html#ExperimentInstance) objects, which can be easily pickled, uploaded to the cluster server, and run in the cluster.\n",
    "Note that, in order to properly execute the instances, it is also necessary to upload the benchmark folders containing the gin configurations to the cluster.\n",
    "\n",
    "## Cluster jobs\n",
    "\n",
    "Before diving into the details of how to set up the benchmarks, the agent configurations, and the necessary folder, we illustrate how to run `ExperimentInstance` objects, which will be the main task of the cluster jobs.\n",
    "\n",
    "The [`run_experiment_instance`](../pdoc_files/colosseum/experiment/experiment_instances.html#run_experiment_instance) function takes as input a ``ExperimentInstance`` object or a string containing a path to a file of a pickled ``ExperimentInstance``object, and runs the corresponding agent/MDP interaction.\n",
    "\n",
    "The [`run_experiment_instances`](../pdoc_files/colosseum/experiment/experiment_instances.html#run_experiment_instances) function takes as input a list of ``ExperimentInstance`` or a list of strings containing paths to files of pickled ``ExperimentInstance`` objects. This function allows to group and to run multiples experiment instances using a single core or multiple cores depending on whether the multiprocessing is enabled or not.\n",
    "\n",
    "## Cluster hyperparameters optimization\n",
    "\n",
    "The hyperparameters optimization procedure is the first step in the {{col}} benchmarking scheme.\n",
    "\n",
    "```{code-block} python\n",
    "# Assume we want to benchmark the following agent classes\n",
    "agent_cls = [QLearningContinuous, QLearningEpisodic]\n",
    "\n",
    "# Obtain the MDP configuration files and instantiate them locally\n",
    "hyperopt_benchmarks = sample_agent_configs_and_benchmarks_for_hyperopt(agent_cls, SMALL_HYPEROPT_CONF)\n",
    "\n",
    "# Create the corresponding ExperimentInstance objects\n",
    "hp_exp_instances = instantiate_and_get_exp_instances_from_agents_and_benchmarks_for_hyperopt(\n",
    "    hyperopt_benchmarks\n",
    ")\n",
    "\n",
    "# Pickle the experiment instances\n",
    "exp_instance_paths = save_instances_to_folder(hp_exp_instances, exp_instances_hpo_folder)\n",
    "```\n",
    "\n",
    "We have now instantiated locally all the files we need to run the hyperparameters optimization procedure: the agents gin configurations, the MDPs gin configurations, and the pickled ExperimentInstances.\n",
    "It is now time to upload them to the cluster.\n",
    "\n",
    "An example of how you can upload the necessary directories to the cluster server is reported below, where the ``upload_folder`` is a function you should implement to recursively copy a folder to the cluster.\n",
    "```{code-block} python\n",
    "for _, b in hyperopt_benchmarks:\n",
    "    upload_folder(b.get_hyperopt_benchmark_log_folder())\n",
    "upload_folder(exp_instances_hpo_folder)\n",
    "```\n",
    "\n",
    "After the upload has finished, you can proceed to run the instances and download the results.\n",
    "Note that, in order to simplify the entire procedure, it is important that the logging files are downloaded in the folders that were previously created when instantiating the agents and MDPs gin configuration files.\n",
    "\n",
    "```{code-block} python\n",
    "for _, b in hyperopt_benchmarks:\n",
    "    download_folder(cluster_ssh_path + b.get_hyperopt_benchmark_log_folder())\n",
    "```\n",
    "\n",
    "After the logging files have been downloaded and are available locally, we can proceed to the hyperparameters selection, which, by default, minimises the average normalized cumulative regret.\n",
    "```{code-block} python\n",
    "# Obtain the best hyperparameters given the performances stored in the loggings\n",
    "agents_configs = retrieve_best_agent_config_from_hp_folder(agent_cls)\n",
    "```\n",
    "\n",
    "## Benchmark\n",
    "\n",
    "````{margin}\n",
    "```{tip}\n",
    "You can substitute the default benchmark with custom benchmarks here, have a look at the [_Custom benchmark_ tutorial](../tutorials/benchmark-custom.md) to review how to create your own benchmarks.\n",
    "```\n",
    "````\n",
    "\n",
    "The first step of the {{col}} benchmarking procedure is done, we now proceed to benchmark the best agent configurations on the default {{col}} benchmark.\n",
    "\n",
    "```{code-block} python\n",
    "# Store the episodic and continuous agents configs separately.\n",
    "agents_configs_episodic = {cl : agents_configs[cl] for cl in agents_configs if cl.is_episodic()}\n",
    "agents_configs_continuous = {cl : agents_configs[cl] for cl in agents_configs if not cl.is_episodic()}\n",
    "\n",
    "# Instantiate the benchmark for the different settings\n",
    "b_cc = ColosseumDefaultBenchmark.CONTINUOUS_COMMUNICATING.get_benchmark()\n",
    "b_ce = ColosseumDefaultBenchmark.CONTINUOUS_ERGODIC.get_benchmark()\n",
    "b_ec = ColosseumDefaultBenchmark.EPISODIC_COMMUNICATING.get_benchmark()\n",
    "b_ee = ColosseumDefaultBenchmark.EPISODIC_ERGODIC.get_benchmark()\n",
    "\n",
    "# Prepare the input for the ExperimentInstance creator function\n",
    "agents_and_benchmarks = [\n",
    "    (agents_configs_continuous, b_cc),\n",
    "    (agents_configs_continuous, b_ce),\n",
    "    (agents_configs_episodic, b_ec),\n",
    "    (agents_configs_episodic, b_ee),\n",
    "]\n",
    "\n",
    "# Instantiate the experiment instances (note the different function compared to the hyperoptimzation procedure)\n",
    "experiment_instances = instantiate_and_get_exp_instances_from_agents_and_benchmarks(agents_and_benchmarks)\n",
    "experiment_instances_paths = save_instances_to_folder(\n",
    "    experiment_instances, exp_instances_bench_folder\n",
    ")\n",
    "```\n",
    "\n",
    "Uploading and running the instances to the cluster should be done in the same way as we did for the hyperparameters optimization procedure.\n",
    "Note the different function used to obtain the folder of the benchmark.\n",
    "```{code-block} python\n",
    "for _, b in agents_and_benchmarks:\n",
    "    upload_folder(b.get_experiments_benchmark_log_folder())\n",
    "upload_folder(exp_instances_bench_folder)\n",
    "```\n",
    "\n",
    "After downloading the results of the benchmarking procedure, you can proceed to analyse the results as explained in the\n",
    "[_Benchmark analysis_ tutorial](../tutorials/benchmark-analysis)."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "source_map": [
   11,
   41,
   67
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}