{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Hardness analysis\n",
    "\n",
    "$\\texttt{Colosseum}$ allows to empirically investigate the measures of hardness\n",
    "in the four scenarios (see the paper for additional details) automatically for\n",
    "MDP families that are implemented in the pacakge.\n",
    "Note, however, that calculating the measures of hardness for customly specified\n",
    "MDPs is particularly simple thanks to the $\\texttt{Custom}$ class.\n",
    "\n",
    "In the following, we show how to reproduce the empirical investigation for\n",
    "the $\\texttt{RiverSwim}$ MDP family and how to calculate the measures of hardness for a $\\texttt{MiniGridEmpty}$ MDP and a custom MDP."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import beta\n",
    "\n",
    "from colosseum.experiments.increasing_size import moving_sizes\n",
    "from colosseum.experiments.increasing_plazy import moving_plazy\n",
    "from colosseum.experiments.increasing_prand import moving_prand\n",
    "from colosseum.mdps.custom import CustomEpisodic\n",
    "from colosseum.mdps.minigrid_empty import MiniGridEmptyContinuous\n",
    "\n",
    "from colosseum.mdps.river_swim import RiverSwimEpisodic\n",
    "\n",
    "sns.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scenario 1**. We vary the probability $\\texttt{p_rand}$ that an MDP executes a random action instead of the action selected by an agent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "moving_prand(\n",
    "    mdp_class=RiverSwimEpisodic,\n",
    "    prandom=np.linspace(0.0001, 0.6, 10),\n",
    "    mdp_kwargs=dict(size=10),\n",
    "    # We investigate RiverSwim with different values of the p_lazy parameter and\n",
    "    # fixed chain length of ten.\n",
    "    n_seeds=1,\n",
    "    # We do not need more than one seed for this MDP family since the MDP is a\n",
    "    # deterministic function of the parameters. Note that this is not the case\n",
    "    # for othe families such as the MiniGrid ones or the FrozenLake.\n",
    "    approximate_regret=False,\n",
    "    # We don't compute the cumulative regret of the tuned near-optimal agent in\n",
    "    # this tutorial for brevity's sake.\n",
    "    save_folder=None,\n",
    "    # We don't need to save the results of the analysis.\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scenario 2**. We vary the probability $\\texttt{p_lazy}$ that an MDP stays in the same state instead of executing the action selected by an agent."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "moving_plazy(\n",
    "    mdp_class=RiverSwimEpisodic,\n",
    "    prandom=np.linspace(0.0001, 0.6, 10),\n",
    "    mdp_kwargs=dict(size=10),\n",
    "    # We investigate RiverSwim with different values of the p_rand parameter and\n",
    "    # fixed chain length of ten.\n",
    "    n_seeds=1,\n",
    "    # We do not need more than one seed for this MDP family since the MDP is a\n",
    "    # deterministic function of the parameters. Note that this is not the case\n",
    "    # for othe families such as the MiniGrid ones or the FrozenLake.\n",
    "    approximate_regret=False,\n",
    "    # We don't compute the cumulative regret of the tuned near-optimal agent in\n",
    "    # this tutorial for brevity's sake.\n",
    "    save_folder=None,\n",
    "    # We don't need to save the results of the analysis.\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scenario 3**. We vary the number of states across MDPs from the same family."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "moving_sizes(\n",
    "    mdp_class=RiverSwimEpisodic,\n",
    "    sizes=np.linspace(5, 20, 6).astype(int),\n",
    "    p_rand=None,\n",
    "    # We investigate RiverSwim with different chain lengths with fixed p_rand.\n",
    "    n_seeds=1,\n",
    "    # We do not need more than one seed for this MDP family since the MDP is a\n",
    "    # deterministic function of the parameters. Note that this is not the case\n",
    "    # for othe families such as the MiniGrid ones or the FrozenLake.\n",
    "    approximate_regret=False,\n",
    "    # We don't compute the cumulative regret of the tuned near-optimal agent in\n",
    "    # this tutorial for brevity's sake.\n",
    "    save_folder=None\n",
    "    # We don't need to save the results of the analysis.\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Scenario 4**. We vary the number of states across MDPs from the same family with $\\texttt{p_rand}=0.1$."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "moving_sizes(\n",
    "    mdp_class=RiverSwimEpisodic,\n",
    "    sizes=np.linspace(5, 20, 6).astype(int),\n",
    "    p_rand=0.1,\n",
    "    # We investigate RiverSwim with different chain lengths with fixed p_rand.\n",
    "    n_seeds=1,\n",
    "    # We do not need more than one seed for this MDP family since the MDP is a\n",
    "    # deterministic function of the parameters. Note that this is not the case\n",
    "    # for othe families such as the MiniGrid ones or the FrozenLake.\n",
    "    approximate_regret=False,\n",
    "    # We don't compute the cumulative regret of the tuned near-optimal agent in\n",
    "    # this tutorial for brevity's sake.\n",
    "    save_folder=None\n",
    "    # We don't need to save the results of the analysis.\n",
    ");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $\\texttt{MiniGridEmpty}$ MDP hardness"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mdp = MiniGridEmptyContinuous(seed=0, size=5, lazy=0.1, random_action_p=None)\n",
    "print(mdp.measures_of_hardness)\n",
    "print(mdp.communication_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### $\\texttt{Custom}$ MDP hardness"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "num_states = 4\n",
    "num_actions = 2\n",
    "T = [\n",
    "    [[0.0, 1.00, 0.00, 0.0], [0.0, 0.0, 1.0, 0.0]],\n",
    "    [[0.0, 0.00, 0.50, 0.5], [0.0, 0.8, 0.1, 0.1]],\n",
    "    [[0.0, 0.50, 0.00, 0.5], [0.0, 0.1, 0.8, 0.1]],\n",
    "    [[0.5, 0.25, 0.25, 0.0], [0.1, 0.1, 0.1, 0.7]],\n",
    "]\n",
    "np.random.seed(42)\n",
    "R = {\n",
    "    (s, a): beta(np.random.uniform(0, 30), np.random.uniform(0, 30))\n",
    "    for s in range(num_states)\n",
    "    for a in range(num_actions)\n",
    "}\n",
    "# R = np.random.randn(num_states, num_actions)  (FOR DETERMINISTIC REWARDS)\n",
    "T_0 = {0: 1.0}\n",
    "mdp = CustomEpisodic(\n",
    "    seed=42,\n",
    "    T_0=T_0,\n",
    "    T=np.array(T),\n",
    "    R=R,\n",
    "    lazy=None,\n",
    "    random_action_p=None,\n",
    "    force_single_thread=True,\n",
    ")\n",
    "\n",
    "print(mdp.measures_of_hardness)\n",
    "print(mdp.communication_type)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}