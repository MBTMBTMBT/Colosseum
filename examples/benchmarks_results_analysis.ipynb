{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Benchmarks results analysis\n",
    "\n",
    "In this section, we demonstrate how to analyse the result of the benchmark using the same data presented in the paper.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hardness of the MDPs in the benchmark\n",
    "In order to investigate the hardness of the MDPs in the benchmark, we position each MDP according to their diameters and environmental value norms.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import seaborn as sns\n",
    "from glob import glob\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import cm\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "from colosseum.experiments.analyze import analyze\n",
    "from colosseum.experiments.visualisation import experiment_summary2\n",
    "from colosseum.experiments.utils import retrieve_experiment_prms, retrieve_n_seed\n",
    "from colosseum.utils.benchmark_analysis import plot_labels_on_benchmarks_hardness_space\n",
    "from colosseum.utils.miscellanea import clear_th, get_all_mdp_classes\n",
    "\n",
    "\n",
    "def crs_in_hardness_space(exp_to_show):\n",
    "    color_map = cm.get_cmap(\"Reds\")\n",
    "    df = experiment_summary2(glob(f\"{exp_to_show}{os.sep}logs{os.sep}**\"))\n",
    "    df_numerical = df.applymap(lambda s: float(re.findall(\"\\d+\\.\\d+\", s)[0]))\n",
    "\n",
    "    fig, axes = plt.subplots(1, len(df.columns), figsize=(len(df.columns) * 7, 7))\n",
    "    for i, (a, ax) in enumerate(zip(df.columns, axes.tolist())):\n",
    "        plot_labels_on_benchmarks_hardness_space(\n",
    "            exp_to_show,\n",
    "            text_f=lambda x: df.loc[x, a]\n",
    "            .replace(\"\\\\textbf\", \"\")\n",
    "            .replace(\"$\", \"\")\n",
    "            .replace(\"{\", \"\")\n",
    "            .replace(\"}\", \"\")\n",
    "            .replace(\"\\\\pm\", \"Â±\")[:4],\n",
    "            color_f=lambda x: color_map(\n",
    "                df_numerical.loc[x, a] / df_numerical.loc[:, a].max()\n",
    "            ),\n",
    "            ax=ax,\n",
    "            multiplicative_factor_xlim=1.1,\n",
    "            underneath_x_label=\"\\n\" + [\"(a)\", \"(b)\", \"(c)\", \"(d)\"][i],\n",
    "            set_ylabel=i == 0,\n",
    "        )\n",
    "        ax.set_title(clear_th(a))\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "available_experiments = list(\n",
    "    sorted(\n",
    "        filter(\n",
    "            lambda x: x.split(os.sep)[-1][0] != \"_\",\n",
    "            glob(f\"experiments_done{os.sep}*\"),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "assert available_experiments == [\n",
    "    \"experiments_done/benchmark_continuous_communicating\",\n",
    "    \"experiments_done/benchmark_continuous_ergodic\",\n",
    "    \"experiments_done/benchmark_episodic_communicating\",\n",
    "    \"experiments_done/benchmark_episodic_ergodic\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cumulative regrets in hardness space\n",
    "\n",
    "In order to illustrate how hardness measures relate with cumulative regret and empirically valide the benchmark, we place the average cumulative regret obtained by each agent in each continuous ergodic MDP in a coordinate that corresponds to the diameter and the environmental value norm of that MDP."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Benchmark continuous communicating\n",
    "We note that increasing values of the diameter induce higher regret for UCRL2 and QLearning but not for PSRL.\n",
    "UCRL2's regrets are not significantly influenced by the value norms, contrary to QLearning and PSRL.\n",
    "Interestingly, higher values of the value norm yield lower regrets for QLearning and higher regrets for PSRL.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mdp_names = list(sorted(set(clear_th(x.__name__) for x in get_all_mdp_classes())))\n",
    "COLORS = list(matplotlib.colors.TABLEAU_COLORS.keys())\n",
    "fig, axes = plt.subplots(1, 4, figsize=(4 * 6, 6))  # , sharey=True)\n",
    "for ii, exp_to_show in enumerate(available_experiments):\n",
    "    plot_labels_on_benchmarks_hardness_space(\n",
    "        exp_to_show,\n",
    "        text_f=lambda x: str(int(x[1][-1]) + 1),\n",
    "        color_f=lambda x: COLORS[mdp_names.index(clear_th(x[0]))],\n",
    "        label_f=lambda x: clear_th(x[0]) if \"0\" in x[1] else None,\n",
    "        ax=axes[ii],\n",
    "        multiplicative_factor_xlim=1.05,\n",
    "        multiplicative_factor_ylim=1.05,\n",
    "        set_ylabel=ii == 0,\n",
    "        set_legend=False,\n",
    "        # xaxis_measure=(\"num_states\", lambda x : x.num_states)\n",
    "        # xaxis_measure = \"suboptimal_gaps\"\n",
    "    )\n",
    "\n",
    "leg = plt.legend(\n",
    "    fontsize=22,\n",
    "    ncol=8,\n",
    "    loc=\"center left\",\n",
    "    bbox_to_anchor=(-4.0, 1.11),\n",
    "    markerscale=1.3,\n",
    ")\n",
    "for x in leg.get_lines():\n",
    "    x.set_linewidth(4)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crs_in_hardness_space(available_experiments[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Benchmark continuous ergodic\n",
    "In this case, the increases in the value norm and the diameter produce increases in the regret in a similar way.\n",
    "A similar phenomenon can be seen for QLearning.\n",
    "For PSRL, instead, the ergodic case looks very similar to the communicating one.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crs_in_hardness_space(available_experiments[1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Benchmark episodic communicating\n",
    "In this scenario, we see that the diameter has relatively low influence on the cumulative regrets of the agents.\n",
    "Increases in the environmental value norm induce more significant increases in the regrets, particularly for PSRL.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crs_in_hardness_space(available_experiments[2])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Benchmark episodic ergodic\n",
    "Contrary to the communicating case, here increasing values of the diameter yield more significant increases of the regrets.\n",
    "Especially for high values of environmental value norm.\n",
    "Similarly to the previous scenario, the environmental value norm drives significant increases in the regrets of the agents,\n",
    "in a more pronounced way for QLearning."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "crs_in_hardness_space(available_experiments[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Interactive analysis of the benchmarks results.\n",
    "In addition to the previous visualizations, $\\texttt{Colosseum}$ offers an interactive analysis of the results. It is possible to group the results *by MDP* to analyse the performances of the agents on the MDPs in the benchmark, or to group *by agent*, to investigate how the parameters/hardness of the MDPs influences the final cumulative regrets."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "analyze(w=5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}